{
 "metadata": {
  "name": "Exp5"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#SCIRL mountain car\n",
      "#Mountain Car\n",
      "from stuff import *\n",
      "from pylab import *\n",
      "from random import *\n",
      "import numpy\n",
      "from rl import *\n",
      "import sys\n",
      "ACTION_SPACE=[-1,0,1]\n",
      "NB_SAMPLES=10\n",
      "#NB_SAMPLES=int(sys.argv[1])\n",
      "RAND_STRING=str(int(rand()*1000000))\n",
      "def mountain_car_next_state(state,action):\n",
      "    position,speed=state\n",
      "    next_speed = squeeze(speed+action*0.001+cos(3*position)*(-0.0025))\n",
      "    next_position = squeeze(position+next_speed)\n",
      "    next_speed = next_speed if next_speed > -0.07 else -0.07\n",
      "    next_speed = next_speed if next_speed < 0.07 else 0.07\n",
      "    next_position = next_position if next_position > -1.2 else -1.2\n",
      "    next_position = next_position if next_position < 0.6 else 0.6\n",
      "    return array([next_position,next_speed])\n",
      "def mountain_car_uniform_state():\n",
      "    return array([numpy.random.uniform(low=-1.2,high=0.6),numpy.random.uniform(low=-0.07,high=0.07)])\n",
      "def mountain_car_interesting_state():\n",
      "    position = choice([numpy.random.uniform(low=-1.2,high=-0.8),numpy.random.uniform(low=0,high=0.6)])\n",
      "    speed = choice([numpy.random.uniform(low=-0.07,high=-0.04),numpy.random.uniform(low=0.04,high=0.07)])\n",
      "    if position < 0 and speed < 0:\n",
      "        speed=-speed\n",
      "    return array([position,speed])\n",
      "mountain_car_mu_position, mountain_car_mu_speed = meshgrid(linspace(-1.2,0.6,5),linspace(-0.07,0.07,5))\n",
      "\n",
      "mountain_car_sigma_position = 2*pow((0.6+1.2)/5.,2)\n",
      "mountain_car_sigma_speed = 2*pow((0.07+0.07)/5.,2)\n",
      "\n",
      "def mountain_car_single_psi(state):\n",
      "    position,speed=state\n",
      "    psi=[]\n",
      "    for mu in zip_stack(mountain_car_mu_position, mountain_car_mu_speed).reshape(5*5,2):\n",
      "        psi.append(exp( -pow(position-mu[0],2)/mountain_car_sigma_position \n",
      "                        -pow(speed-mu[1],2)/mountain_car_sigma_speed))\n",
      "    return array(psi).reshape((5*5,1))\n",
      "mountain_car_psi= non_scalar_vectorize(mountain_car_single_psi,(2,),(25,1))\n",
      "def mountain_car_single_phi(sa):\n",
      "    state=sa[:2]\n",
      "    index_action = int(sa[-1])+1\n",
      "    answer=zeros((5*5*3,1))\n",
      "    answer[index_action*5*5:index_action*5*5+5*5] = mountain_car_single_psi(state)\n",
      "    return answer\n",
      "\n",
      "mountain_car_phi= non_scalar_vectorize(mountain_car_single_phi,(3,),(75,1))\n",
      "\n",
      "def mountain_car_reward(sas):\n",
      "    position=sas[0]\n",
      "    return 1 if position > 0.5 else 0\n",
      "\n",
      "def mountain_car_training_data(freward=mountain_car_reward):\n",
      "    traj = []\n",
      "    random_policy = lambda s:choice(ACTION_SPACE)\n",
      "    for i in range(0,500):\n",
      "        state = mountain_car_uniform_state()\n",
      "        for i in range(0,10):\n",
      "            action = random_policy(state)\n",
      "            next_state = mountain_car_next_state(state, action)\n",
      "            reward = freward(hstack([state, action, next_state]))\n",
      "            traj.append(hstack([state, action, next_state, reward]))\n",
      "            state=next_state\n",
      "    return array(traj)\n",
      "#omega=genfromtxt(\"mountain_car_expert_omega.mat\")\n",
      "#policy=greedy_policy(omega, mountain_car_phi, ACTION_SPACE)\n",
      "data = mountain_car_training_data()\n",
      "policy,omega = lspi( data, s_dim=2,a_dim=1, A=ACTION_SPACE, phi=mountain_car_phi, phi_dim=75, iterations_max=20 )\n",
      "savetxt(\"data/Expert_omega_\"+str(NB_SAMPLES)+\"_\"+RAND_STRING+\".mat\",omega)\n",
      "def mountain_car_testing_data(policy):\n",
      "    traj = []\n",
      "    state = mountain_car_interesting_state()\n",
      "    t=0\n",
      "    reward = 0\n",
      "    while t < 300 and reward == 0:\n",
      "        t+=1\n",
      "        action = policy(state)\n",
      "        next_state = mountain_car_next_state(state, action)\n",
      "        next_action = policy(next_state)\n",
      "        reward = mountain_car_reward(hstack([state, action, next_state]))\n",
      "        traj.append(hstack([state, action, next_state, next_action, reward]))\n",
      "        state=mountain_car_interesting_state()\n",
      "    return array(traj)\n",
      "while True:#Do..while\n",
      "    data = vstack([mountain_car_testing_data(policy) for i in range(0,10)])\n",
      "    TRANS = array([choice(data) for i in range(0,NB_SAMPLES)])\n",
      "    actions = TRANS[:,2]\n",
      "    if any(actions == 0) and any(actions == -1) and any(actions == 1):\n",
      "        break\n",
      "psi=mountain_car_psi\n",
      "phi=mountain_car_phi\n",
      "s=TRANS[:,:2]\n",
      "a=TRANS[:,2]\n",
      "s_dash=TRANS[:,3:5]\n",
      "a_dash=TRANS[:,5]\n",
      "sa=TRANS[:,:3]\n",
      "sa_dash=TRANS[:,3:6]\n",
      "\n",
      "##SCIRL\n",
      "#Precomputing mu, with Monte-Carlo + heuristics\n",
      "#On est oblig\u00e9s d'utiliser LSTDmu puisqu'on a que des transitions d\u00e9corell\u00e9es et non des trajectoires\n",
      "A = zeros((75,75))\n",
      "b = zeros((75,25))\n",
      "phi_t = phi(sa)\n",
      "phi_t_dash = phi(sa_dash)\n",
      "psi_t = psi(s)\n",
      "for phi_t,phi_t_dash,psi_t in zip(phi_t,phi_t_dash,psi_t):\n",
      "    A = A + dot(phi_t,\n",
      "            (phi_t - GAMMA*phi_t_dash).transpose())\n",
      "    b = b + dot(phi_t,psi_t.transpose())\n",
      "omega_lstd_mu = dot(inv(A+LAMBDA*identity(75)),b)\n",
      "phi_t.shape, phi_t_dash.shape, psi_t.shape\n",
      "feature_expectations = {}\n",
      "for state,action in zip(s,a):\n",
      "    state_action = hstack([state,action])\n",
      "    mu = dot(omega_lstd_mu.transpose(),phi(state_action))\n",
      "    feature_expectations[str(state_action)] = mu\n",
      "    for other_action in [a for a in ACTION_SPACE if a != action]:\n",
      "        state_action=hstack([state,other_action])\n",
      "        feature_expectations[str(state_action)]=GAMMA*mu\n",
      "#Structured Classifier\n",
      "class GradientDescent(object):\n",
      "    \n",
      "   def alpha( self, t ):\n",
      "      raise NotImplementedError, \"Cannot call abstract method\"\n",
      "\n",
      "   theta_0=None\n",
      "   Threshold=None\n",
      "   T = -1\n",
      "   sign = None\n",
      "        \n",
      "   def run( self, f_grad, f_proj=None, b_norm=False ): #grad is a function of theta\n",
      "      theta = self.theta_0.copy()\n",
      "      best_theta = theta.copy()\n",
      "      best_norm = float(\"inf\")\n",
      "      best_iter = 0\n",
      "      t=0\n",
      "      while True:#Do...while loop\n",
      "         t+=1\n",
      "         DeltaTheta = f_grad( theta )\n",
      "         current_norm = norm( DeltaTheta )\n",
      "         if b_norm and  current_norm > 0.:\n",
      "             DeltaTheta /= norm( DeltaTheta )\n",
      "         theta = theta + self.sign * self.alpha( t )*DeltaTheta\n",
      "         if f_proj:\n",
      "             theta = f_proj( theta )\n",
      "         print \"Norme du gradient : \"+str(current_norm)+\", pas : \"+str(self.alpha(t))+\", iteration : \"+str(t)\n",
      "\n",
      "         if current_norm < best_norm:\n",
      "             best_norm = current_norm\n",
      "             best_theta = theta.copy()\n",
      "             best_iter = t\n",
      "         if norm < self.Threshold or (self.T != -1 and t >= self.T):\n",
      "             break\n",
      "\n",
      "      print \"Gradient de norme : \"+str(best_norm)+\", a l'iteration : \"+str(best_iter)\n",
      "      return best_theta\n",
      "\n",
      "\n",
      "class StructuredClassifier(GradientDescent):\n",
      "    sign=-1.\n",
      "    Threshold=0.1 #Sensible default\n",
      "    T=40 #Sensible default\n",
      "    phi=None\n",
      "    phi_xy=None\n",
      "    inputs=None\n",
      "    labels=None\n",
      "    label_set=None\n",
      "    dic_data={}\n",
      "    x_dim=None\n",
      "    \n",
      "    def alpha(self, t):\n",
      "        return 3./(t+1)#Sensible default\n",
      "    \n",
      "    def __init__(self, data, x_dim, phi, phi_dim, Y):\n",
      "        self.x_dim=x_dim\n",
      "        self.inputs = data[:,:-1]\n",
      "        shape = list(data.shape)\n",
      "        shape[-1] = 1\n",
      "        self.labels = data[:,-1].reshape(shape)\n",
      "        self.phi=phi\n",
      "        self.label_set = Y\n",
      "        self.theta_0 = zeros((phi_dim,1))\n",
      "        self.phi_xy = self.phi(data)\n",
      "        for x,y in zip(self.inputs,self.labels):\n",
      "            self.dic_data[str(x)] = y\n",
      "        print self.inputs.shape\n",
      "    \n",
      "    def structure(self, xy):\n",
      "        return 0. if xy[-1] == self.dic_data[str(xy[:-1])] else 1.\n",
      "        \n",
      "    def structured_decision(self, theta):\n",
      "        def decision( x ):\n",
      "            score = lambda xy: dot(theta.transpose(),self.phi(xy)) + self.structure(xy)\n",
      "            input_label_couples = [hstack([x,y]) for y in self.label_set]\n",
      "            best_label = argmax(input_label_couples, score)[-1]\n",
      "            return best_label\n",
      "        vdecision = non_scalar_vectorize(decision, (self.x_dim,), (1,1))\n",
      "        return lambda x: vdecision(x).reshape(x.shape[:-1]+(1,))\n",
      "    \n",
      "    def gradient(self, theta):\n",
      "        classif_rule = self.structured_decision(theta)\n",
      "        y_star = classif_rule(self.inputs)\n",
      "        #print \"Gradient : \"+str(y_star)\n",
      "        #print str(self.labels)\n",
      "        phi_star = self.phi(hstack([self.inputs,y_star]))\n",
      "        return mean(phi_star-self.phi_xy,axis=0)\n",
      "    \n",
      "    def run(self):\n",
      "        f_grad = lambda theta: self.gradient(theta)\n",
      "        theta = super(StructuredClassifier,self).run( f_grad, b_norm=True)\n",
      "        classif_rule = greedy_policy(theta,self.phi,self.label_set)\n",
      "        return classif_rule,theta\n",
      "single_mu = lambda sa:feature_expectations[str(sa)]\n",
      "mu_E = non_scalar_vectorize(single_mu, (3,), (25,1))\n",
      "SCIRL = StructuredClassifier(sa, 2, mu_E, 25, ACTION_SPACE)\n",
      "void,theta_SCIRL = SCIRL.run()\n",
      "#Evaluation de SCIRL\n",
      "SCIRL_reward = lambda sas:dot(theta_SCIRL.transpose(),psi(sas[:2]))[0]\n",
      "vSCIRL_reward = non_scalar_vectorize( SCIRL_reward, (5,),(1,1) )\n",
      "data_LSPI = mountain_car_training_data(freward=SCIRL_reward)\n",
      "policy_CSI,omega_CSI = lspi( data_LSPI, s_dim=2,a_dim=1, A=ACTION_SPACE, phi=mountain_car_phi, phi_dim=75, iterations_max=20 )#None,zeros((75,1))#\n",
      "savetxt(\"data/SCIRL_omega_\"+str(NB_SAMPLES)+\"_\"+RAND_STRING+\".mat\",omega_CSI)\n",
      "s=TRANS[:,:2]\n",
      "a=TRANS[:,2]\n",
      "s_dash=TRANS[:,3:5]\n",
      "a_dash=TRANS[:,5]\n",
      "sa=TRANS[:,:3]\n",
      "sa_dash=TRANS[:,3:6]\n",
      "\n",
      "##CSI\n",
      "#Classification\n",
      "from sklearn import svm\n",
      "clf = svm.SVC(C=10, probability=True)\n",
      "clf.fit(s, a)\n",
      "import pickle\n",
      "RAND_STRING=str(int(rand()*100000))\n",
      "with open('data/Classif_'+str(NB_SAMPLES)+\"_\"+RAND_STRING+\".obj\", 'wb') as output:\n",
      "    pickle.dump(clf, output, pickle.HIGHEST_PROTOCOL)\n",
      "clf_predict= lambda state : clf.predict(squeeze(state))\n",
      "vpredict = non_scalar_vectorize( clf_predict, (2,), (1,1) )\n",
      "pi_c = lambda state: vpredict(state).reshape(state.shape[:-1]+(1,))\n",
      "clf_score = lambda sa : squeeze(clf.predict_proba(squeeze(sa[:2])))[sa[-1]]\n",
      "vscore = non_scalar_vectorize( clf_score,(3,),(1,1) )\n",
      "q = lambda sa: vscore(sa).reshape(sa.shape[:-1])\n",
      "#Donn\u00e9es pour la regression\n",
      "column_shape = (len(TRANS),1)\n",
      "s = TRANS[:,0:2]\n",
      "a = TRANS[:,2].reshape(column_shape)\n",
      "sa = TRANS[:,0:3]\n",
      "s_dash = TRANS[:,3:5]\n",
      "a_dash = pi_c(s_dash).reshape(column_shape)\n",
      "sa_dash = hstack([s_dash,a_dash])\n",
      "hat_r = (q(sa)-GAMMA*q(sa_dash)).reshape(column_shape)\n",
      "r_min = min(hat_r)-1.*ones(column_shape)\n",
      "##Avec l'heuristique : \n",
      "regression_input_matrices = [hstack([s,action*ones(column_shape)]) for action in ACTION_SPACE] \n",
      "def add_output_column( reg_mat ):\n",
      "    actions = reg_mat[:,-1].reshape(column_shape)\n",
      "    hat_r_bool_table = array(actions==a)\n",
      "    r_min_bool_table = array(hat_r_bool_table==False) #\"not hat_r_bool_table\" does not work as I expected\n",
      "    output_column = hat_r_bool_table*hat_r+r_min_bool_table*r_min\n",
      "    return hstack([reg_mat,output_column])\n",
      "regression_matrix = vstack(map(add_output_column,regression_input_matrices))\n",
      "#R\u00e9gression\n",
      "from sklearn.svm import SVR\n",
      "y = regression_matrix[:,-1]\n",
      "X = regression_matrix[:,:-1]\n",
      "reg = SVR(C=1.0, epsilon=0.2)\n",
      "reg.fit(X, y)\n",
      "CSI_reward = lambda sas:reg.predict(sas[:3])[0]\n",
      "vCSI_reward = non_scalar_vectorize( CSI_reward, (5,),(1,1) )\n",
      "#Evaluation de CSI\n",
      "data_LSPI = mountain_car_training_data(freward=CSI_reward)\n",
      "policy_CSI,omega_CSI = lspi( data_LSPI, s_dim=2,a_dim=1, A=ACTION_SPACE, phi=mountain_car_phi, phi_dim=75, iterations_max=20 )#None,zeros((75,1))#\n",
      "savetxt(\"data/CSI_omega_\"+str(NB_SAMPLES)+\"_\"+RAND_STRING+\".mat\",omega_CSI)\n",
      "def mountain_car_episode_length(initial_position,initial_speed,policy):\n",
      "    answer = 0\n",
      "    reward = 0.\n",
      "    state = array([initial_position,initial_speed])\n",
      "    while answer < 1500 and reward == 0. :\n",
      "        action = policy(state)\n",
      "        next_state = mountain_car_next_state(state,action)\n",
      "        reward = mountain_car_reward(hstack([state, action, next_state]))\n",
      "        state=next_state\n",
      "        answer+=1\n",
      "    return answer\n",
      "\n",
      "def mountain_car_tricky_episode_length(policy):\n",
      "    return mountain_car_episode_length(-0.9,0,policy)\n",
      "\n",
      "print \"Nb_samples :\\t\"+str(NB_SAMPLES)\n",
      "print \"Length CSI :\\t\"+str(mountain_car_tricky_episode_length(policy_CSI))\n",
      "print \"Length Class :\\t\"+str(mountain_car_tricky_episode_length(pi_c))\n",
      "print \"Length exp :\\t\"+str(mountain_car_tricky_episode_length(policy))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "LSPI, iter :1, diff : 23.157307563\n",
        "LSPI, iter :2, diff : 7.60232188082"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "LSPI, iter :3, diff : 0.0338270022259"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "LSPI, iter :4, diff : 4.25094607795e-06"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Nb_samples :\t10"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Length CSI :\t300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Length Class :\t300"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Length exp :\t55"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 1
    }
   ],
   "metadata": {}
  }
 ]
}