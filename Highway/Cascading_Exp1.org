#+TITLE: Cascading IRL on the Highway

On compare les performences des différentes configurations de notre algo : 
 - Features naturelles (N) ou features informatives (I)
 - Utilisation de $a'_i$ (E) ou de $\pi_C(s')$ (C)

Soit 4 combinaisons en tout : NE, NC, IE et IC. Un seul script qui prend en argument le fichier de données et affiche le nombre de samples et les performances sur la sortie standard, sur une seule ligne, séparés par des tabulations. Dans l'ordre : NE, NC, IE et IC. 
* Computation
#+begin_src python :tangle Cascading_Exp1.py
import sys
sys.path+=['..']
from numpy import *
import scipy
import Cascading
import Highway

D_E = genfromtxt( sys.argv[1] )
Data = map( lambda trans: [trans[0:4],trans[4:5][0],trans[5:9],trans[10:11][0]], D_E ) #s,a,s',eoe 
theta_NE,theta_NC = Cascading.run( Data, Highway.psi, Highway.phi, Highway.A )
theta_IE,theta_IC = Cascading.run( Data, Highway.psi2, Highway.phi2, Highway.A )

perf_NE = Highway.evaluate_theta( theta_NE, Highway.psi )
perf_NC = Highway.evaluate_theta( theta_NC, Highway.psi )
perf_IE = Highway.evaluate_theta( theta_IE, Highway.psi2 )
perf_IC = Highway.evaluate_theta( theta_IC, Highway.psi2 )

print "%d\t%lf\t%lf\t%lf\t%lf"%(D_E.shape[0], perf_NE, perf_NC, perf_IE, perf_IC )

#+end_src

#+srcname: Cascading_Exp1_make
#+begin_src makefile
Cascading_Exp1.py: Cascading_Exp1.org
	$(call tangle,"Cascading_Exp1.org")

LM_VALUES="3 3" "5 5" "7 7" "10 10" "15 15" "20 20"
Cascading_Exp1: Cascading_Exp1.py Highway.py ../DP.py Mu_E.mat ../Cascading.py ../LAFEM.py DE.py ../a2str.py
	for i in $(LM_VALUES); \
	do \
	python DE.py $$i > DE.mat; \
	python Cascading_Exp1.py DE.mat >> Cascading_Exp1.mat; \
	done


../Cascading.py:
	make -C .. Cascading.py

#+end_src

* Plotting
#+begin_src python :tangle Cascading_Exp1_plot.py
from matplotlib import rc
rc('text', usetex=True)
import sys
sys.path+=['..']
from numpy import *
import scipy
import pylab as pylab
from Plot import *

D_ANIRL = genfromtxt( "ANIRL_Exp6.mat" )
D_Cascading = genfromtxt( "Cascading_Exp1.mat" )
D_NE = D_Cascading[:,[0,1]]
#D_NC = D_Cascading[:,[0,2]]
D_IE = D_Cascading[:,[0,3]]
#D_IC = D_Cascading[:,[0,4]]
[XA,Y_meanA,Y_minA,Y_maxA] = map( array, mean_min_max( D_ANIRL ))
[XA,Y_meanA,Y_varA] = map( array, mean_var( D_ANIRL ))
[XNE,Y_meanNE,Y_minNE,Y_maxNE] = map( array, mean_min_max( D_NE ))
[XNE,Y_meanNE,Y_varNE] = map( array, mean_var( D_NE ))
#[XNC,Y_meanNC,Y_minNC,Y_maxNC] = mean_min_max( D_NC )
[XIE,Y_meanIE,Y_minIE,Y_maxIE] = map( array,mean_min_max( D_IE ))
[XIE,Y_meanIE,Y_varIE] = map( array,mean_var( D_IE ))
#[XIC,Y_meanIC,Y_minIC,Y_maxIC] = mean_min_max( D_IC )

Expert = 7.74390968*ones(XA.shape) #python Expert.py to get this value
Random_mean = -1.5821833963484*ones(XA.shape)#See Highway.org about Random.py for information on these values
Random_min = -4.0007295890199996*ones(XA.shape)#python Random.py to get this value
Random_max = 2.7064859345599999*ones(XA.shape)#python Random.py to get this value
Random_var = 1.4465398450419833*ones(XA.shape)#python Random.py to get this value
x_min = min(XA)
x_max = max(XA)
y_max = 9.5 #Comme ca l'expert peut respirer
y_min = min( Random_min[0],min(Y_minA), min(Y_minNE), min(Y_minIE) ) - 0.2

#First plot, Cascading with natural features, alone
pylab.figure(1)
pylab.clf()
pylab.axis([x_min,x_max,y_min,y_max])
pylab.xlabel('Number of samples from the expert')
pylab.ylabel('${1\over card(S)}\sum\limits_{s\in S}V(s)$')
pylab.grid(True)
filled_mean_min_max( pylab, XNE, Y_meanNE, Y_minNE, Y_maxNE, 'red', 0.2,'--',"Cascading IRL",None)
filled_mean_min_max( pylab, XNE, Y_meanNE, Y_meanNE - Y_varNE, Y_meanNE + Y_varNE, 'red', 0.4,'-.' ,None,None)
filled_mean_min_max( pylab, XNE, Random_mean, Random_min, Random_max, 'cyan',0.2,'--',"Agent trained on a random reward",None)
filled_mean_min_max( pylab, XNE, Random_mean, Random_mean-Random_var, Random_mean+Random_var, 'cyan',0.4,'-.',None,None)
pylab.plot(XA,Expert, color='blue',label="Expert",lw=2)
pylab.savefig('Cascading_Exp1_fig1.pdf')

#Second plot, Cascading avec les features informatives, seul
pylab.figure(2)
pylab.clf()
pylab.axis([x_min,x_max,y_min,y_max])
pylab.xlabel('Number of samples from the expert')
pylab.ylabel('${1\over card(S)}\sum\limits_{s\in S}V(s)$')
pylab.grid(True)
filled_mean_min_max( pylab, XIE, Y_meanIE, Y_minIE, Y_maxIE, 'red', 0.2,'--',"Cascading IRL",None)
filled_mean_min_max( pylab, XIE, Y_meanIE, Y_meanIE-Y_varIE, Y_meanIE+Y_varIE, 'red', 0.4,'-.',None,None)
filled_mean_min_max( pylab, XNE, Random_mean, Random_min, Random_max, 'cyan',0.2,'--',"Agent trained on a random reward",None)
filled_mean_min_max( pylab, XNE, Random_mean, Random_mean-Random_var, Random_mean+Random_var, 'cyan',0.4,'-.',None,None)
pylab.plot(XA,Expert, color='blue',label="Expert",lw=2)
pylab.savefig('Cascading_Exp1_fig2.pdf',transparent=True)

#Third plot, ANIRL avec les features informatives, seul
#Second plot, Cascading avec les features informatives, seul
pylab.figure(3)
pylab.clf()
pylab.axis([x_min,x_max,y_min,y_max])
pylab.xlabel('Number of samples from the expert')
pylab.ylabel('${1\over card(S)}\sum\limits_{s\in S}V(s)$')
pylab.grid(True)
filled_mean_min_max( pylab, XA, Y_meanA, Y_minA, Y_maxA, 'green', 0.2,'--',"Abbeel \& Ng IRL",None)
filled_mean_min_max( pylab, XA, Y_meanA, Y_meanA - Y_varA, Y_meanA + Y_varA, 'green', 0.4,'-.',None,None)
filled_mean_min_max( pylab, XNE, Random_mean, Random_min, Random_max, 'cyan',0.2,'--',"Agent trained on a random reward",None)
filled_mean_min_max( pylab, XNE, Random_mean, Random_mean-Random_var, Random_mean+Random_var, 'cyan',0.4,'-.',None,None)
pylab.plot(XA,Expert, color='blue',label="Expert",lw=2)
pylab.savefig('Cascading_Exp1_fig3.pdf',transparent=True)

#Fourth file : the legend
fig = pylab.figure(4)
figlegend = pylab.figure(figsize=(4,2))
ax = fig.add_subplot(111)
lines = ax.plot([-1,-1],[-1,-2], color='red',label="Cascading IRL",lw=2,linestyle='-')
lines += ax.plot([-1,-1],[-1,-2], color='green',label="Abbeen \& Ng IRL",lw=2,linestyle='-')
lines += ax.plot([-1,-1],[-1,-2], color='cyan',label="Agent trained on a random reward",lw=2,linestyle='-')
lines += ax.plot([-1,-1],[-1,-2], color='black',label="min, max",lw=1,linestyle='--')
lines += ax.plot([-1,-1],[-1,-2], color='black',label="Standard deviation",lw=1,linestyle='-.')
lines += ax.plot([-1,-1],[-1,-2], color='blue',label="Expert",lw=2,linestyle='-')
figlegend.legend(lines,("Cascading IRL","Abbeel \& Ng IRL","Agent trained on a random reward","min, max","Standard deviation","Expert"),"center")
figlegend.savefig("Cascading_Exp1_legend.pdf")

#+end_src
FIXME: Le plot concernat ANIRL devrait avoir son code dans ANIRL_Exp6.org et non ici.

#+srcname: Cascading_Exp1_make
#+begin_src makefile
Cascading_Exp1_plot.py: Cascading_Exp1.org
	$(call tangle,"Cascading_Exp1.org")

Cascading_Exp1_fig1.pdf: Cascading_Exp1_plot.py Cascading_Exp1.mat ../Plot.py
	python Cascading_Exp1_plot.py
Cascading_Exp1_fig2.pdf: Cascading_Exp1_plot.py Cascading_Exp1.mat ../Plot.py
	python Cascading_Exp1_plot.py
Cascading_Exp1_fig3.pdf: Cascading_Exp1_plot.py Cascading_Exp1.mat ../Plot.py
	python Cascading_Exp1_plot.py


../Plot.py:
	make -C .. Plot.py

#+end_src


* Stuff
  #+srcname: Cascading_Exp1_clean_make
  #+begin_src makefile
Cascading_Exp1_clean:
	find . -maxdepth 1 -iname "Cascading_Exp1.py"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "Cascading_Exp1.tex"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "Cascading_Exp1_*"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "Cascading_Exp1.mat"   | xargs $(XARGS_OPT) rm
  #+end_src
