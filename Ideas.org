* Sur la sparsité de la fonction de récompense
  - La sparsité de $\theta$ est une hypothèse valable si les features $\phi$ avec lesquelles on multiplie $\theta$ ont un sens. En construisant ses features, un humain aura tendance à leur donner du sens par exemple pour la conduite : position spatiale, détection de collision, respect du code de la route. Ces trois features portent l'intégralité de la récompense : on veut arriver à un certain endroit sans rien cogner et en restant dans les clous. Elles sont noyées au milieu d'autres infos : régime moteur, distances des autres véhicule, vitesse etc. De fait la récompense est sparse.
  - Lorsqu'on transforme l'espace des features (par exemple pour faire de la réduction de dimension, ou du tranfer learning) rien ne dit que le sens est préservé. Les features sparse (rarement rencontrées) qui forment la récompense vont même probablement être transformées les premières. De fait il est probable que la récompense ne sera pas sparse dans l'espace transformé.
  - Une récompense non sparse peut être la vraie récompense, à une transformation équivalent près. Est ce que la régularisation élimine cette récompense ou bien force la solution vers la récompense sparse équivalente ?
* Sur la sous optimalité de l'expert
  - Un expert ayant des défauts (ex : humain et temps de réaction lors de la conduite) peut être vu comme un expert optimal, et on peut faire passer ses défauts dans la récompense. Exemple : l'expert humain lent à la réaction va maximiser une récompense ou la lenteur de réaction est récompensée. Quand on entraîne le robot, il suffit ensuite d'enlever cette feature de l'espace d'état. Pas besoin de s'embêter avec un expert sous optimal.
