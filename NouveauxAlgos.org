#+OPTIONS: LaTeX:dvipng

#+EXPORT_EXCLUDE_TAGS: code
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \usepackage{natbib}
#+TITLE:Deux nouveaux algos d'IRL
[TABLE-OF-CONTENTS]

* Rappel sur le RL et l'IRL
** RL
  L'apprentissage par renforcement (/Reinforcement Learning/, RL) consiste, à partir d'une fonction de récompense définie par exemple sur l'espace d'état $R : S \rightarrow \mathbb{R}$, à trouver la politique optimale $\pi^* : S\rightarrow A$ optimisant le cumul pondéré des récompenses sur le long terme.

  L'agent évolue dans un processus decisionnel de markov (/Markov Decision Process/, MDP) ; à chaque pas de temps $t$ il choisis l'action $a_t = \pi(s_t)$ dictée par sa politique et se retrouve en conséquence dans un état $s_{t+1}$ selon une loi de probabilité $p(s_{t+1}|s_t, a_t )$. Le cumul pondéré des récompenses sur le long terme si l'on suit une politique donnée à partir d'un certain état donne la fonction de valeur de cette politique : 
  \begin{equation}
  V^\pi(s) = E\left[\left.\sum_{t=0}^\infty \gamma^tR(s_t)\right|s_0 = s,\pi\right].
  \end{equation}
  Une politique est dite optimale lorsqu'à chaque état sa fonction de valeur est supérieure ou égale à la fonction de valeur de toute autre politique.

  Lorsque l'on cherche à résoudre le MDP, on va souvent faire appel à la fonction de qualité qui est en quelque sorte une fonction de valeur avec un degré de liberté en plus, celui du choix de la prochaine action :
  \begin{equation}
  Q^\pi(s,a) = E\left[\left.\sum_{t=0}^\infty \gamma^tR(s_t)\right|s_0 = s,a_0=a,\pi\right].
  \end{equation}


  L'approximation de la fonction de valeur est un thème central en RL, souvent on utilise un approximateur linéare utilisant un ensemble de fonction de base (/features/) $\psi : S\rightarrow \mathbb{R}^p$ ou $\phi : S\times A \rightarrow \mathbb{R}^k$ :
  \begin{eqnarray}
  \hat Q^\pi(s,a) &= \omega^T\phi(s,a),\\
  \hat V^\pi(s) &= \omega^T\psi(s).
  \end{eqnarray}


  Le but est d'arriver à approximer la fonction de valeur ou de qualité (notées $V^*$ et $Q^*$) d'une des politiques optimales. Bien qu'il puisse exister plusieurs politiques optimales, elles partagent par définition la même fonction de valeur et de qualité, que l'on appelle fonction de valeur optimale et fonction de qualité optimale. La politique optimale, notée $\pi^*$ dans le cadre du RL et $\pi_E$ dans le cadre de l'IRL est liée à la fonction de qualité par un mécanisme glouton :
\begin{equation}
  \pi^*(s) = \arg\max_{a} Q^*(s,a).
\end{equation}
** IRL
   L'apprentissage par renforcement inverse (/Inverse Reinforcement Learning/, IRL) consiste à retrouver la récompense qu'un expert agissant dans un MDP essaye d'optimiser. Pour reprendre les termes ci-dessus il s'agit de retrouver la récompense à partir de la politique optimale ou plus pragmatiquement à partir de trajectoires tirés selon celle-ci.

   Puisque ce que l'on recherche est une fonction, il est naturel de définir un approximateur linéaire et d'effectuer la recherche sur les paramètres de celui-ci :
   \begin{equation}
   \hat R(s) = \theta^T \psi(s).
   \end{equation}
  
   Lorsque l'on injecte cet approximateur dans la définition de $V$ ou $Q$ apparaît le terme $\mu$, défini sous cette notation pour la première fois par \citet{abbeel2004apprenticeship} et qui désigne l'éspérance pondérée des fonctions de base (/feature expectation/). Ce terme est présent (ou peut être retrouvé) dans la majorité des publications en IRL.
  \begin{eqnarray}
  V^\pi_\theta(s) &=& E\left[\left.\sum\limits_{t=0}^\infty \gamma^t\hat R(s_t)\right|s_0 = s,\pi\right]\\
  &=& E\left[\left.\sum\limits_{t=0}^\infty \gamma^t\theta^T \psi(s_t)\right|s_0 = s,\pi\right]\\
  &=& \theta^T E\left[\left.\sum\limits_{t=0}^\infty \gamma^t \psi(s_t)\right|s_0 = s,\pi\right]\\
  \label{mudef.eqn}
  &=& \theta^T\mu^\pi(s).
  \end{eqnarray}

  
  Le problème de l'IRL est mal posé : il existe plus d'une récompense pour laquelle le comportement de l'expert est optimal. Pire, la récompense uniformément nulle est l'une d'entre elle, et tous les comportements sont optimaux pour cette récompense. Pour cette raison, les algorithmes d'IRL vont chercher un vecteur de paramètres $\theta$ satisfaisant une contrainte exprimée bien souvent à l'aide de la /feature expectation/ de l'expert et de celle de l'agent, étant supposé que l'agent maximise la récompense définie par $\theta$. Le survey \citep{neu2009training} recense quelques-uns de ces critères. A des subtilités concernant la régularisation près, ces critères aboutissent quelques fois à rapprocher en norme les deux vecteurs $\mu^E(s_0)$ et $\mu^\pi(s_0)$ ; il s'agit d'imitation par le biais de l'IRL plutôt que d'IRL au sens strict.
* Nouveaux Algorithmes
** Résumé spécifique des publications de Ratliff
   \label{sdef.sec}
   Dans \citep{ratliff2006maximum}, les auteurs présentent une méthode d'IRL en posant le problème sous forme d'un /maximum margin classification problem/. La méthode utilisée pour résoudre ce problème peut être ramenée à un algorithme itératif dans la veine de \cite{abbeel2004apprenticeship}, comme cela est expliqué dans \cite{neu2009training}. Vu à un plus haut niveau d'abstraction, la classification a lieu dans l'espace des trajectoires : à un MDP (entrée à classifier) est associée une politique (classe).

   Une seconde publication, \citep{ratliff2007boosting} vient compléter la précédente en incluant du /boosting/. On y voit apparaître à nouveau une fonction de coût empirique (en omettant quelques détails comme par exemple la pondération des trajectoires) : 
   \begin{equation}
   \label{dix.eqn}%On l'appelle comme ça parce que dans le draft et dans le papier de Ratliff elles ont le même numéro : 10.
   {1\over n}\sum\limits_{i=1}^n \left(\theta^T\mu^E_i - \min_\pi\left(\theta^T \mu^\pi - l(\pi)\right)\right) + {\lambda \over 2} ||\theta||^2.
   \end{equation}
  
   On constate la présence d'une fonction de coût $l$ jugeant de la similitude entre la solution proposée et celle démontrée par l'expert et permettant au passage d'inclure dans le problème de l'information à priori s'il y en a de disponible.

   La recherche a lieu dans l'espace des politiques via une méthode de descente de gradient sur les paramètres $\theta$ définissant la récompense, ce qui implique la résolution d'un MDP à chaque itération. En effet, le $\min\limits_\pi(\dots)$ présent dans la fonction de coût est analogue au $\pi^* = \arg\max\limits_\pi V^\pi$ décrivant la résolution classique d'un MDP.

   Finalement, dans \citep{ratliff2007imitation} les auteurs utilisent un système de classification multi-classe qui met en jeu une fonction de score. Cette fonction de score $s : X\times Y \rightarrow \mathbb{R}$ est utilisée à chaque entrée pour prendre une décision de classification i.e. choisir le label $y\in Y$ à appliquer à l'entrée $x\in X$ :
   \begin{equation}
   \label{sdef.eqn}
   y^* = \arg\max_{y \in Y} s(x,y)
   \end{equation}
   Cette fonction de score est apprise dans l'espace fonctionnel en optimisant la fonction suivante, dont celle donnée en \ref{dix.eqn} est une instanciation : 
   \begin{equation}
   \label{rdef.eqn}
   r[s] = {1\over N} \sum_{i=1}^N\left(\max_{y\in Y}(s(x_i,y) + l(x_i,y)) - s(x_i,y_i) \right)
   \end{equation}
   Cela suppose l'accès à une base de données $\{(x_i,y_i)\}_i$; i.e. une trace de l'expert. On note dans ce papier un changement par rapport aux papiers précédent : les entrées sont maintenant les états et les classes représentent les actions.
** Principe commun aux deux nouveaux algorithmes
   Les deux nouveaux algorithmes se basent sur une supposition quant à la fonction de score $s$. Nous allons supposer que cette fonction capture la structure du MDP. En effet l'équation \ref{sdef.eqn} est similaire à :
  \begin{equation}
  \label{greedy.eqn}
  \pi^*(s) = \arg\max_{a} Q^*(s,a)
  \end{equation}
  qui décrit le mécanisme glouton liant fonction de valeur optimale et politique optimale. Cela nous permet de prendre la fonction de qualité de la politique optimale $Q^*$, fonction qui en IRL est inconnue puisqu'elle dépend très étroitement de la récompense, comme candidat pour $s$ lorsque nous instancions l'équation \ref{rdef.eqn}.

  
  Les deux algorithmes, comme certains autres algorithmes récents, fonctionnent en appliquant des techniques d'apprentissage supervisé développées par ailleurs et en trouvant un moyen d'introduire de l'information relative à la structure du MDP.
** Loss-augmented feature expectation matching (LAFEM ?)
*** Principe
    En se basant sur le principe énoncé ci dessus, nous allons minimiser la fonction de risque énoncée par \citep{ratliff2007imitation} et reproduite équation \ref{rdef.eqn} et que nous réécrivons par soucis de cohérence de notations :
   \begin{equation}
   R_N(q) = {1\over N} \sum_{i=1}^N\left(\max_{a}(q(s_i,a) + l(s_i,a)) - q(s_i,a_i) \right)
   \end{equation}

   Pour extraire la logique derrière cette équation, considérons que la fonction de perte $l$ est uniformément nulle et regardons ce qui se passe lorsque la fonction de risque $R_N(q)$ est minimisée. On introduit $a^* = \arg\max\limits_a(q(s_i,a))$. Si la fonction de risque $R_N(q)$ est positive, alors il arrive pour certains $i$ que $q(s_i,a^*) > q(s_i,a_i)$. Cela signifie que la fonction $q$ ne permet pas de justifier le choix de l'expert. A l'opposé, si l'on parvient à anuller $R_N(q)$, on a $\forall i, q(s_i,a^*) = q(s_i,a_i)$, et donc $a_i \in \arg\max\limits_a q(s_i,a)$, ce qui justifie le choix de l'expert.\\

   La fonction de perte $l$ permet d'introduire de la connaissance à priori en laissant l'opérateur influer sur la forme de la fonction $q$ recherchée.\\

   Nous allons profiter de la structure présente dans $q$ pour introduire la notion de /feature expectation/ dans le problème. On rappelle qu'en IRL il est classique d'écrire $Q(s,a) = \theta^T \mu(s,a)$. Nous nous basons donc sur la /feature expectation/ de l'expert, $\mu_E$ et un vecteur de paramètres $\theta$ contrôlant la fonction de récompense que l'on recherche pour réécrire la fonction de risque que nous allons minimiser :
  \begin{equation}
   R_N(\theta) = {1\over N} \sum_{i=1}^N\left(\max_{a}(\theta ^T \mu_E(s_i,a) + l(s_i,a)) - \theta ^T \mu_E(s_i,a_i) \right)
   \end{equation}
   Il faut noter ici l'utilisation non pas de $\mu_E : S\rightarrow \mathbb{R}^p$ mais de $\mu_E : S\times A \rightarrow \mathbb{R}^k$. Cette dernière partage avec la première la même relation que $Q$ partage avec $V$, c'est à dire que l'on dispose d'un degré de liberté en plus correspondant à la prochaine action, qui peut différer de celle choisie par l'expert.

   
   De la même manière que précedemment, on note $a^* = \arg\max\limits_{a} = \theta_t ^T \mu_E(s_i,a) + l(s_i,a)$, l'expression de la fonction de risque devient alors :
   \begin{eqnarray}
   R_N(\theta) &=& {1\over N} \sum_{i=1}^N\left(\theta ^T \mu_E(s_i,a^*) + l(s_i,a^*) - \theta ^T \mu_E(s_i,a_i) \right)\\
   &=& {1\over N} \sum_{i=1}^N\left(\theta^T\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right) + l(s_i,a^*)  \right).
   \end{eqnarray}

   La présence d'un $\max$ dans cette expression (caché dans le $a^*$) nous oblige à utiliser le sous gradient (FIXME: Trouver une ref précise en regardant qui Ratliff cite et mettre quelque chose de plus rigoureux que ce qui suit) dont la règle est que le sous gradient du $\max$ est le gradient de l'$\arg\max$.

   Le gradient de cette expression est : 
   \begin{equation}
   \nabla L_N(\theta) =  {1\over N}\sum_{i=1}^N\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right).
   \end{equation}

   Une descente de gradient sur le risque donne la règle d'update suivante :
   \begin{equation}
   \theta_{t+1} = \theta_t -\alpha_t{1\over N}\sum_{i=1}^N\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right),
   \end{equation}

   avec $\alpha_t$ un pas d'apprentissage.

   FIXME: Dans le futur, il faudrait voir ce que ça donne si on minimise non plus le risque $R_N(q)$ mais le risque au carré.
*** Algorithme détaillé
   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$
   - un vecteur de paramètres initial $\theta_0$
   - un nombre maximum d'itérations $T\in \mathbb N$
   - une description de l'espace d'action $A$

     
   L'algorithme se déroule comme suit : 
   - Pour le nombre d'itérations prévues :
     - Initialiser $\Delta\theta \leftarrow [0...0]^T$
     - Pour chacun des points de données
       - Déterminer $a^* = \arg\max\limits_{a} = \theta_t ^T \mu_E(s_i,a) + l(s_i,a)$
       - $\Delta\theta \leftarrow \Delta\theta + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$
     - Effectuer la mise à jour $\theta_{t+1} = \theta_t -\alpha_t{1\over N}\Delta\theta$
   - retourner $\theta_T$
*** Commentaires
    L'expérience montre que la plus simple des fonctions $l$, celle qui vaut $1$ lorsqu'on est en désaccord avec l'expert et $0$ sinon donne de bons résultats.

    Les premiers tests sont très encourageants, l'algorithme parvient à résoudre le problème discret classique du /GridWorld/, ainsi qu'un problème continu (on ne travaille alors non plus sur la politique de l'expert mais sur des échantillons tirés grâce à celle-ci) simple, celui du pendule inversé. Dans le premier cas, la /feature expectation/ est calculée de manière exacte grâce à un algorithme de programmation dynamique, dans le second grâce à une simulation de Monte-Carlo. La prochaine étape consiste à introduire notre précédent algorithme, LSTD$\mu$, afin de réaliser le calcul de $\mu_E$ de manière /batch/, permettant ainsi à l'algorithme de fonctionner avec comme unique entrée des données de l'expert.

    Un autre avantage de notre algorithme est de ne pas nécessiter la résolution du problème direct, point d'étranglement lors de l'analyse de la complexité (aussi bien en termes d'échantillons que de calculs) des algorithmes existants.

** Hoping $s$ is Bellman compatible (HSIBC ?)
*** Principe
   La seconde technique à laquelle nous avons pensé mettrait en jeu une méthode de classification multi-classe permettant de trouver une fonction de score telle que décrite en section \ref{sdef.sec}.\\

   Afin d'introduire la structure du MDP dans l'algorithme, nous allons à nouveau supposer que cette fonction de score est cohérente avec la fonction de qualité $Q_E$ de l'expert, qui est une fonction de qualité optimale vis-à-vis de la récompense inconnue que nous cherchons. De fait, $Q_E$ vérifie l'équation \ref{greedy.eqn}, elle même similaire à l'équation \ref{sdef.eqn} qui caractérise la fonction de score considérée.\\

   Pour une politique $\pi$ donnée, il est aisé de passer de la fonction de qualité à la fonction de valeur, en effet :
   \begin{equation}
   \label{Q2V.eqn}
   V^\pi(\cdot) = Q^\pi(\cdot,\pi(\cdot)).
   \end{equation}

  En partant de l'équation de Bellman définissant la fonction de valeur optimale : 
  \begin{equation}
  \label{BellmanStart.eqn}
  V^* = R + \gamma P_{\pi^*}V^*
  \end{equation}
  on trouve facilement 
  \begin{equation}
  R = (Id - \gamma P_{\pi^*})V^*
  \end{equation}
  Si l'on quitte la forme matricielle, on obtient pour chaque $(s_i,a_i)$ de la base d'exemple :
  \begin{eqnarray}
  R(s_i) &=& \sum_{s'}\left[\left(I(s_i,s') - \gamma p(s'|s_i,\pi^*)\right)V^*(s')\right]\\
  &=& \sum_{s'}\left[I(s_i,s')V^*(s') - \gamma p(s'|s_i,\pi^*)V^*(s')\right]\\
  &=& \sum_{s'}I(s_i,s')V^*(s') - \gamma \sum_{s'}p(s'|s_i,\pi^*)V^*(s')\\
  &=& V^*(s_i) - \gamma E[V^*(s_{i+1})|s_i,\pi^*]\\
  \end{eqnarray}

  L'on introduit alors la fonction de score $q$ fournie par la méthode de classification, en considérant qu'il s'agit d'un bon candidat pour approximer la fonction de qualité de l'expert $Q_E$ (qui, rappelons le est une fonction de qualité optimale et de ce fait respecte l'équation de Bellman \ref{BellmanStart.eqn} dont nous sommes partis). En utilisant de plus l'équation \ref{Q2V.eqn} on obtient alors :
  \begin{equation}
  \label{29.eqn} %lame name
  R(s_i) = q(s_i,a_i) - \gamma E[q(s_{i+1},a_{i+1})|s_i,\pi_E].
  \end{equation}
  Nous disposon d'un estimateur non biaisé de $E[q(s_{i+1},a_{i+1})|s_i,\pi_E]$ en allant simplement chercher dans la base d'exemple les données $(s_{i+1},a_{i+1})$. Finalement, on obtient une base de données d'entraînement pour la récompense $(s_i,R(s_i))$ en calculant :
  \begin{equation}
  \hat R(s_i) = q(s_i,a_i) - \gamma q(s_{i+1},a_{i+1}).
  \end{equation}
  Il ne reste plus alors qu'à employer une méthode de régression permettant de généraliser ces exemples à tout l'espace d'état, et nous possédons une représentation d'une récompense compatible avec le comportement de l'expert.

*** Instanciation avec une architecture linéaire

    Si l'on suppose que la méthode de classification employée maintient une représentation de $q$ linéaire de la forme 
\begin{equation}
q(s,a) = \theta^T \mu_E(s,a),
\end{equation}
c'est à dire utilisant la /feature expectation/ de l'expert en tant que fonctions de bases, alors il est intéressant de repartir de l'équation \ref{29.eqn} :
\begin{eqnarray}
  R(s_i) &=& q(s_i,a_i) - \gamma E[q(s_{i+1},a_{i+1})|s_i,\pi_E]\\
  &=& \theta^T \mu_E(s_i,a_i)- \gamma E[\theta^T\mu_E(s_{i+1},a_{i+1})|s_i,\pi_E]\\
  &=& \theta^T (\mu_E(s_i,a_i)- \gamma E[\mu_E(s_{i+1},a_{i+1})|s_i,\pi_E]
\end{eqnarray}
La définition de la /feature expectation/ $\mu$ (equation \ref{mudef.eqn}) peut se réécrire :
\begin{equation}
\mu_E(s_t,a_t) = \phi(s_t) + \gamma E[\mu_E(s_{t+1},a_{t+1})|s_t,a_t,\pi_E].
\end{equation}
Il suffit d'injecter cette dernière équation dans la précédente et l'on obtient :
\begin{equation}
R(s_i) = \theta^T \phi(s_i).
\end{equation}
Cela signifie qu'en choisissant les bonnes fonctions de base l'on arrive directement à une représentation pratique de la récompense.

\bibliographystyle{plainnat}
\bibliography{../Biblio/Biblio}

* Implémentation python de LAFEM 				       :code:
  :PROPERTIES:
  :ID:       879A40A3-5890-4665-86C0-826ABD3BC1BC
  :END:
   L'algorithme est implémenté dans une classe abstraite en python. Les éléments nécessaires à l'algorithme sont définis comme des fonctions abstraites, ce qui fait qu'il est impossible de créer ou d'utiliser cette classe sans la sous classer en définissant les éléments nécessaires.

#+begin_src python :tangle LAFEM.py
from numpy import *
import scipy.linalg
import os
import sys
class LAFEM:
   def __init__( self ):
      if self.__class__ == LAFEM:
         raise NotImplementedError, "Cannot create object of class LAFEM"
#+end_src

   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$, prenant deux vecteurs ligne $s$ et $a$ en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def l( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$, prenant deux vecteurs lignes $s$ et $a$ en argument, renvoyant un vecteur colonne
     #+begin_src python :tangle LAFEM.py
   def mu_E( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$, prenant un entier en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def alpha( self, t ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$, il faut que ce membre soit iterable et renvoie des couples de vecteurs ligne $(s,a)$
     #+begin_src python :tangle LAFEM.py
   data=[]
     #+end_src
   - un vecteur de paramètres initial $\theta_0$
     #+begin_src python :tangle LAFEM.py
   theta_0=array([])
     #+end_src
   - une valeur pour la norme du grandient en dessous de laquelle on stoppe l'algorithme
     #+begin_src python :tangle LAFEM.py
   Threshold = 'a'
     #+end_src
   - un nombre d'itérations maximum
     #+begin_src python :tangle LAFEM.py
   T = -1
     #+end_src
   - une description de l'espace d'action $A$, ce membre doit être itérable
     #+begin_src python :tangle LAFEM.py
   A=[]
     #+end_src

     
   L'algorithme se déroule comme suit : 
   - Initialiser $\theta_{t=0} \leftarrow \theta_0$, $\theta_T\leftarrow \theta_{t=0}$
   #+begin_src python :tangle LAFEM.py
   def run( self ):
      theta = self.theta_0.copy()
      best_theta = theta.copy()
      best_norm = 1000000.#FIXME:Il faudrait mettre plus l'infini
      best_iter = 0

   #+end_src
   - Pour le nombre d'itérations prévues :
     #+begin_src python :tangle LAFEM.py
      #for t in range(0,self.T):
      t=-1
      while True:#Do...while loop
         t += 1
     #+end_src
     - Initialiser $\Delta\theta \leftarrow [0...0]^T$
       #+begin_src python :tangle LAFEM.py
         DeltaTheta = zeros(( self.theta_0.size, 1 ))
       #+end_src
     - Pour chacun des points de données
       #+begin_src python :tangle LAFEM.py
         for sa in self.data:
       #+end_src
       - Déterminer $a^* = \arg\max\limits_{a} = \theta_t ^T \mu_E(s_i,a) + l(s_i,a)$
         #+begin_src python :tangle LAFEM.py
            val = -Inf
            a_star = array([])
            for a in self.A:
               newval = dot( theta.transpose(), self.mu_E( sa[0], a ) ) + self.l( sa[0], a )
               assert(newval.size == 1)
               if newval[0] > val:
                  val = newval
                  a_star = a

         #+end_src
       - $\Delta\theta \leftarrow \Delta\theta + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$	 
         #+begin_src python :tangle LAFEM.py
            DeltaTheta = DeltaTheta + self.mu_E( sa[0], a_star ) - self.mu_E( sa[0], sa[1] )
         #+end_src
     - Effectuer la mise à jour $\theta_{t+1} = \theta_t -\alpha_t {1\Delta\theta\over N ||\Delta\theta||_2}$
       #+begin_src python :tangle LAFEM.py
         DeltaTheta = DeltaTheta / len(self.data) #1/N
         norm = scipy.linalg.norm(DeltaTheta)
         theta = theta - self.alpha( t ) * DeltaTheta / norm
         sys.stderr.write("Norme du gradient : "+str(norm)+", pas : "+str(self.alpha(t))+", iteration : "+str(t)+"\n")
       #+end_src
     - Si $||\theta_{t+1}||_2 < ||\theta_T||_2$, effectuer $\theta_T \leftarrow\theta_{t+1}$
       #+begin_src python :tangle LAFEM.py
         if norm < best_norm:
             best_norm = norm
             best_theta = theta.copy()
             best_iter = t
         if norm < self.Threshold or (self.T != -1 and t >= self.T):
             break
       #+end_src
       
   - retourner $\theta_T$
     #+begin_src python :tangle LAFEM.py
      sys.stderr.write("Gradient de norme : "+str(best_norm)+", a l'iteration : "+str(best_iter)+"\n")
      return best_theta
     #+end_src
     
* Making this document :code:
This document can be compiled into a pdf.
#+srcname: NA_org2pdf_make
#+begin_src makefile
NouveauxAlgos.pdf: NouveauxAlgos.org
	$(call org2pdf,"NouveauxAlgos")
#+end_src

It can also be tangled.
#+srcname: NA_code_make
#+begin_src makefile
LAFEM.py: NouveauxAlgos.org
	$(call tangle,"NouveauxAlgos.org")
#+end_src
We provide a rule to clean the corresponding mess.
#+srcname: NA_clean_make
#+begin_src makefile
NA_clean:
	find . -maxdepth 1 -iname "NouveauxAlgos.aux"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.bbl"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.blg"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.tex"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.pdf"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.log"| xargs $(XARGS_OPT) rm 
	find . -maxdepth 1 -iname "NouveauxAlgos.toc"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "LAFEM.py" | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "LAFEM.pyc" | xargs $(XARGS_OPT) rm
#+end_src
