#+OPTIONS: LaTeX:dvipng

#+EXPORT_EXCLUDE_TAGS: code
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \usepackage{natbib}
#+TITLE:Deux nouveaux algos d'IRL
[TABLE-OF-CONTENTS]

* Rappel sur le RL et l'IRL
** RL
  L'apprentissage par renforcement (/Reinforcement Learning/, RL) consiste, à partir d'une fonction de récompense définie par exemple sur l'espace d'état $R : S \rightarrow \mathbb{R}$, à trouver la politique optimale $\pi^* : S\rightarrow A$ optimisant le cumul pondéré des récompenses sur le long terme.

  L'agent évolue dans un processus decisionnel de markov (/Markov Decision Process/, MDP) ; à chaque pas de temps $t$ il choisis l'action $a_t = \pi(s_t)$ dictée par sa politique et se retrouve en conséquence dans un état $s_{t+1}$ selon une loi de probabilité $p(s_{t+1}|s_t, a_t )$. Le cumul pondéré des récompenses sur le long terme si l'on suit une politique donnée à partir d'un certain état donne la fonction de valeur de cette politique : 
  \begin{equation}
  V^\pi(s) = E\left[\left.\sum_{t=0}^\infty \gamma^tR(s_t)\right|s_0 = s,\pi\right].
  \end{equation}
  Une politique est dite optimale lorsqu'à chaque état sa fonction de valeur est supérieure ou égale à la fonction de valeur de toute autre politique.

  Lorsque l'on cherche à résoudre le MDP, on va souvent faire appel à la fonction de qualité qui est en quelque sorte une fonction de valeur avec un degré de liberté en plus, celui du choix de la prochaine action :
  \begin{equation}
  Q^\pi(s,a) = E\left[\left.\sum_{t=0}^\infty \gamma^tR(s_t)\right|s_0 = s,a_0=a,\pi\right].
  \end{equation}


  L'approximation de la fonction de valeur est un thème central en RL, souvent on utilise un approximateur linéare utilisant un ensemble de fonction de base (/features/) $\psi : S\rightarrow \mathbb{R}^p$ ou $\phi : S\times A \rightarrow \mathbb{R}^k$ :
  \begin{eqnarray}
  \hat Q^\pi(s,a) &= \omega^T\phi(s,a),\\
  \hat V^\pi(s) &= \omega^T\psi(s).
  \end{eqnarray}


  Le but est d'arriver à approximer la fonction de valeur ou de qualité (notées $V^*$ et $Q^*$) d'une des politiques optimales. Bien qu'il puisse exister plusieurs politiques optimales, elles partagent par définition la même fonction de valeur et de qualité, que l'on appelle fonction de valeur optimale et fonction de qualité optimale. La politique optimale, notée $\pi^*$ dans le cadre du RL et $\pi_E$ dans le cadre de l'IRL est liée à la fonction de qualité par un mécanisme glouton :
\begin{equation}
  \pi^*(s) = \arg\max_{a} Q^*(s,a).
\end{equation}
** IRL
   L'apprentissage par renforcement inverse (/Inverse Reinforcement Learning/, IRL) consiste à retrouver la récompense qu'un expert agissant dans un MDP essaye d'optimiser. Pour reprendre les termes ci-dessus il s'agit de retrouver la récompense à partir de la politique optimale ou plus pragmatiquement à partir de trajectoires tirés selon celle-ci.

   Puisque ce que l'on recherche est une fonction, il est naturel de définir un approximateur linéaire et d'effectuer la recherche sur les paramètres de celui-ci :
   \begin{equation}
   \hat R(s) = \theta^T \psi(s).
   \end{equation}
  
   Lorsque l'on injecte cet approximateur dans la définition de $V$ ou $Q$ apparaît le terme $\mu$, défini sous cette notation pour la première fois par \citet{abbeel2004apprenticeship} et qui désigne l'éspérance pondérée des fonctions de base (/feature expectation/). Ce terme est présent (ou peut être retrouvé) dans la majorité des publications en IRL.
  \begin{eqnarray}
  V^\pi_\theta(s) &=& E\left[\left.\sum\limits_{t=0}^\infty \gamma^t\hat R(s_t)\right|s_0 = s,\pi\right]\\
  &=& E\left[\left.\sum\limits_{t=0}^\infty \gamma^t\theta^T \psi(s_t)\right|s_0 = s,\pi\right]\\
  &=& \theta^T E\left[\left.\sum\limits_{t=0}^\infty \gamma^t \psi(s_t)\right|s_0 = s,\pi\right]\\
  &=& \theta^T\mu^\pi(s).
  \end{eqnarray}

  
  Le problème de l'IRL est mal posé : il existe plus d'une récompense pour laquelle le comportement de l'expert est optimal. Pire, la récompense uniformément nulle est l'une d'entre elle, et tous les comportements sont optimaux pour cette récompense. Pour cette raison, les algorithmes d'IRL vont chercher un vecteur de paramètres $\theta$ satisfaisant une contrainte exprimée bien souvent à l'aide de la /feature expectation/ de l'expert et de celle de l'agent, étant supposé que l'agent maximise la récompense définie par $\theta$. Le survey \citep{neu2009training} recense quelques-uns de ces critères. A des subtilités concernant la régularisation près, ces critères aboutissent quelques fois à rapprocher en norme les deux vecteurs $\mu^E(s_0)$ et $\mu^\pi(s_0)$ ; il s'agit d'imitation par le biais de l'IRL plutôt que d'IRL au sens strict.
* Nouveaux Algorithmes
** Résumé spécifique des publications de Ratliff
   \label{sdef.sec}
   Dans \citep{ratliff2006maximum}, les auteurs présentent une méthode d'IRL en posant le problème sous forme d'un /maximum margin classification problem/. La méthode utilisée pour résoudre ce problème peut être ramenée à un algorithme itératif dans la veine de \cite{abbeel2004apprenticeship}, comme cela est expliqué dans \cite{neu2009training}. Vu à un plus haut niveau d'abstraction, la classification a lieu dans l'espace des trajectoires : à un MDP (entrée à classifier) est associée une politique (classe).

   Une seconde publication, \citep{ratliff2007boosting} vient compléter la précédente en incluant du /boosting/. On y voit apparaître à nouveau une fonction de coût empirique (en omettant quelques détails comme par exemple la pondération des trajectoires) : 
   \begin{equation}
   \label{dix.eqn}%On l'appelle comme ça parce que dans le draft et dans le papier de Ratliff elles ont le même numéro : 10.
   {1\over n}\sum\limits_{i=1}^n \left(\theta^T\mu^E_i - \min_\pi\left(\theta^T \mu^\pi - l(\pi)\right)\right) + {\lambda \over 2} ||\theta||^2.
   \end{equation}
  
   On constate la présence d'une fonction de coût $l$ jugeant de la similitude entre la solution proposée et celle démontrée par l'expert et permettant au passage d'inclure dans le problème de l'information à priori s'il y en a de disponible.

   La recherche a lieu dans l'espace des politiques via une méthode de descente de gradient sur les paramètres $\theta$ définissant la récompense, ce qui implique la résolution d'un MDP à chaque itération. En effet, le $\min\limits_\pi(\dots)$ présent dans la fonction de coût est analogue au $\pi^* = \arg\max\limits_\pi V^\pi$ décrivant la résolution classique d'un MDP.

   Finalement, dans \citep{ratliff2007imitation} les auteurs utilisent un système de classification multi-classe qui met en jeu une fonction de score. Cette fonction de score $s : X\times Y \rightarrow \mathbb{R}$ est utilisée à chaque entrée pour prendre une décision de classification i.e. choisir le label $y\in Y$ à appliquer à l'entrée $x\in X$ :
   \begin{equation}
   \label{sdef.eqn}
   y^* = \arg\max_{y \in Y} s(x,y)
   \end{equation}
   Cette fonction de score est apprise dans l'espace fonctionnel en optimisant la fonction suivante, dont celle donnée en \ref{dix.eqn} est une instanciation : 
   \begin{equation}
   \label{rdef.eqn}
   r[s] = {1\over N} \sum_{i=1}^N\left(\max_{y\in Y}(s(x_i,y) + l(x_i,y)) - s(x_i,y_i) \right)
   \end{equation}
   Cela suppose l'accès à une base de données $\{(x_i,y_i)\}_i$; i.e. une trace de l'expert. On note dans ce papier un changement par rapport aux papiers précédent : les entrées sont maintenant les états et les classes représentent les actions.
** Principe commun aux deux nouveaux algorithmes
   Les deux nouveaux algorithmes se basent sur une supposition quant à la fonction de score $s$. Nous allons supposer que cette fonction capture la structure du MDP. En effet l'équation \ref{sdef.eqn} est similaire à :
  \begin{equation}
  \pi^*(s) = \arg\max_{a} Q^*(s,a)
  \end{equation}
  qui décrit le mécanisme glouton liant fonction de valeur optimale et politique optimale. Cela nous permet de prendre la fonction de qualité de la politique optimale $Q^*$, fonction qui en IRL est inconnue puisqu'elle dépend très étroitement de la récompense, comme candidat pour $s$ lorsque nous instancions l'équation \ref{rdef.eqn}.

  
  Les deux algorithmes, comme certains autres algorithmes récents, fonctionnent en appliquant des techniques d'apprentissage supervisé développées par ailleurs et en trouvant un moyen d'introduire de l'information relative à la structure du MDP.
** Loss-augmented feature expectation matching (LAFEM ?)
*** Principe
    En se basant sur le principe énoncé ci dessus, nous allons minimiser la fonction de risque énoncée par \citep{ratliff2007imitation} et reproduite équation \ref{rdef.eqn} et que nous réécrivons par soucis de cohérence de notations :
   \begin{equation}
   L_N(q) = {1\over N} \sum_{i=1}^N\left(\max_{a}(q(s_i,a) + l(s_i,a)) - q(s_i,a_i) \right)
   \end{equation}

   Nous allons profiter de la structure présente dans $q$ pour introduire la notion de /feature expectation/ dans le problème. On rappelle qu'en IRL il est classique d'écrire $Q(s,a) = \theta^T \mu(s,a)$. Nous nous basons donc sur la /feature expectation/ de l'expert, $\mu_E$ et un vecteur de paramètres $\theta$ contrôlant la fonction de récompense que l'on recherche pour réécrire la fonction de risque que nous allons minimiser :
  \begin{equation}
   L_N(\theta) = {1\over N} \sum_{i=1}^N\left(\max_{a}(\theta ^T \mu_E(s_i,a) + l(s_i,a)) - \theta ^T \mu_E(s_i,a_i) \right)
   \end{equation}
   Il faut noter ici l'utilisation non pas de $\mu_E : S\rightarrow \mathbb{R}^p$ mais de $\mu_E : S\times A \rightarrow \mathbb{R}^k$. Cette dernière partage avec la première la même relation que $Q$ partage avec $V$, c'est à dire que l'on dispose d'un degré de liberté en plus correspondant à la prochaine action, qui peut différer de celle choisie par l'expert.

   
   On introduit $a^* = \arg\max\limits_{a} = \theta_t ^T \mu_E(s_i,a) + l(s_i,a)$, l'expression de la fonction de risque devient alors :
   \begin{eqnarray}
   L_N(\theta) &=& {1\over N} \sum_{i=1}^N\left(\theta ^T \mu_E(s_i,a^*) + l(s_i,a^*) - \theta ^T \mu_E(s_i,a_i) \right)\\
   &=& {1\over N} \sum_{i=1}^N\left(\theta^T\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right) + l(s_i,a^*)  \right).
   \end{eqnarray}

   Le gradent de cette expression est : 
   \begin{equation}
   \nabla L_N(\theta) =  {1\over N}\sum_{i=1}^N\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right).
   \end{equation}

   Une descente de gradient sur le risque donne la règle d'update suivante :
   \begin{equation}
   \theta_{t+1} = \theta_t -\alpha_t\sum_{i=1}^N\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right),
   \end{equation}

   avec $\alpha_t$ un pas d'apprentissage.



   La fonction $l$ joue ici un rôle similaire à celui décrit dans les publications de Ratliff : il s'agit de juger de la similitude entre le choix proposé et celui de l'expert mais à un échelon local, au niveau de la prochaine action.
*** Algorithme détaillé
   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$
   - un vecteur de paramètres initial $\theta_0$
   - un nombre maximum d'itérations $T\in \mathbb N$
   - une description de l'espace d'action $A$

     
   L'algorithme se déroule comme suit : 
   - Pour le nombre d'itérations prévues :
     - Initialiser $\Delta\theta \leftarrow [0...0]^T$
     - Pour chacun des points de données
       - Déterminer $a^* = \arg\max\limits_{a} = \theta_t ^T \mu_E(s_i,a) + l(s_i,a)$
       - $\Delta\theta \leftarrow \Delta\theta + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$
     - Effectuer la mise à jour $\theta_{t+1} = \theta_t -\alpha_t\Delta\theta$
   - retourner $\theta_T$
*** Commentaires
    L'expérience montre que la plus simple des fonctions $l$, celle qui vaut $1$ lorsqu'on est en désaccord avec l'expert et $0$ sinon donne de bons résultats.

    Les premiers tests sont très encourageants, l'algorithme parvient à résoudre le problème discret classique du /GridWorld/, ainsi qu'un problème continu (on ne travaille alors non plus sur la politique de l'expert mais sur des échantillons tirés grâce à celle-ci) simple, celui du pendule inversé. Dans le premier cas, la /feature expectation/ est calculée de manière exacte grâce à un algorithme de programmation dynamique, dans le second grâce à une simulation de Monte-Carlo. La prochaine étape consiste à introduire notre précédent algorithme, LSTD$\mu$, afin de réaliser le calcul de $\mu_E$ de manière /batch/, permettant ainsi à l'algorithme de fonctionner avec comme unique entrée des données de l'expert.

    Un autre avantage de notre algorithme est de ne pas nécessiter la résolution du problème direct, point d'étranglement lors de l'analyse de la complexité (aussi bien en termes d'échantillons que de calculs) des algorithmes existants.

** Hoping $s$ is Bellman compatible (HSIBC ?)

   La seconde technique à laquelle nous avons pensé mettrait en jeu une méthode de classification multi-classe permettant de trouver une fonction de score telle que décrite en section \ref{sdef.sec}.

  En partant de l'équation de Bellman définissant la fonction de valeur optimale : 
  \begin{equation}
  Q^* = R + \gamma P_{\pi^*}Q^*
  \end{equation}
  on trouve facilement 
  \begin{equation}
  R = Q^*(Id - \gamma P_{\pi^*})
  \end{equation}
  
  Ainsi, la fonction de score obtenue permet de construire une base de données $\{(s_i,R(s_i))\}_i$ se basant sur les mêmes points $\{s_i\}_i$ que ceux utilisés pour l'entraînement du classifieur, ou sur un maillage de l'espace d'état etc.

  A partir de cette base de données, on peut faire de la régression pour trouver une fonction de récompense $\hat R$.

\bibliographystyle{plainnat}
\bibliography{../Biblio/Biblio}

* Implémentation python de LAFEM :code:
  :PROPERTIES:
  :ID:       879A40A3-5890-4665-86C0-826ABD3BC1BC
  :END:
   L'algorithme est implémenté dans une classe abstraite en python. Les éléments nécessaires à l'algorithme sont définis comme des fonctions abstraites, ce qui fait qu'il est impossible de créer ou d'utiliser cette classe sans la sous classer en définissant les éléments nécessaires.

#+begin_src python :tangle LAFEM.py
from numpy import *
import os
class LAFEM:
   def __init__( self ):
      if self.__class__ == LAFEM:
         raise NotImplementedError, "Cannot create object of class LAFEM"
#+end_src

   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$, prenant deux vecteurs ligne $s$ et $a$ en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def l( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$, prenant deux vecteurs lignes $s$ et $a$ en argument, renvoyant un vecteur colonne
     #+begin_src python :tangle LAFEM.py
   def mu_E( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$, prenant un entier en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def alpha( self, t ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$, il faut que ce membre soit iterable et renvoie des couples de vecteurs ligne $(s,a)$
     #+begin_src python :tangle LAFEM.py
   data=[]
     #+end_src
   - un vecteur de paramètres initial $\omega_0$
     #+begin_src python :tangle LAFEM.py
   omega_0=array([])
     #+end_src
   - un nombre maximum d'itérations $T\in \mathbb N$
     #+begin_src python :tangle LAFEM.py
   T=-1
     #+end_src
   - une description de l'espace d'action $A$, ce membre doit être itérable
     #+begin_src python :tangle LAFEM.py
   A=[]
     #+end_src

     
   L'algorithme se déroule comme suit : 
   #+begin_src python :tangle LAFEM.py
   def run( self ):
      omega = self.omega_0.copy()
   #+end_src
   - Pour le nombre d'itérations prévues :
     #+begin_src python :tangle LAFEM.py
      for t in range(0,self.T):
     #+end_src
     - Initialiser $\Delta\omega \leftarrow [0...0]^T$
       #+begin_src python :tangle LAFEM.py
         DeltaOmega = zeros(( self.omega_0.size, 1 ))
       #+end_src
     - Pour chacun des points de données
       #+begin_src python :tangle LAFEM.py
         for sa in self.data:
       #+end_src
       - Déterminer $a^* = \arg\max\limits_{a} = \omega_t ^T \mu_E(s_i,a) + l(s_i,a)$
         #+begin_src python :tangle LAFEM.py
            val = -Inf
            a_star = array([])
            for a in self.A:
               newval = dot( omega.transpose(), self.mu_E( sa[0], a ) ) + self.l( sa[0], a )
               assert(newval.size == 1)
               if newval[0] > val:
                  val = newval
                  a_star = a
         #+end_src
       - $\Delta\omega \leftarrow \Delta\omega + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$	 
         #+begin_src python :tangle LAFEM.py
            DeltaOmega = DeltaOmega + self.mu_E( sa[0], a_star ) - self.mu_E( sa[0], sa[1] )
         #+end_src
     - Effectuer la mise à jour $\omega_{t+1} = \omega_t -\alpha_t\Delta\omega$
       #+begin_src python :tangle LAFEM.py
            omega = omega - self.alpha( t ) * DeltaOmega
       #+end_src
   - retourner $\omega_T$
     #+begin_src python :tangle LAFEM.py
      return omega
     #+end_src
     
* Making this document :code:
This document can be compiled into a pdf.
#+srcname: NA_org2pdf_make
#+begin_src makefile
NouveauxAlgos.pdf: NouveauxAlgos.org
	$(call org2pdf,"NouveauxAlgos")
#+end_src

It can also be tangled.
#+srcname: NA_code_make
#+begin_src makefile
LAFEM.py: NouveauxAlgos.org
	$(call tangle,"NouveauxAlgos.org")
#+end_src
We provide a rule to clean the corresponding mess.
#+srcname: NA_clean_make
#+begin_src makefile
NA_clean:
	find . -maxdepth 1 -iname "NouveauxAlgos.aux"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.bbl"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.blg"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.tex"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.pdf"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.log"| xargs $(XARGS_OPT) rm 
	find . -maxdepth 1 -iname "NouveauxAlgos.toc"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "LAFEM.py" | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "LAFEM.pyc" | xargs $(XARGS_OPT) rm
#+end_src
