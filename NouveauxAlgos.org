#+OPTIONS: LaTeX:dvipng

#+EXPORT_EXCLUDE_TAGS: code
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \usepackage{natbib}
#+TITLE:Deux nouveaux algos d'IRL
[TABLE-OF-CONTENTS]

* Ce qu'on tire de Ratliff
  \label{sdef.sec}
  Dans \citep{ratliff2007imitation}, les auteurs utilisent un système classification multi classe qui met en jeu une fonction de score. Cette fonction de score $s : X\times Y \rightarrow \mathbb{R}$ est utilisée à chaque entrée pour prendre une décision de classification i.e. choisir le label $y\in Y$ à appliquer à l'entrée $x\in X$ :
  \begin{equation}
  \label{sdef.eqn}
  y^* = \arg\max_{y \in Y} s(x,y)
  \end{equation}
  Cette fonction de score est apprise dans l'espace fonctionnel en optimisant la fonction suivante : 
  \begin{equation}
  r[s] = {1\over N} \sum_{i=1}^N\left(\max_{y\in Y}(s(x_i,y) + l(x_i,y)) - s(x_i,y_i) \right)
  \end{equation}
  Cela suppose l'accès à une base de données $\{(x_i,y_i)\}_i$; i.e. une trace de l'expert.
* Hoping $s$ is Bellman compatible (HSIBC ?)
  La fonction de score $s$ joue le même rôle que la fonction $Q^*$, en effet l'equation \ref{sdef.eqn} est similaire à :
  \begin{equation}
  \pi^*(s) = \arg\max_{a} Q^*(s,a)
  \end{equation}
  qui décrit le mécanisme glouton liant fonction de valeur optimale et politique optimale.
  
  En appliquant une méthode de classification multi-classe permettant de trouver une fonction de score telle que décrite en section \ref{sdef.sec} (FIXME: Demander à Yann), on peut potentiellement arriver à retrouver la récompense.

  En partant de l'équation de Bellman définissant la fonction de valeur optimale : 
  \begin{equation}
  Q^* = R + \gamma PQ^*
  \end{equation}
  on trouve facilement 
  \begin{equation}
  R = Q^*(Id - \gamma P)
  \end{equation}
  
  Le mécanisme de classification utilisé fournit une fonction de score dont la structure importe peu mais qui permet sans doute de construire une base de données $\{(s_i,R(s_i))\}_i$ se basant sur les mêmes points $\{s_i\}_i$ que ceux utilisés pour l'entraînement du classifieur, ou sur un maillage de l'espace d'état etc. A partir de cette base de données, on peut faire de la régression pour trouver une fonction de récompense $\hat R$ ensuite utilisée dans un algorithme d'apprentissage par renforcement direct.


  Il convient probablement d'entraîner le classifieur sur des données ayant une structure temporelle (des trajectoires) afin que la fonction de score capture cette structure.
* Loss-augmented feature expectation matching (LAFEM ?)
** Principe
  Toujours en se basant sur la ressemblance entre $s$ et $Q^*$, on peut choisir de directement optimiser la fonction :
  \begin{equation}
  L_n(q) = {1\over N} \sum_{i=1}^N\left(\max_{a}(q(s_i,a) + l(s_i,a)) - q(s_i,a_i) \right)
  \end{equation}

  FIXME: Introduction correcte de $\mu(s,a)$ et sa représentation.

  Pour cela, on exprime la fonction de valeur suivie par l'expert comme linéaire selon certaines features. En notant $\mu_E$ la fonction de feature expectation, on peut écrire $\hat Q(s,a) = \omega ^T \mu_E(s,a)$, que l'on réinjecte dans notre fonction de coût pour obtenir : 
 \begin{equation}
  L_n(\omega) = {1\over N} \sum_{i=1}^N\left(\max_{a}(\omega ^T \mu_E(s_i,a) + l(s_i,a)) - \omega ^T \mu_E(s_i,a_i) \right)
  \end{equation}

  Une descente de gradient donne la règle d'update suivante :
  \begin{equation}
  \omega_{t+1} = \omega_t -\alpha_t\sum_{i=1}^N\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right)
  \end{equation}

  Avec $\alpha_t$ un pas d'apprentissage, et $a^* = \arg\max\limits_{a} = \omega_t ^T \mu_E(s_i,a) + l(s_i,a)$
** Algorithme détaillé
   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$
   - un vecteur de paramètres initial $\omega_0$
   - un nombre maximum d'itérations $T\in \mathbb N$
   - une description de l'espace d'action $A$

     
   L'algorithme se déroule comme suit : 
   - Pour le nombre d'itérations prévues :
     - Initialiser $\Delta\omega \leftarrow [0...0]^T$
     - Pour chacun des points de données
       - Déterminer $a^* = \arg\max\limits_{a} = \omega_t ^T \mu_E(s_i,a) + l(s_i,a)$
       - $\Delta\omega \leftarrow \Delta\omega + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$
     - Effectuer la mise à jour $\omega_{t+1} = \omega_t -\alpha_t\Delta\omega$
   - retourner $\omega_T$

* Implémentation python de LAFEM :code:
  :PROPERTIES:
  :ID:       879A40A3-5890-4665-86C0-826ABD3BC1BC
  :END:
   L'algorithme est implémenté dans une classe abstraite en python. Les éléments nécessaires à l'algorithme sont définis comme des fonctions abstraites, ce qui fait qu'il est impossible de créer ou d'utiliser cette classe sans la sous classer en définissant les éléments nécessaires.

#+begin_src python :tangle LAFEM.py
from numpy import *
import os
class LAFEM:
   def __init__( self ):
      if self.__class__ == LAFEM:
         raise NotImplementedError, "Cannot create object of class LAFEM"
#+end_src

   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$, prenant deux vecteurs ligne $s$ et $a$ en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def l( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$, prenant deux vecteurs lignes $s$ et $a$ en argument, renvoyant un vecteur colonne
     #+begin_src python :tangle LAFEM.py
   def mu_E( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$, prenant un entier en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def alpha( self, t ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$, il faut que ce membre soit iterable et renvoie des couples de vecteurs ligne $(s,a)$
     #+begin_src python :tangle LAFEM.py
   data=[]
     #+end_src
   - un vecteur de paramètres initial $\omega_0$
     #+begin_src python :tangle LAFEM.py
   omega_0=array([])
     #+end_src
   - un nombre maximum d'itérations $T\in \mathbb N$
     #+begin_src python :tangle LAFEM.py
   T=-1
     #+end_src
   - une description de l'espace d'action $A$, ce membre doit être itérable
     #+begin_src python :tangle LAFEM.py
   A=[]
     #+end_src

     
   L'algorithme se déroule comme suit : 
   #+begin_src python :tangle LAFEM.py
   def run( self ):
      omega = self.omega_0.copy()
   #+end_src
   - Pour le nombre d'itérations prévues :
     #+begin_src python :tangle LAFEM.py
      for t in range(0,self.T):
     #+end_src
     - Initialiser $\Delta\omega \leftarrow [0...0]^T$
       #+begin_src python :tangle LAFEM.py
         DeltaOmega = zeros(( self.omega_0.size, 1 ))
       #+end_src
     - Pour chacun des points de données
       #+begin_src python :tangle LAFEM.py
         for sa in self.data:
       #+end_src
       - Déterminer $a^* = \arg\max\limits_{a} = \omega_t ^T \mu_E(s_i,a) + l(s_i,a)$
         #+begin_src python :tangle LAFEM.py
            val = -Inf
            a_star = array([])
            for a in self.A:
               newval = dot( omega.transpose(), self.mu_E( sa[0], a ) ) + self.l( sa[0], a )
               assert(newval.size == 1)
               if newval[0] > val:
                  val = newval
                  a_star = a
         #+end_src
       - $\Delta\omega \leftarrow \Delta\omega + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$	 
         #+begin_src python :tangle LAFEM.py
            DeltaOmega = DeltaOmega + self.mu_E( sa[0], a_star ) - self.mu_E( sa[0], sa[1] )
         #+end_src
     - Effectuer la mise à jour $\omega_{t+1} = \omega_t -\alpha_t\Delta\omega$
       #+begin_src python :tangle LAFEM.py
            omega = omega - self.alpha( t ) * DeltaOmega
       #+end_src
   - retourner $\omega_T$
     #+begin_src python :tangle LAFEM.py
      return omega
     #+end_src
     
* Making this document :code:
This document can be compiled into a pdf.
#+srcname: NA_org2pdf_make
#+begin_src makefile
NouveauxAlgos.pdf: NouveauxAlgos.org
	$(call org2pdf,"NouveauxAlgos.org")
#+end_src

It can also be tangled.
#+srcname: NA_code_make
#+begin_src makefile
LAFEM.py: NouveauxAlgos.org
	$(call tangle,"NouveauxAlgos.org")
#+end_src
We provide a rule to clean the corresponding mess.
#+srcname: NA_clean_make
#+begin_src makefile
NA_clean:
	find . -maxdepth 1 -iname "NouveauxAlgos.aux"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.bbl"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.blg"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.tex"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.pdf"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "NouveauxAlgos.log"| xargs $(XARGS_OPT) rm 
	find . -maxdepth 1 -iname "NouveauxAlgos.toc"| xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "LAFEM.py" | xargs $(XARGS_OPT) rm
#+end_src
\bibliographystyle{plainnat}
\bibliography{../Biblio/Biblio}
