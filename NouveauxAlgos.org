#+OPTIONS: LaTeX:dvipng

#+EXPORT_EXCLUDE_TAGS: code
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \usepackage{natbib}
#+TITLE:Deux nouveaux algos d'IRL
[TABLE-OF-CONTENTS]

* Ce qu'on tire de Ratliff
  \label{sdef.sec}
  Dans \citep{ratliff2007imitation}, les auteurs utilisent un système classification multi classe qui met en jeu une fonction de score. Cette fonction de score $s : X\times Y \rightarrow \mathbb{R}$ est utilisée à chaque entrée pour prendre une décision de classification i.e. choisir le label $y\in Y$ à appliquer à l'entrée $x\in X$ :
  \begin{equation}
  \label{sdef.eqn}
  y^* = \arg\max_{y \in Y} s(x,y)
  \end{equation}
  Cette fonction de score est apprise dans l'espace fonctionnel en optimisant la fonction suivante : 
  \begin{equation}
  r[s] = {1\over N} \sum_{i=1}^N\left(\max_{y\in Y}(s(x_i,y) + l(x_i,y)) - s(x_i,y_i) \right)
  \end{equation}
  Cela suppose l'accès à une base de données $\{(x_i,y_i)\}_i$; i.e. une trace de l'expert.
* Hoping $s$ is Bellman compatible (HSIBC ?)
  La fonction de score $s$ joue le même rôle que la fonction $Q^*$, en effet l'equation \ref{sdef.eqn} est similaire à :
  \begin{equation}
  \pi^*(s) = \arg\max_{a} Q^*(s,a)
  \end{equation}
  qui décrit le mécanisme glouton liant fonction de valeur optimale et politique optimale.
  
  En appliquant une méthode de classification multi-classe permettant de trouver une fonction de score telle que décrite en section \ref{sdef.sec} (FIXME: Demander à Yann), on peut potentiellement arriver à retrouver la récompense.

  En partant de l'équation de Bellman définissant la fonction de valeur optimale : 
  \begin{equation}
  Q^* = R + \gamma PQ^*
  \end{equation}
  on trouve facilement 
  \begin{equation}
  R = Q^*(Id - \gamma P)
  \end{equation}
  
  Le mécanisme de classification utilisé fournit une fonction de score dont la structure importe peu mais qui permet sans doute de construire une base de données $\{(s_i,R(s_i))\}_i$ se basant sur les mêmes points $\{s_i\}_i$ que ceux utilisés pour l'entraînement du classifieur, ou sur un maillage de l'espace d'état etc. A partir de cette base de données, on peut faire de la régression pour trouver une fonction de récompense $\hat R$ ensuite utilisée dans un algorithme d'apprentissage par renforcement direct.


  Il convient probablement d'entraîner le classifieur sur des données ayant une structure temporelle (des trajectoires) afin que la fonction de score capture cette structure.
* Loss-augmented feature expectation matching (LAFEM ?)
** Principe
  Toujours en se basant sur la ressemblance entre $s$ et $Q^*$, on peut choisir de directement optimiser la fonction :
  \begin{equation}
  L_n(q) = {1\over N} \sum_{i=1}^N\left(\max_{a}(q(s_i,a) + l(s_i,a)) - q(s_i,a_i) \right)
  \end{equation}

  FIXME: Introduction correcte de $\mu(s,a)$ et sa représentation.

  Pour cela, on exprime la fonction de valeur suivie par l'expert comme linéaire selon certaines features. En notant $\mu_E$ la fonction de feature expectation, on peut écrire $\hat Q(s,a) = \omega ^T \mu_E(s,a)$, que l'on réinjecte dans notre fonction de coût pour obtenir : 
 \begin{equation}
  L_n(\omega) = {1\over N} \sum_{i=1}^N\left(\max_{a}(\omega ^T \mu_E(s_i,a) + l(s_i,a)) - \omega ^T \mu_E(s_i,a_i) \right)
  \end{equation}

  Une descente de gradient donne la règle d'update suivante :
  \begin{equation}
  \omega_{t+1} = \omega_t -\alpha_t\sum_{i=1}^N\left(\mu_E(s_i,a^*) - \mu_E(s_i,a_i)\right)
  \end{equation}

  Avec $\alpha_t$ un pas d'apprentissage, et $a^* = \arg\max\limits_{a} = \omega_t ^T \mu_E(s_i,a) + l(s_i,a)$
** Algorithme détaillé
   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$
   - un vecteur de paramètres initial $\omega_0$
   - un nombre maximum d'itérations $T\in \mathbb N$
   - une description de l'espace d'action $A$

     
   L'algorithme se déroule comme suit : 
   - Pour le nombre d'itérations prévues :
     - Initialiser $\Delta\omega \leftarrow [0...0]^T$
     - Pour chacun des points de données
       - Déterminer $a^* = \arg\max\limits_{a} = \omega_t ^T \mu_E(s_i,a) + l(s_i,a)$
       - $\Delta\omega \leftarrow \Delta\omega + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$
     - Effectuer la mise à jour $\omega_{t+1} = \omega_t -\alpha_t\Delta\omega$
   - retourner $\omega_T$

* Code :code:
** LAFEM
   L'algorithme est implémenté dans une classe abstraite en python. Les éléments nécessaires à l'algorithme sont définis comme des fonctions abstraites, ce qui fait qu'il est impossible de créer ou d'utiliser cette classe sans la sous classer en définissant les éléments nécessaires.

#+begin_src python :tangle LAFEM.py
from numpy import *
import os
class LAFEM:
   def __init__( self ):
      if self.__class__ == LAFEM:
         raise NotImplementedError, "Cannot create object of class LAFEM"
#+end_src

   L'algorithme prend en entrée :
   - une fonction de perte $l : S\times A \rightarrow \mathbb R$, prenant deux vecteurs ligne $s$ et $a$ en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def l( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - la /feature expectation/ de l'expert $\mu_E : S\times A \rightarrow \mathbb R ^p$, prenant deux vecteurs lignes $s$ et $a$ en argument, renvoyant un vecteur colonne
     #+begin_src python :tangle LAFEM.py
   def mu_E( self, s, a ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - un pas d'apprentissage $\alpha : \mathbb N \rightarrow \mathbb R$, prenant un entier en argument, renvoyant un réel
     #+begin_src python :tangle LAFEM.py
   def alpha( self, t ):
      raise NotImplementedError, "Cannot call abstract method"
     #+end_src
   - des données provenant de l'expert $\{(s_i,a_i)\}_{i=1..N}$, il faut que ce membre soit iterable et renvoie des couples de vecteurs ligne $(s,a)$
     #+begin_src python :tangle LAFEM.py
   data=[]
     #+end_src
   - un vecteur de paramètres initial $\omega_0$
     #+begin_src python :tangle LAFEM.py
   omega_0=array([])
     #+end_src
   - un nombre maximum d'itérations $T\in \mathbb N$
     #+begin_src python :tangle LAFEM.py
   T=-1
     #+end_src
   - une description de l'espace d'action $A$, ce membre doit être itérable
     #+begin_src python :tangle LAFEM.py
   A=[]
     #+end_src

     
   L'algorithme se déroule comme suit : 
   #+begin_src python :tangle LAFEM.py
   def run( self ):
      omega = self.omega_0.copy()
   #+end_src
   - Pour le nombre d'itérations prévues :
     #+begin_src python :tangle LAFEM.py
      for t in range(0,self.T):
     #+end_src
     - Initialiser $\Delta\omega \leftarrow [0...0]^T$
       #+begin_src python :tangle LAFEM.py
         DeltaOmega = zeros(( self.omega_0.size, 1 ))
       #+end_src
     - Pour chacun des points de données
       #+begin_src python :tangle LAFEM.py
         for sa in self.data:
       #+end_src
       - Déterminer $a^* = \arg\max\limits_{a} = \omega_t ^T \mu_E(s_i,a) + l(s_i,a)$
         #+begin_src python :tangle LAFEM.py
            val = -Inf
            a_star = array([])
            for a in self.A:
               newval = dot( omega.transpose(), self.mu_E( sa[0], a ) ) + self.l( sa[0], a )
               assert(newval.size == 1)
               if newval[0] > val:
                  val = newval
                  a_star = a
         #+end_src
       - $\Delta\omega \leftarrow \Delta\omega + \mu_E(s_i,a^*) - \mu_E(s_i,a_i)$	 
         #+begin_src python :tangle LAFEM.py
            DeltaOmega = DeltaOmega + self.mu_E( sa[0], a_star ) - self.mu_E( sa[0], sa[1] )
         #+end_src
     - Effectuer la mise à jour $\omega_{t+1} = \omega_t -\alpha_t\Delta\omega$
       #+begin_src python :tangle LAFEM.py
            omega = omega - self.alpha( t ) * DeltaOmega
       #+end_src
   - retourner $\omega_T$
     #+begin_src python :tangle LAFEM.py
      return omega
     #+end_src
     
* Expériences
** Expérience 1
*** Description
    FIXME: Description correcte du Gridworld.

    La première expérience consiste à entraîner un expert sur un gridworld de 5 cases par 5 case, avec la récompense en haut à droite, puis à faire tourner LAFEM sur un set de données couvrant la totalité de l'espace d'état, un calcul exact de $\mu_E$, une fonction de perte extrêmement simple ($1$ en cas de désaccord avec l'expert, $0$ sinon) et une initialisation de la récompense à $0$ partout. Comme le problème est discret, la représentation est tabulaire.


    L'expérience se déroule comme suit :
    - Entraîner un expert sur le problème du gridworld, i.e. définir $\pi_E : S\rightarrow A$
    - Définir $l$ telle que $l(s,a) = 0$ si $a=\pi_E(s)$, $1$ sinon
    - définir $\alpha(t) = 0.1,\forall t$ (pifomètre)
    - Initialiser $D\leftarrow \emptyset$
    - Pour chaque état $s \in S$
      - $D \leftarrow D \cup (s,\pi_E(s))$
    - Initialiser $\omega_0 = [0...0]^T$
    - Fixer $T=20$ (pifomètre)
    - Faire tourner LAFEM
    - Entrainer un agent sur le problème du gridworld, avec la récompense trouvée par LAFEM : définir $\pi : S\rightarrow A$
    - Définir une politique stochastique uniforme $\pi_r$
    - Comparer $E[\omega^T\mu_E(s)| s\in S]$, $E[\omega^T\mu_\pi(s)| s\in S]$ et $E[\omega^T\mu_r(s)| s\in S]$

*** Code :code:
**** Calcul exact de la /feature expectation/
     En ce qui concerne l'implémentation, on reprend le protocole ci dessus et on se soucie des détails.

    De la même manière qu'il est possible d'écrire l'equation de Bellman sous forme matricielle : 
    \begin{equation}
    V^\pi = R + \gamma P_\pi V^\pi 
    \end{equation}
    On peut écrire le même type d'équation pour $\mu$, pour une composante $i$ :
    \begin{equation}
    \mu^\pi_i = \phi_i + \gamma P_\pi\mu^\pi_i 
    \end{equation}
    Si l'on considère la matrice $\mathbf \mu$ comme étant indexée par les états sur les lignes et par les composentes sur les colonnes, et la même technique pour la matrice $\mathbf \Phi$, alors il est possible d'écrire : 
    \begin{equation}
    \mathbf \mu^\pi = \mathbf\Phi + \gamma P_\pi\mathbf\mu^\pi
    \end{equation}
    Grâce à cette équation il est possible d'adapter les algorithmes de programmation dynamique itératifs pour obtenir un calcul exact de $\mathbf \mu^\pi$.

    Cependant, la fonction dont nous avons besoin n'est pas $\mu^\pi : S \rightarrow \mathbb R^p$, mais $\mu^\pi : S \times A \rightarrow \mathbb R^p$ (la notation reste la même pour ne pas tout surcharger, la différence étant réglée assze vite). Ces deux fonctions ont entre elles la même relation qu'ont $V$ et $Q$.
    Ainsi puisque l'on peut écrire :
    \begin{eqnarray}
    Q^\pi(s,a) &=& R(s) + \gamma P_a(s)V^\pi\\
    Q^\pi_a &=& R + \gamma P_aV^\pi
    \end{eqnarray}
    On peut faire le parallèle (en notant $\mu^\pi(s,\cdot) = \mu^\pi_a(s)$) :
    \begin{eqnarray}
    \mu^\pi(s,a) &=& \phi(s) + \gamma P_a(s)\mathbf \mu^\pi\\
    \mathbf \mu^\pi_a &=& \mathbf \Phi + \gamma P_a\mathbf \mu^\pi
    \end{eqnarray}
    
    Pour résumer, il est possible de calculer la matrice $\mathbf \mu^\pi$ grâce à une adaptation des algorithmes de programmation dynamique, et à partir de celle ci de déduire les matrices $\mu^\pi_a$, qui sont celles dont nous avons besoin.

    #+begin_src python :tangle NA_DP_mu.py
from numpy import *
import scipy

g_fGamma = 0.9

def NA_DP_mu( pi, Phi ):
    "Returns the matrix corresponding to the feature expectation of the given policy."
    global g_fGamma
    answer = scipy.rand( Phi.shape[0], Phi.shape[1] )
    changed = True
    while changed:
        new_answer = Phi + g_fGamma*dot(pi,answer)
        diff = sum( abs( new_answer - answer ) )
        answer = new_answer
        if diff > 0.01:
            changed = True
        else:
            changed = False
    return answer
    #+end_src
**** Configuration de LAFEM

    On importe ce qu'il importe d'importer
    #+begin_src python :tangle NA_Exp1.py
from a2str import *
from LAFEM import *
from NA_DP_mu import *
from TT_DP import *
from Pi2txt import *
    #+end_src
    
    On définit ce qui est nécessaire au fonctionnement de LAFEM en sous classant la classe abstraite LAFEM
      #+begin_src python :tangle NA_Exp1.py
class LAFEM_Exp1( LAFEM ):
    P_north = array([])
    P_south = array([])
    P_east = array([])
    P_west = array([])
    Pi_E = array([])
    Pi_E_txt = {}
    Mu_north = array([])
    Mu_west = array([])
    Mu_east = array([])
    Mu_south = array([])
      #+end_src

    On définit $S$ et $A$ de manière à pouvoir itérer dessus.
    #+begin_src python :tangle NA_Exp1.py
    def Sgenerator( self ):
        for x in range(0,5):
            for y in range(0,5):
                yield array([x,y])

    S = Sgenerator
    A = ['North','West','East','South']
    #+end_src

    Pour entraîner un expert sur le problème du gridworld, i.e. définir $\pi_E : S\rightarrow A$, on réutilise le code du [[file:TaskTransfer.org::*Code][task transfer]] tel quel pour obtenir la politique optimale.
      #+begin_src python :tangle NA_Exp1.py
    def __init__( self ):
        cmd = "python TT_5x5_expertPGen.py";
        os.system( cmd )
        self.Pi_E = genfromtxt("TT_5x5_Ppi.mat")
      #+end_src      
    On peuple $D$ : 
      #+begin_src python :tangle NA_Exp1.py
        self.P_north = genfromtxt( "TT_5x5_PENorth.mat" )
        self.P_east = genfromtxt( "TT_5x5_PEEast.mat" )
        self.P_west = genfromtxt( "TT_5x5_PEWest.mat" )
        self.P_south = genfromtxt( "TT_5x5_PESouth.mat" )
        self.Pi_E_txt = Pi2txt( self.S, self.Pi_E )
        for s in self.S():
            self.data = self.data+[ [ s, self.Pi_E_txt[l2str(s)] ] ]
      #+end_src
    On calcule les matrices de /feature expectation/
      #+begin_src python :tangle NA_Exp1.py
        self.Mu_E = NA_DP_mu( self.Pi_E, identity(25) )
        self.Mu_north = identity(25) + 0.9*dot(self.P_north,self.Mu_E)
        self.Mu_west = identity(25) + 0.9*dot(self.P_west,self.Mu_E)
        self.Mu_east = identity(25) + 0.9*dot(self.P_east,self.Mu_E)
        self.Mu_south = identity(25) + 0.9*dot(self.P_south,self.Mu_E)
      #+end_src

    On définit la fonction de perte $l$ :
    #+begin_src python :tangle NA_Exp1.py
    def l( self, s, a ):
        if self.Pi_E_txt[l2str(s)] == a:
            return 0
        else:
            return 1
    #+end_src
    
    On définit la fonction renvoyant $\mu_E(s,a)$ : 
    #+begin_src python :tangle NA_Exp1.py
    def mu_E( self, s, a ):
        mu_a = array([])
        if a == 'North':
            mu_a = self.Mu_north
        elif a == 'West':
            mu_a = self.Mu_west
        elif a == 'East':
            mu_a = self.Mu_east
        elif a == 'South':
            mu_a = self.Mu_south
        else:
            print "On calcule mu sur une action qui nexiste pas"
            exit(-1)
        index = s[0]+5*s[1]
        return (zeros((1,25)) + mu_a[index]).transpose() #Ugly hack to get a column vector and not a line vector
    #+end_src
    


    On définit $\alpha(t) = 0.1,\forall t$ (pifomètre)
      #+begin_src python :tangle NA_Exp1.py
    def alpha( self, t ):
        return 0.1
      #+end_src
    On initialise $\omega_0 = [0...0]^T$
      #+begin_src python :tangle NA_Exp1.py
    omega_0 = zeros( (25, 1) )
      #+end_src
    - Fixer $T=20$ (pifomètre)
      #+begin_src python :tangle NA_Exp1.py
    T = 20
      #+end_src
**** Affichage de la politique
     On crée une fonction permettant un joli affichage de la politique, pour comparer celle de l'expert et celle de l'agent : 
      #+begin_src python :tangle Pi2txt.py
from numpy import *
import scipy
from a2str import *

def Pi2txt( S, Pi ):
    answer = {}
    P_north = genfromtxt( "TT_5x5_PENorth.mat" )
    P_east = genfromtxt( "TT_5x5_PEEast.mat" )
    P_west = genfromtxt( "TT_5x5_PEWest.mat" )
    P_south = genfromtxt( "TT_5x5_PESouth.mat" )
    for s in S():
        index = s[0]+5*s[1]
        pi_s = Pi[index]
        a = ''
        if all(pi_s == P_north[index]):
            answer[l2str(s)] = 'North'
        elif all(pi_s == P_south[index]):
            answer[l2str(s)] = 'South'
        elif all(pi_s == P_west[index]):
            answer[l2str(s)] = 'West'
        elif all(pi_s == P_east[index]):
            answer[l2str(s)] = 'East'
        else:
            print 'On narrive pas a reconnaitre la politique'
            exit(-1)
    return answer

def Pi2Asciiart( Pi ):
    answer = ''
    P_north = genfromtxt( "TT_5x5_PENorth.mat" )
    P_east = genfromtxt( "TT_5x5_PEEast.mat" )
    P_west = genfromtxt( "TT_5x5_PEWest.mat" )
    P_south = genfromtxt( "TT_5x5_PESouth.mat" )
    for y in range(0,5):
        for x in range(0,5):
            index = x+5*y
            pi_s = Pi[index]
            a = ''
            if all(pi_s == P_north[index]):
                answer = answer + ' ^'
            elif all(pi_s == P_south[index]):
                answer = answer + ' v'
            elif all(pi_s == P_west[index]):
                answer = answer + ' <'
            elif all(pi_s == P_east[index]):
                answer = answer + ' >'
            else:
                print 'On narrive pas a reconnaitre la politique'
                exit(-1)
        answer = answer + '\n'
    return answer

      #+end_src
     
**** Expérience 1
     On fait tourner le machin :

      #+begin_src python :tangle NA_Exp1.py
lafem = LAFEM_Exp1()
omega_lafem = lafem.run()
      #+end_src
     
     Entrainer un agent sur le problème du gridworld, avec la récompense trouvée par LAFEM : définir $\pi : S\rightarrow A$
     #+begin_src python :tangle NA_Exp1.py
Pi = TT_DP_txt( omega_lafem, (lafem.P_north, lafem.P_south, lafem.P_west, lafem.P_east), "V_agent.mat" )
     #+end_src
      
     Définir une politique stochastique uniforme $\pi_r$
     Comparer $E[\omega^T\mu_E(s)| s\in S]$, $E[\omega^T\mu_\pi(s)| s\in S]$ et $E[\omega^T\mu_r(s)| s\in S]$
     #+begin_src python :tangle NA_Exp1.py
true_reward = zeros((25,1))
true_reward[4,0] = 1
perf_expert = 0
cnt = 0
for s in lafem.S():
    index = s[0] + 5*s[1]
    perf_expert = perf_expert + dot( true_reward.transpose(), lafem.Mu_E[index].transpose() )
    cnt = cnt + 1
perf_expert = perf_expert/cnt

Mu_pi = NA_DP_mu( Pi, identity(25) )
perf_agent = 0
cnt = 0
for s in lafem.S():
    index = s[0] + 5*s[1]
    perf_agent = perf_agent + dot( true_reward.transpose(), Mu_pi[index].transpose() )
    cnt = cnt + 1
perf_agent = perf_agent/cnt

print "Performance moyenne de l'expert : "
print perf_expert

print "Performance moyenne de l'agent :"
print perf_agent

print "Politique de l'expert :"
print Pi2Asciiart( lafem.Pi_E )

print "Politique de l'agent : "
print Pi2Asciiart( Pi )

V_agent = genfromtxt("V_agent.mat")
V_expert = genfromtxt("V_expert.mat")


f = open( "V_agent_plot.txt", "w" )

for y in range(0,5):
    for x in range(0,5):
        index = x+5*y
        f.write( "%d %d %e\n"%(x,y,V_agent[index]) )
    f.write("\n")
f.close()


f = open( "V_expert_plot.txt", "w" )
for y in range(0,5):
    for x in range(0,5):
        index = x+5*y
        f.write( "%d %d %e\n"%(x,y,V_expert[index]) )
    f.write("\n")
f.close()

f = open( "True_reward.txt", "w" )
for y in range(0,5):
    for x in range(0,5):
        index = x+5*y
        f.write( "%d %d %e\n"%(x,y,true_reward[index]) )
    f.write("\n")
f.close()

f = open( "retrieved_reward.txt", "w" )
for y in range(0,5):
    for x in range(0,5):
        index = x+5*y
        f.write( "%d %d %e\n"%(x,y,omega_lafem[index]) )
    f.write("\n")
f.close()

      #+end_src
      
*** Résultats
* Making this document :code:

#+begin_src makefile :tangle Makefile
NouveauxAlgos.pdf: NouveauxAlgos.org
	emacs -batch --visit NouveauxAlgos.org --funcall org-export-as-latex --script ~/.emacs
	pdflatex NouveauxAlgos.tex && bibtex NouveauxAlgos && pdflatex NouveauxAlgos.tex && pdflatex NouveauxAlgos.tex
#+end_src

#+srcname: NA_cleanDoc_make
#+begin_src makefile
NA_cleanDoc:
	find . -maxdepth 1 -iname "*.aux"| xargs -t rm &&\
	find . -maxdepth 1 -iname "*.bbl"| xargs -t rm &&\
	find . -maxdepth 1 -iname "*.blg"| xargs -t rm &&\
	find . -maxdepth 1 -iname "NouveauxAlgos.tex"| xargs -t rm &&\
	find . -maxdepth 1 -iname "NouveauxAlgos.pdf"| xargs -t rm &&\
	find . -maxdepth 1 -iname "*.log"| xargs -t rm &&\
	find . -maxdepth 1 -iname "*.py"| xargs -t rm &&\
	find . -maxdepth 1 -iname "*.pyc"| xargs -t rm &&\
	find . -maxdepth 1 -iname "*.toc"| xargs -t rm
#+end_src
\bibliographystyle{plainnat}
\bibliography{../Biblio/Biblio}
