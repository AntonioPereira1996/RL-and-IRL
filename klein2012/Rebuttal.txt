We would like to thank the reviewers for their thorough reading and remarks. We wish to somewhat clarify some points the reviewers showed concern about.

- As mentioned in the paper, the feature expectation can be seen as a vector of Q-functions, of which each component can be estimated using LSTD. That's what LSTDmu does; this is just a temporal difference algorithm (and not a contribution of this paper, as rightly mentioned). As each component shares the same dynamic, the computational cost is the one of performing LSTD on one component (in O(p^3)), see Klein et al. We have the same guarantees for estimating mu_E from expert trajectories than for estimating a Q-function with LSTD in an on-policy setting (e.g., see Lazaric et al.). Klein et al. used it in its off-policy form because of the requirement of the algorithm of (Abbeel et al. 2004) to compute mu for an arbitrary policy. The on-policy setting considered in our work only requires data from the expert.

- As mentioned by reviewer 2, our algorithm is inspired by MMP but differs in several ways. We are not merely combining existing components as assessed by reviewer 3.
 - All approaches build on the structured large margin framework of Taskar et al., but they solve quite different problems.
 - Our labels are actions, MMP's labels are policies. We only need expert transitions whereas MMP needs the whole model (for a set of MDPs, not just one). 
 - We are not merely replacing mu with an approximation based on \mu_E (this is Klein et al). We introduce a new way to look at the score function q, which allows us to minimize the risk function using only \mu_E evaluated only in states present in the expert's dataset, which can be estimated solely from expert transitions. MMP needs a lot more information. 
 - Any classifier (and not only the structured margin framework of Taskar et al. used here) using such a parameterized score function could be plugged in our algorithm. 

- The lack of quantitative results mentionned by reviewer 2 is addressed lines 612 and 657, we show that SCIRL works in conditions where state of the art algorithms fail due to lack of information. Klein et al.'s results (not recalled in our paper due to space constraints) show that the algorithm of Abbeel and Ng needs around 3000 more arbitrary samples (not the expert's) in order to work (this is due to the off-policy learning aspect). We lift this requirement with SCIRL which is to the best of our knowledge the only algorithm that works in such drastic conditions.

- As discussed in the paper, we recognize that the assumptions of the analysis are strong. However, we show that the algorithm computes a reward for which the expert policy is the unique optimal policy, which is not a straightforward problem (even knowing the dynamic and the expert policy). Given that recovering a reward is the problem we address, we think it is valuable despite the strong assumptions.

- We show that our algorithm stands comparison when put in competition with MWAL. The latter needs access to the system's dynamics whereas our algorithm only uses the expert's transitions. Marginally subpar results thus are a contribution, indeed SCIRL can be used in cases where the lack of information would prohibit the use of MWAL.

- In addition to needing far less information than state of the art methods, by using a LSTD-like method once and for all at the beginning, our algorithm is also computationally more efficient than others that require solving one or more MDP at each iteration. We empirically notice a ten-fold gain on computation time on a desktop computer compared to MWAL.

- To clarify what we meant l.354, we insist on retrieving the reward because task transfer (getting an agent with different capabilities than the expert's to fulfill the same task) cannot be done with a policy. With a reward function on the states, it is possible to train any agent on the problem, even if the action space is different.
