We would like to thank the reviewers for their thorough reading and their remarks.
We wish to raise the following points to somewhat clarify aspects of our work the reviewers showed concern about.

- As mentioned in the paper, the feature expectation can be seen as a vector of Q-functions, of which each component can be estimated using LSTD. That's what LSTDmu does; this is just a temporal difference algorithm (and not a contribution of this paper, as rightly mentioned). As each component shares the same dynamic, the computational cost is the one of performing LSTD on one component (in O(p^3)), see (Klein et al. 2011). Also, we have the same guarantees for estimating mu_E from expert trajectories than for estimating a Q-function with LSTD in an on-policy setting (e.g., see Lazaric et al, 2010). In (Klein et al. 2011) it was used in its off-policy form because of the requirement of the algorithm of (Abbeel et al. 2004) to compute \mu for an arbitrary policy. The on-policy setting considered in our contribution only requires data from the expert.

- Our algorithm differs from Ratliff et al.'s work in several ways.
  - All approaches build on the structured large margin framework of Taskar et al., but they solve quite different problems.
  - Our labels are actions, MMP's labels are policies. We only need expert transitions whereas MMP needs the whole model (for a set of MDPs, not just one). 
  - We are not merely replacing mu with an approximation based on \mu_E (this is Klein et al). We introduce a new way to look at the score function q, which allows addressing two major problems of IRL.
  - Any classifier (and not only the structured margin framework of Taskar et al. used here) using such a parameterized score function could be plugged in our algorithm.
  - The way we express the score function q allows us to minimize the risk function using only the feature expectation of the expert, which can be estimated solely from expert transitions. MMP needs a lot more information to achieve its goal.
- With the pendulum experiment, we show that our method works (quantitative results given line 657) in conditions where state of the art algorithms fail due to lack of information. Quantitatively, (klein et al. 2011) have shown that the algorithm of (Abbeel and Ng 2004) needs 2000 to 3000 more arbitrary samples (not the expert's) in order to work (this is due to the off-policy learning aspect). We lift this requirement with SCIRL.

- We recognize that the assumptions of the analysis are strong (as discussed in the paper). However, we show that the algorithm computes a reward for which the expert policy is the unique optimal policy, which is not a straightforward problem (even knowing the dynamic and the expert policy). Given that recovering a reward is the problem we address, we think it is valuable, despite the strong assumptions.

- We show that our algorithm stands comparison when put in competition with MWAL. The latter has access to every bit of information about the problem whereas our algorithm only uses the expert's transitions.

- In addition to needing far less information than state of the art methods, by using a LSTD-like method once and for all at the beginning, our algorithm is also computationally more efficient than others that require solving one or more MDP at each iteration. Indeed, solving a MDP implies (e.g. if one uses LSPI) using LSTD more than once. This empirically shows with a ten-fold gain on computation time on a desktop computer compared to MWAL.

- We insist on retrieving the reward because task transfer (getting an agent with different capabilities than the expert's to fulfill the same task) cannot be done with a policy. With a reward function on the states, it is possible to train any agent on the problem, even if the action space is different.
