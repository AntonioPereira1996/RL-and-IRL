{
 "metadata": {
  "name": "Exp13"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext autoreload\n",
      "%autoreload 2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#CSI on the Highway\n",
      "from DP import *\n",
      "from stuff import *\n",
      "from pylab import *\n",
      "from random import *\n",
      "import numpy\n",
      "from rl import *\n",
      "P = genfromtxt(\"Highway_P.mat\")\n",
      "R = genfromtxt(\"Highway_R.mat\")\n",
      "Gamma = 0.9\n",
      "ACTION_SPACE = range(0,5)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "Highway = MDP(P,R)\n",
      "mPi_E, V_E, Pi_E = Highway.optimal_policy()\n",
      "Highway.evaluate(mPi_E)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MDP OK3\n",
        "Shape of P (3645, 729)\n",
        "Shape of R (3645,)\n",
        "Card of S 729\n",
        "Card of A 5\n",
        "Card of SA 3645\n",
        "Iteration 0, 888\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 1, 800\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 2, 234\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 3, 50\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 4, 0\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 3,
       "text": [
        "7.7439104526748785"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rho = lambda : int(rand()*729) #uniform distribtion over S\n",
      "l_D_E = [array(Highway.D_func(Pi_E, 1, 7,  rho)) for i in range(0,7)]\n",
      "D_E = vstack(l_D_E)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Classification\n",
      "#Structured Classifier\n",
      "class GradientDescent(object):\n",
      "   def alpha( self, t ):\n",
      "      raise NotImplementedError, \"Cannot call abstract method\"\n",
      "\n",
      "   theta_0=None\n",
      "   Threshold=None\n",
      "   T = -1\n",
      "   sign = None\n",
      "        \n",
      "   def run( self, f_grad, f_proj=None, b_norm=False ): #grad is a function of theta\n",
      "      theta = self.theta_0.copy()\n",
      "      best_theta = theta.copy()\n",
      "      best_norm = float(\"inf\")\n",
      "      best_iter = 0\n",
      "      t=0\n",
      "      while True:#Do...while loop\n",
      "         t+=1\n",
      "         DeltaTheta = f_grad( theta )\n",
      "         current_norm = norm( DeltaTheta )\n",
      "         if b_norm and  current_norm > 0.:\n",
      "             DeltaTheta /= norm( DeltaTheta )\n",
      "         theta = theta + self.sign * self.alpha( t )*DeltaTheta\n",
      "         if f_proj:\n",
      "             theta = f_proj( theta )\n",
      "         print \"Norme du gradient : \"+str(current_norm)+\", pas : \"+str(self.alpha(t))+\", iteration : \"+str(t)\n",
      "\n",
      "         if current_norm < best_norm:\n",
      "             best_norm = current_norm\n",
      "             best_theta = theta.copy()\n",
      "             best_iter = t\n",
      "         if current_norm < self.Threshold or (self.T != -1 and t >= self.T):\n",
      "             break\n",
      "\n",
      "      print \"Gradient de norme : \"+str(best_norm)+\", a l'iteration : \"+str(best_iter)\n",
      "      return best_theta\n",
      "\n",
      "\n",
      "class StructuredClassifier(GradientDescent):\n",
      "    sign=-1.\n",
      "    Threshold=0.01 #Sensible default\n",
      "    T=100 #Sensible default\n",
      "    phi=None\n",
      "    label_set=None\n",
      "    \n",
      "    def alpha(self, t):\n",
      "        return 3./(t+1)#Sensible default\n",
      "    \n",
      "    def __init__(self, psi, actions, nb_actions):\n",
      "        self.label_set=range(0,nb_actions)\n",
      "        self.N,self.K = psi.shape\n",
      "        self.A = nb_actions\n",
      "        self.theta_0 = zeros(self.K*self.A)\n",
      "        self.ExpertDecision = zeros((self.N,self.A))\n",
      "        for i,j in zip( range(0,self.N), actions.reshape(self.N) ):\n",
      "            self.ExpertDecision[i,j] = 1.\n",
      "        self.Structure = array(self.ExpertDecision!=1)\n",
      "        self.ExpertDecision = self.ExpertDecision.reshape(self.N,self.A,1)\n",
      "        self.Psi_3 = array([[p for i in range(0,self.A)] for p in psi])\n",
      "        self.Phi = self.ExpertDecision*self.Psi_3\n",
      "        self.Phi = self.Phi.reshape(self.N,self.K*self.A)\n",
      "        self.Psi = psi\n",
      "    \n",
      "    def gradient(self, theta):\n",
      "        theta_2 = hstack([theta[i*self.K:(i+1)*self.K].reshape((self.K,1)) for i in range(0,self.A)])\n",
      "        score = dot(self.Psi,theta_2)+self.Structure\n",
      "        maxScore = dot(score.max(axis=1).reshape((self.N,1)),ones((1,self.A)))\n",
      "        decision = (score==maxScore).reshape(self.N,self.A,1)\n",
      "        #We restrict ourselves to one arbitrary decision\n",
      "        for i in range(0,self.N):\n",
      "            gotOne = False\n",
      "            for j in range(0,self.A):\n",
      "                if decision[i,j] and not gotOne:\n",
      "                    gotOne = True\n",
      "                elif decision[i,j] and gotOne:\n",
      "                    decision[i,j] = False\n",
      "        phi_star = decision*self.Psi_3\n",
      "        phi_star = phi_star.reshape(self.N,self.K*self.A)\n",
      "        return mean(phi_star-self.Phi,axis=0)\n",
      "    \n",
      "    def run(self):\n",
      "        f_grad = lambda theta: self.gradient(theta)\n",
      "        theta = super(StructuredClassifier,self).run( f_grad, b_norm=True)\n",
      "        return theta\n",
      "    \n",
      "Psi = zeros((len(D_E),729))\n",
      "for i,j in zip(range(0,len(D_E)),D_E[:,0]):\n",
      "    Psi[i,j]=1.\n",
      "clf = StructuredClassifier(Psi, D_E[:,1], 5)\n",
      "theta_C = clf.run()\n",
      "mPi_C = Highway.Q2Pi(theta_C)\n",
      "\n",
      "def single_phi(sa):\n",
      "    answer = zeros((3645,1))\n",
      "    answer[sa[0] + 729*sa[1]] = 1.\n",
      "    return answer\n",
      "phi = non_scalar_vectorize(single_phi, (2,), (3645,1))\n",
      "q = lambda sa: squeeze(dot(theta_C.transpose(),phi(sa)))\n",
      "#pi_c = greedy_policy(theta_C,phi,ACTION_SPACE, s_dim=1)\n",
      "single_pi_c = lambda s : Highway.control(s, mPi_C)\n",
      "pi_c = vectorize(single_pi_c)\n",
      "Highway.evaluate(mPi_C)\n",
      "#theta_C.min(),theta_C.max(),theta_C.mean(),theta_C.var()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Norme du gradient : 0.259753511456, pas : 1.5, iteration : 1\n",
        "Norme du gradient : 0.259753511456, pas : 1.0, iteration : 2\n",
        "Norme du gradient : 0.259753511456, pas : 0.75, iteration : 3\n",
        "Norme du gradient : 0.259753511456, pas : 0.6, iteration : 4\n",
        "Norme du gradient : 0.212087853988, pas : 0.5, iteration : 5\n",
        "Norme du gradient : 0.212087853988, pas : 0.428571428571, iteration : 6\n",
        "Norme du gradient : 0.212087853988, pas : 0.375, iteration : 7\n",
        "Norme du gradient : 0.147165358182, pas : 0.333333333333, iteration : 8\n",
        "Norme du gradient : 0.147165358182, pas : 0.3, iteration : 9\n",
        "Norme du gradient : 0.147165358182, pas : 0.272727272727, iteration : 10\n",
        "Norme du gradient : 0.147165358182, pas : 0.25, iteration : 11"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Norme du gradient : 0.147165358182, pas : 0.230769230769, iteration : 12\n",
        "Norme du gradient : 0.147165358182, pas : 0.214285714286, iteration : 13\n",
        "Norme du gradient : 0.147165358182, pas : 0.2, iteration : 14\n",
        "Norme du gradient : 0.147165358182, pas : 0.1875, iteration : 15\n",
        "Norme du gradient : 0.147165358182, pas : 0.176470588235, iteration : 16\n",
        "Norme du gradient : 0.147165358182, pas : 0.166666666667, iteration : 17\n",
        "Norme du gradient : 0.147165358182, pas : 0.157894736842, iteration : 18\n",
        "Norme du gradient : 0.147165358182, pas : 0.15, iteration : 19"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Norme du gradient : 0.147165358182, pas : 0.142857142857, iteration : 20\n",
        "Norme du gradient : 0.0, pas : 0.136363636364, iteration : 21\n",
        "Gradient de norme : 0.0, a l'iteration : 21\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0.18603042225294997"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Donn\u00e9es pour la regression\n",
      "column_shape = (len(D_E),1)\n",
      "s = D_E[:,0].reshape(column_shape)\n",
      "a = D_E[:,1].reshape(column_shape)\n",
      "sa = D_E[:,:2]\n",
      "s_dash = D_E[:,3].reshape(column_shape)\n",
      "a_dash = pi_c(s_dash).reshape(column_shape)\n",
      "sa_dash = hstack([s_dash,a_dash])\n",
      "hat_r = (q(sa)-Gamma*q(sa_dash)).reshape(column_shape)\n",
      "r_min = min(hat_r)-1.*ones(column_shape)\n",
      "hat_r.max(),hat_r.min(),hat_r.mean(),hat_r.var()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 19,
       "text": [
        "(0.90745370252273583,\n",
        " 0.81378476049769144,\n",
        " 0.84300549231483712,\n",
        " 0.0012630405356859849)"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "##Avec l'heuristique : \n",
      "thetas = zeros((5,729))\n",
      "for action in ACTION_SPACE:\n",
      "    X = Psi\n",
      "    hat_r_bool_table = array(a==action)\n",
      "    r_min_bool_table = array(hat_r_bool_table==False) #\"not hat_r_bool_table\" does not work as I expected\n",
      "    Y =  hat_r_bool_table*hat_r + r_min_bool_table*r_min\n",
      "    #if sum(hat_r_bool_table) == 0:\n",
      "    #    thetas[action] = zeros(729)\n",
      "    #else:\n",
      "    thetas[action] = squeeze( dot(dot(inv(dot(X.transpose(),X)+0.1*identity(X.shape[1])),X.transpose()),Y) )\n",
      "theta_CSI = thetas.reshape(3645)\n",
      "fCSI_reward = lambda sa:dot(theta_CSI.transpose(),phi(sa))\n",
      "theta_CSI.min(),theta_CSI.max(),theta_CSI.mean(),theta_CSI.var()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "(-0.18020829629255666,\n",
        " 0.87818100244135722,\n",
        " 0.00077942998079289676,\n",
        " 0.0069801983477599302)"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def Sgenerator( ):\n",
      "    for v in range(0,3):\n",
      "        for x_b in range(0,9):\n",
      "            for y_r in range(0,9):\n",
      "                for x_r in range(0,3):\n",
      "                    yield [v,x_b,y_r,x_r]\n",
      "\n",
      "S = [s for s in Sgenerator()]\n",
      "\n",
      "A = range(0,5)\n",
      "\n",
      "def s_index( state ):\n",
      "    v = state[0]\n",
      "    x_b = state[1]\n",
      "    y_r = state[2]\n",
      "    x_r = state[3]\n",
      "    index = x_r + y_r*3 + x_b*3*9 + v*3*9*9\n",
      "    return index\n",
      "\n",
      "def sa_index( state, action ):\n",
      "    index = s_index(state) + action*3*9*9*3\n",
      "    return index\n",
      "\n",
      "reward_CSI=zeros((3645,1))\n",
      "for state in S:\n",
      "    for action in ACTION_SPACE:\n",
      "        index = sa_index(state, action)\n",
      "        reward_CSI[index] = fCSI_reward(array([s_index(state),action]))\n",
      "Highway2 = MDP(P,reward_CSI)\n",
      "mPi_A, V_A, Pi_A = Highway2.optimal_policy()\n",
      "Highway.evaluate(mPi_A)\n",
      "#reward_CSI.min(),reward_CSI.max(),reward_CSI.mean(),reward_CSI.var()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MDP OK3\n",
        "Shape of P (3645, 729)\n",
        "Shape of R (3645, 1)\n",
        "Card of S 729\n",
        "Card of A 5\n",
        "Card of SA 3645\n",
        "Iteration 0, 602\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 1, 878\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 2, 512\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 3, 162\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 4, 52\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 5, 10\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 6, 0\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 21,
       "text": [
        "7.342572892181054"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "Highway3 = MDP(P,rand(3645,1))\n",
      "mPi_R, V_R, Pi_R = Highway3.optimal_policy()\n",
      "Highway.evaluate(mPi_R)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "MDP OK3\n",
        "Shape of P (3645, 729)\n",
        "Shape of R (3645, 1)\n",
        "Card of S 729\n",
        "Card of A 5\n",
        "Card of SA 3645\n",
        "Iteration 0, 1244\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 1, 892\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 2, 550\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 3, 334\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 4, 224\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 5, 86\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 6, 24\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 7, 6\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Iteration 8, 0\tactions changed."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      },
      {
       "output_type": "pyout",
       "prompt_number": 22,
       "text": [
        "-0.69886028633730768"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "savetxt('RewardFaisantPlanterDP.mat',reward_CSI)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    }
   ],
   "metadata": {}
  }
 ]
}