% Created 2012-06-27 Mer 16:06
\documentclass[11pt]{article}
\usepackage[plain,cm]{fullpage}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage[integrals]{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{amsmath}
\usepackage[frenchb]{babel}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{bibtopic}
\providecommand{\alert}[1]{\textbf{#1}}

\title{Rapport Intermédiaire de Thèse en Informatique à l'Université de Lorraine (\'Ecole doctorale IAEM)\\
Découverte automatique d'attributs pour l'imitation}
\author{Edouard KLEIN}
\date{\today}
\hypersetup{
  pdfkeywords={},
  pdfsubject={},
  pdfcreator={Emacs Org-mode version 7.8.11}}

\begin{document}

\maketitle
\section*{Sujet de thèse}
L'adoption de systèmes automatisés autonomes peut réduire les coûts et les risques, cependant la complexité de la mise au point de tels systèmes gêne leur adoption généralisée. Des approches permettant à ces systèmes de prendre en charge des environnements de plus en plus difficiles font apparaître de nouveaux défis. L'un de ceux-ci est la définition de la consigne. A mesure que les tâches confiées aux systèmes automatiques se complexifient, la définition de ces dernières devient moins aisée.\\

  L'\emph{apprentissage par renforcement inverse} (ARI), cadre dans lequel s'inscrit cette thèse, a pour objet de contourner les difficultés évoquées : de la même manière que les jeunes gens n'apprennent pas à conduire en lisant le manuel de leur auto mais en observant leurs parents et leurs moniteurs de conduite puis en se mettant derrière le volant, nous comptons apprendre la tâche à effectuer en observant un \emph{expert} la réaliser. Cette démarche exploite la capacité humaine à résoudre intuitivement et rapidement des conflits qu'il serait difficile d'analyser sur papier. Pour reprendre l'exemple précédent, un automobiliste saura après un rapide coup d'œil dans son rétroviseur s'il vaut mieux qu'il pile ou qu'il déboîte en urgence et effectuera sa manœuvre dans la foulée.\\

  Notre but est de dériver, du comportement d'un expert effectuant une tâche, une description de cette tâche sous la forme d'une fonction de récompense, ce qui permet ensuite l'utilisation des outils d'apprentissage par renforcement pour apprendre cette tâche à un agent. Cela ouvrirait le champ d'application de l'apprentissage par renforcement à des tâches encore inaccessibles car trop complexes pour être ``expliquées''.\\
  L'intitulé de la thèse, \emph{Découverte automatique d'attributs pour l'imitation}, isole une partie du problème : il s'agit d'extraire de la description de la suite d'\emph{états} traversés par l'expert les informations pertinentes à l'expression de la récompense.\\

\section*{Contributions au domaine}
\label{sec-3}

  Il faut attendre 2000 pour qu'une publication pose formellement le problème. La contribution centrale intervient en 2004 et introduit la notion d'\emph{attribut moyen}, que suggérait déjà l'analyse de 2000. Après 2004, plusieurs travaux apparaissent qui utilisent cette notion d'attribut moyen .\\
  L'attribut moyen est une mesure liée à la distribution des états que traverse un agent en suivant sa politique dans l'environnement. Cette mesure tient une place centrale dans beaucoup des algorithmes existants, notamment car deux agents ayant des attributs moyens similaires rempliront une certaine tâche (\emph{i.e.}, optimiseront une certaine récompense) de manière similaire.\\

  La première contribution au domaine consiste en un mécanisme de calcul de cet attribut moyen \cite{klein2011batch}. Inspiré d'algorithmes existants pour l'approximation de fonction de valeur, thème central en apprentissage par renforcement, cette contribution apporte une méthode de calcul permettant l'évaluation \emph{off-policy} de l'attribut moyen d'une politique. Sans l'anglicisme, cela signifie que l'on peut évaluer une grandeur relative à une politique en observant \emph{une autre politique} (comme par exemple celle de l'expert). Testée en l'injectant dans l'approche centrale de 2004, cette idée a permis de résoudre les problème de l'apprentissage par renforcement inverse sur un problème jouet simple à partir des seules données issues de l'expert, mais n'a pas permis de complètement lever les obstacles imposés par la structure des algorithmes existants. Ceux-ci nécessitent en effet dans leur immense majorité de résoudre le problème direct (celui de l'apprentissage par renforcement) de manière répétée. Cela n'est pas toujours possible uniquement avec les données de l'expert, il faut une autre source d'information.\\
  Cette contribution permet néanmoins de s'affranchir d'un simulateur pour estimer une grandeur centrale dans la majorité des approches du domaine. Cela permet d'éviter les soucis liés à la modélisation de l'environnement ou à un trop grand besoin en données (coûteuses à générer).
  Une autre contribution de plus faible envergure consiste en un apport sur la définition mathématique formelle du problème \cite{klein2011dimensionality} permettant de réduire l'espace dans lequel on doit chercher les solutions.\\
  Enfin, lors de la deuxième année de thèse, deux nouveaux algorithmes ont été trouvés qui permettent la résolution de l'ARI dans des domaines où les algorithmes existant ne fonctionnent pas. L'un de ces algorithmes a été publié dans une conférence francophone \cite{Supelec773} ainsi qu'une conférence internationale \cite{klein2012}. L'autre est en cours de relecture.

  Il m'a été permis de participer à cinq conférences en temps qu'auteur, j'ai pu rencontrer la communauté \emph{Machine Learning} au sens large à IJCAI \footnote{\href{http://ijcai-11.iiia.csic.es/}{http://ijcai-11.iiia.csic.es/}
 }, ainsi qu'à ECML\footnote{\href{http://www.ecmlpkdd2011.org/}{http://www.ecmlpkdd2011.org/}
 } qui précédait EWRL\footnote{\href{http://ewrl.wordpress.com/past-ewrl/ewrl9-2011/}{http://ewrl.wordpress.com/past-ewrl/ewrl9-2011/}
 } où j'ai pu présenter mes travaux à un public plus directement concerné du fait du sujet précis de ce \emph{workshop}. Enfin j'ai rencontré la communauté francophone à JFPDA\footnote{\href{https://zanuttini.users.greyc.fr/jfpda2011/}{https://zanuttini.users.greyc.fr/jfpda2011/}} puis l'année suivante à CAp \footnote{http://cap2012.loria.fr/}. J'ai à nouveau participé à EWRL en 2012. Les échanges formels et informels permis par ces déplacements ont enrichi ma culture et ma reflexion.

  
\section{Perspectives}
\label{sec-4}

Outre la rédaction de la thèse, qui sera commencée aux alentours de Février pour une remise du manuscrit en Juin et une soutenance en Septembre, cette année sera occupée par l'application de nos nouveaux algorithmes sur des problèmes complexes et tirés de cas concrets afin de démontrer empiriquement leur performance.
  
 \begin{btSect}[alpha]{Biblio.bib}
 \section{Mes publications}
 \nocite{klein2011dimensionality,klein2011batch,klein2011batch2,klein2011batch3,klein2012,Supelec773}
 \btPrintCited
 \end{btSect}

\end{document}
