@InProceedings{Supelec773,
author = {Edouard Klein and Bilal PIOT and Matthieu Geist and Olivier Pietquin},
title = {{Classification structur\'ee pour l’apprentissage par renforcement inverse}},
year = {2012},
booktitle = {{Actes de la Conf\'erence Francophone sur l'Apprentissage Automatique (CAp 2012)}},
address = {Nancy, France},
abstract = {Cette contribution traite du probl\`eme de l’apprentissage par imitation par le biais de l’apprentissage par renforcement inverse (ARI). Dans ce contexte, un expert accomplit une t\^ache qu’un agent artificiel doit essayer de reproduire. L’ARI part du postulat que l’expert optimise avec succ\`es une fonction d’utilit\'e ; le probl\`eme consiste à deviner cette fonction (appel\'ee r\'ecompense) à partir de traces du comportement de l’expert. Les algorithmes d’ARI existants n\'ecessitent une ou plusieurs des conditions suivantes pour fonctionner : trajectoires compl\`etes de la part de l’expert, un mod\`ele g\'en\'eratif pour les estimations de type Monte-Carlo, la connaissance des probabilit\'es de transition, la capacit\'e de r\'esoudre le probl\`eme direct (celui de l’apprentissage par ren- forcement) de mani\`ere r\'ep\'et\'ee ou l’acc\`es à la strategie compl\`ete de l’expert. Notre con- tribution consiste en un nouvel algorithme d’ARI levant l’ensemble de ces contraintes. En utilisant une m\'ethode supervis\'ee dans laquelle nous introduisons implicitement la structure du processus d\'ecisionnel de Markov (PDM) sous-jacent, nous cr\'eons un algorithme bas\'e sur une descente de sous-gradient, poss\'edant une faible complexit\'e tant en \'echantillons que calculatoire et surtout ne n\'ecessitant pas la r\'esolution du probl\`eme direct. }
}

%% Saved with string encoding Unicode (UTF-8) 
@InProceedings{klein2011dimensionality,
author = {Edouard Klein and Matthieu Geist and Olivier Pietquin},
title = {{Reducing the dimentionality of the reward space in the Inverse Reinforcement Learning problem}},
year = {2011},
booktitle = {{Proceedings of the IEEE Workshop on Machine Learning Algorithms, Systems and Applications (MLASA 2011)}},
pages = {4 pages},
month = {December},
address = {Honolulu (USA)},
url = {http://www.metz.supelec.fr//metz/personnel/geist_mat/pdfs/supelec761.pdf},
abstract = {This paper deals with the Inverse Reinforcement Learning framework, whose purpose is to learn control policies from demonstrations by an expert. This method inferes from demonstrations a utility function the expert is allegedly maximizing. In this paper we map the reward space into a subset of smaller dimensionality without loss of generality for all Markov Decision Processes (MDPs). We then present three experimental results showing both the promising aspect of the application of this result to existing IRL methods and its shortcomings. We conclude with considerations on further research.}
}



@inproceedings{klein2012,
author = {Edouard Klein and Bilal Piot and Matthieu Geist and Olivier Pietquin},
title = {{Structured Classification for Inverse Reinforcement Learning}},
year = {2012},
booktitle = {{European Workshop on Reinforcement Learning (EWRL 2012)}},
address = {Edinburgh (UK)},
abstract = {TBD}
}
@inproceedings{klein20122,
author = {Edouard Klein and Bilal Piot and Matthieu Geist and Olivier Pietquin},
title = {{Structured Classification for Inverse Reinforcement Learning}},
year = {2012},
booktitle = {{NIPS}},
address = {Lake Tahoe (USA)},
abstract = {TBD}
}

@inproceedings{klein2011batch,
author = {Edouard Klein and Matthieu Geist and Olivier Pietquin},
title = {{Batch, Off-policy and Model-free Apprenticeship Learning}},
year = {2011},
booktitle = {{Proceedings of the European Workshop on Reinforcement Learning (EWRL 2011)}},
publisher = {Springer Verlag - Heidelberg Berlin},
pages = {12 pages},
month = {september},
series = {Lecture Notes in Computer Science (LNCS)},
address = {Athens (Greece)},
url = {http://www.metz.supelec.fr//metz/personnel/geist_mat/pdfs/supelec726.pdf},
abstract = {This paper addresses the problem of apprenticeship learning, that is learning control policies from demonstration by an expert. An efficient framework for it is inverse reinforcement learning (IRL). Based on the assumption that the expert maximizes a utility function, IRL aims at learning the underlying reward from example trajectories. Many IRL algorithms assume that the reward function is linearly parameterized and rely on the computation of some associated feature expectations, which is done through Monte Carlo simulation. However, this assumes to have full trajectories for the expert policy as well as at least a generative model for intermediate policies. In this paper, we introduce a temporal difference method, namely LSTD-mu, to compute these feature expectations. This allows extending apprenticeship learning to a batch and off-policy setting.}
}

@inproceedings{klein2011batch2,
author = {Edouard Klein and Matthieu Geist and Olivier Pietquin},
title = {{Apprentissage par imitation \'etendu au cas batch, off-policy et sans mod\`ele}},
year = {2011},
booktitle = {{Sixi\`emes Journ\'ees Francophones de Planification, D\'ecision et Apprentissage pour la conduite de syst\`emes (JFPDA 2011)}},
pages = {9 pages},
month = {June},
address = {Rouen (France)},
url = {http://www.metz.supelec.fr/~geist_mat/pdfs/Supelec702.pdf},
abstract = {Ce papier s'int\'eresse au probl\`eme de l'apprentissage par imitation, c'est à dire la r\'esolution du probl\`eme du contr\^ole optimal à partir de donn\'ees tir\'ees d'une d\'emonstration d'expert. L'apprentissage par renforcement inverse (IRL) propose un cadre efficace pour r\'esoudre ce probl\`eme. En se basant sur l'hypoth\`ese que l'expert maximise un crit\`ere, l'IRL essaie d'apprendre la r\'ecompense qui d\'efinit ce crit\`ere à partir de trajectoires d'exemple. Beaucoup d'algorithmes d'IRL font l'hypoth\`ese de l'existence d'un bon approximateur lin\'eaire pour la fonction de r\'ecompense et calculent l'attribut moyen (le cumul moyen pond\'er\'e des fonctions de base, relatives à la param\'etrisation lin\'eaire suppos\'ee de la r\'ecompense, \'evalu\'ees en les \'etats d'une trajectoire associ\'ee à une certaine politique) via une estimation de Monte-Carlo. Cela implique d'avoir acc\`es à des trajectoires compl\`ete de l'expert ainsi qu'à au moins un mod\`ele g\'en\'eratif pour tester les politiques interm\'ediaires. Dans ce papier nous introduisons une m\'ethode de diff\'erence temporelle, LSTD-$\mu$, pour calculer cet attribut moyen. Cela permet d'\'etendre l'apprentissage par imitation aux cas batch et off-policy.}
}
@inproceedings{klein2011batch3,
author = {Edouard Klein and Matthieu Geist and Olivier Pietquin},
title = {{Batch, Off-policy and Model-Free Apprenticeship Learning}},
year = {2011},
booktitle = {{IJCAI Workshop on Agents Learning Interactively from Human Teachers (ALIHT 2011)}},
month = {July},
note = {6 pages},
address = {Barcelona (Spain)},
url = {http://www.cs.utexas.edu/~bradknox/IJCAI-ALIHT11/Accepted_Papers.html},
abstract = {This paper addresses the problem of apprenticeship learning, that is learning control policies from demonstration by an expert. An efficient framework for it is inverse reinforcement learning (IRL). Based on the assumption that the expert maximizes a utility function, IRL aims at learning the underlying reward from example trajectories. Many IRL algorithms assume that the reward function is linearly parameterized and rely on the computation of some associated feature expectations, which is done through Monte Carlo simulation. However, this assumes to have full trajectories for the expert policy as well as at least a generative model for intermediate policies. In this paper, we introduce a temporal difference method, namely LSTD-mu, to compute these feature expectations. This allows extending apprenticeship learning to a batch and offpolicy setting.}
}