#+LaTeX_CLASS: beamer

#+LaTeX_HEADER: \usetheme[secheader]{Boadilla}
#+LaTeX_HEADER: \usepackage[english]{babel}
#+LaTeX_HEADER: \setbeamercolor{title}{fg=black,bg=black!10!brown!50}
#+LaTeX_HEADER: \setbeamercolor{block body}{fg=black,bg=black!10!brown!30}
#+LaTeX_HEADER: \setbeamercolor{block title}{fg=black,bg=black!30!brown!40}

#+LaTeX_HEADER: \setbeamercolor{frametitle}{fg=black,bg=black!30!brown!50}
#+LaTeX_HEADER: \beamersetaveragebackground{brown!50!black!20}

#+LaTeX_HEADER: \setbeamercolor{author in head/foot}{fg=black,bg=black!30!brown!50}
#+LaTeX_HEADER: \setbeamercolor{title in head/foot}{fg=black,bg=black!20!brown!50}
#+LaTeX_HEADER: \setbeamercolor{date in head/foot}{fg=black,bg=black!10!brown!50}

#+LaTeX_HEADER: \setbeamercolor{section in head/foot}{fg=black,bg=black!30!brown!30}
#+LaTeX_HEADER: \setbeamercolor{subsection in head/foot}{fg=black,bg=black!20!brown!30}

#+LaTeX_HEADER: \usepackage{animate} %need the animate.sty file 

#+LaTeX_HEADER: \usepackage{color}
#+LaTeX_HEADER: \usepackage[ruled]{algorithm2e}


#+LaTeX_HEADER: \include{headertikz}
#+LaTeX_HEADER: \usetikzlibrary{decorations.pathmorphing,shapes.misc}

#+BEAMER_HEADER_EXTRA:\title{Structured Classification for Inverse Reinforcement Learning}
#+BEAMER_HEADER_EXTRA:\author{{\Small European Workshop on Reinforcement Learning}\\\underline{Edouard Klein}$^{\dag\ddag}$, Bilal Piot$^{\dag\textrm{\Ankh}}$, Matthieu Geist$^\dag$ and Olivier Pietquin$^{\dag\textrm{\Ankh}}$\\\texttt{firstname.lastname@supelec.fr}}
#+BEAMER_HEADER_EXTRA:\institute[Supélec]{$\dag$Equipe IMS/MaLIS (Supélec), France\\$\ddag$Equipe ABC UMR 7503 (Loria-CNRS), France\\\Ankh UMI 2958 (GeorgiaTech-CNRS)}


#+COLUMNS: %40ITEM %10BEAMER_env(Env) %9BEAMER_envargs(Env Args) %4BEAMER_col(Col) %10BEAMER_extra(Extra)
#+OPTIONS: toc:nil
#+BEAMER_FRAME_LEVEL: 3
#+TITLE: Structured Classification for Inverse Reinforcement Learning
#+AUTHOR: Edouard Klein and and Bilal Piot and Matthieu Geist and Olivier Pietquin

#+Begin_LaTeX
\tikzstyle{state}=[circle,
thick,
minimum size=1.0cm,
draw=blue!80,
fill=blue!20]
\tikzstyle{action}=[rectangle,thick,
minimum size=1.0cm,
draw=orange!80,
fill=orange!20]
\tikzstyle{element}=[rectangle,
thick,
minimum size=1.0cm,
draw=blue!80,
fill=blue!20]
\tikzstyle{action}=[rectangle,thick,
minimum size=1.0cm,
draw=orange!80,
fill=orange!20]
#+end_LaTeX
* Non-technical Abstract
** Context
*** Imitation: Expert

**** Null					     :BMCOL:B_ignoreheading:
    :PROPERTIES:
    :BEAMER_col: .4\textwidth
    :BEAMER_env: ignoreheading
    :END:
     #+BEGIN_LaTeX
     \begin{overlayarea}{\textwidth}{5cm}
     \only<1>{\animategraphics[autoplay,loop,height=5cm]{1}{Expert00}{1}{9} }
     \only<2>{ \animategraphics[autoplay,loop,height=5cm]{1}{Agent}{001}{014} }
     \end{overlayarea}
     #+END_LaTeX

**** Expert 						      :BMCOL:B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :BEAMER_col: .4\textwidth
    :END:
     - The expert is an optimal agent in an MDP
     - Its behavior is observed

**** Apprenticeship learning				     :BMCOL:B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     Reward inference
** Contribution
*** Contribution 
**** SCIRL 							    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :ORDERED:  t
    :END:
     - SCIRL Algorithm
       - Classification with the structure of the MDP
       - Only needs data from the expert
       - Can use other data if available
       - Different subroutines for different settings
     - Theoretical results
     - Experimental results
* IRL
** RL
*** Quick definitions
     #+BEGIN_LaTeX
       \begin{columns}
    \begin{column}{4cm}
      \begin{block}{}
        \begin{overlayarea}{\textwidth}{4.4cm}
          \only<1->{\input{img/MDP4.tex}}
        \end{overlayarea}
      \end{block}
    \end{column}
    \begin{column}{4cm}
      \begin{block}{Notions}
        \begin{itemize}
          \item<1-> State $s_t\in S$
          \item<1-> Action $a_t \in A$
          \item<1-> Reward $r_t \in \mathbb{R}$
          \item<1-> Transition $(s_t,a_t,s_{t+1},r_t)\in S\times A\times S\times\mathbb{R}$
     \item     $\pi: S\rightarrow A$
        \end{itemize}
      \end{block}
      \begin{block}<1->{Markovian criterion}
        Past states are irrelevant
      \end{block}
    \end{column}
  \end{columns}
     #+END_LaTeX

*** RL problem and solution
**** Value function						   :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     \begin{equation}
     \label{eqn:V}
     V^\pi(s_t) = E\left[\left.\sum\limits_{i}\gamma^i r_{t+i}\right|\pi\right]
     \end{equation}
**** Goal							   :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     #+begin_latex
     Optimal policy $\pi^* = \arg\max\limits_\pi V^\pi$ \hfill \uncover<1>{$\pi^*(s) = \arg\max\limits_{a} Q^{\pi^*}(s,a)$}
     #+end_latex
**** Value function approximation				   :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     $\hat V^\pi(s_t) = \omega^T\psi (s_t)$ \hfill $\hat Q^\pi(s_t,a) = \omega'^T\phi (s_t,a)$
** IRL
*** IRL problem and solutions
**** Goal							   :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     Finding the reward $R$ so that the observed behavior is optimal
**** Ill-posed 						      :B_alertblock:
    :PROPERTIES:
    :BEAMER_env: alertblock
    :END:
     The null reward $\forall s, R(s) = 0$ is a solution
**** Approximation of the reward 				   :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     $R = \theta^T\psi(s)$
*** IRL solutions
**** Introduction of the expected, cumulative, discounted feature values :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     \scriptsize
     #+begin_latex
     \begin{eqnarray*}
     Q^\pi(s_t,a) &=& E\left[\left.\sum\limits_{i}\gamma^i r_{t+i}\right|\pi,s_t,a\right] \\
     Q^\pi(s_t,a) &=& E\left[\left.\sum\limits_{i}\gamma^i \theta^T\psi(s_{t+i})\right|\pi,s_t,a\right]\\
     Q^\pi(s_t,a) &=& \theta^T\underbrace{E\left[\left.\sum\limits_{i}\gamma^i \psi(s_{t+i})\right|\pi,s_t,a\right]}_{\textrm{\large\color{red}$\mu^\pi(s_t,a)$} }\\
     Q^\pi(s_t,a) &=& \theta^T\mu^\pi(s_t,a)
     \end{eqnarray*}
     #+end_latex
     
** SCIRL Algorithm
*** A certain class of classifiers
**** Score function based classifiers 				    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     - Classifier : map inputs $x\in \mathcal{X}$ to labels $y \in \mathcal{Y}$
     - Data : $\{(x_i,y_i)_{1\leq i \leq N}\}$
     - Decision rule : $g\in\mathcal{Y}^\mathcal{X}$
     - Score function : $g_s(x) \in \arg\max\limits_{y\in\mathcal{Y}}s(x,y)$
     
**** Linearly parameterized score function 			    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     \begin{equation}
     s(x,y) = \theta^T \phi(x,y)
     \end{equation}
     
*** The idea behind SCIRL
#+begin_latex
\uncover<2->{
\begin{alertblock}{Putting it all together}
    \uncover<2->{$\mathcal{X} \equiv S$}, \uncover<3->{$\mathcal{Y} \equiv A$}, \uncover<4->{$s\equiv Q^{\pi_E}} \uncover<5->{\Rightarrow \phi \equiv \mu^{\pi_E}}$
\end{alertblock}
}
#+end_latex
**** Linearly parametrized score function based classifiers 	    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :BEAMER_col: .45
    :END:
     - $g_s(x) \in \arg\max\limits_{y\in\mathcal{Y}}s(x,y)$
     - $s(x,y) = \theta^T \phi(x,y)$
     - $g_s(x) \in \arg\max\limits_{y\in\mathcal{Y}}\theta^T\phi(x,y)$
     
**** Règle de décision de l'expert 				    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :BEAMER_col: .45
    :END:
     - $\pi_E(s) = \arg\max\limits_{a} Q^{\pi_E}(s,a)$
     - $Q^\pi(s_t,a) = \theta^T\mu^\pi(s_t,a)$
     - $\pi_E(s) = \arg\max\limits_{a} \theta^T\mu^{\pi_E}(s,a)$
     
*** SCIRL Pseudo-code
#+begin_latex
\begin{algorithm}[H]%[tbh]
    %\small
  %\SetVline
  \caption{SCIRL algorithm}
  \label{algo:scirl}
  %
  \BlankLine
  \emph{\textbf{Given}} a training set $\mathcal{D} = \{(s_i,a_i=\pi_E(s_i))_{1\leq i\leq N}\}$,
  an estimate $\hat{\mu}^{\pi_E}$ of the expert feature expectation $\mu^{\pi_E}$ and a classification algorithm\;
  %
  \BlankLine
  \emph{\textbf{Compute}} the parameter vector $\theta_c$ using the
  classification algorithm
  fed with the training set $\mathcal{D}$ and considering the parameterized score function
  $\theta^T\hat{\mu}^{\pi_E}(s,a)$\;
  %
  \BlankLine
  \emph{\textbf{Output}} the reward function $R_{\theta_c}(s) = \theta_c^T\psi(s)$ \;
\end{algorithm}
#+end_latex

* Theoretical results
** Analysis
*** Error bound
#+begin_latex
\begin{alertblock}<1->{Theorem}
  \begin{equation}
    0\leq
    \mathop{E}_{s\sim\rho_E}[V^*_{R_{\theta_c}}-V^{\pi_E}_{R_{\theta_c}}]
    \leq \frac{C_f}{1-\gamma}\left(\bar{\epsilon}_Q +
    \epsilon_c\frac{2\gamma\|R_{\theta_c}\|_\infty}{1-\gamma}
    \right)
  \end{equation}
\end{alertblock}
#+end_latex
* Experimental results
** Inverted Pendulum
*** Results on the Inverted Pendulum
#+begin_latex
\includegraphics[width=0.47\textwidth]{LAFEM_Exp3_true_R}\includegraphics[width=0.47\textwidth]{LAFEM_Exp3_lafem_R}\\
\vspace{-30pt}
\includegraphics[width=0.47\textwidth]{LAFEM_Exp3_Vexpert}\includegraphics[width=0.47\textwidth]{LAFEM_Exp3_Vagent}\\
#+end_latex
** Highway Driving
*** Results on the driving problem
**** Toto 					      :BMCOL:B_ignoreheading:
    :PROPERTIES:
    :BEAMER_env: ignoreheading
    :END:
#+begin_latex
     \uncover<2->{
\includegraphics[width=0.47\textwidth]{Cascading_Exp5_fig2}
     \includegraphics[width=0.47\textwidth]{SCIRL_Exp3_fig1}
}
#+end_latex
**** Description 						    :B_block:
    :PROPERTIES:
    :BEAMER_env: block
    :END:
     - Widespread benchmark
     - Goal of the expert : avoid other cars, do not go off-road, go fast
     - Using only data from the expert and natural features

* Opening and future work
** Future work
*** Possible future work
**** Real-world problem (eNTERFACE'12)
**** Task Transfer (same state space, different action space)
*** Thank you...
    ... for your attention

#* Corrections
#** TODO Petits textes en bas
#** TODO Expliquer d'où vient mu
#** TODO Mettre des uncover dans le .tex
#** TODO Commiter le tout

