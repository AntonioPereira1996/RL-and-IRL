#+TITLE: Exact dynamic programming algorithms


* Matricial representation
  We deal here with environments that can be represented as matrices. In other words, we deal with finite state space, finite action space MDPs. Note that this can be generalized to a non markovian framework.. We leave it as an exercise for the reader ;)

  As they are finite, the state, action, and state action space are indexable by a subset of $\mathbb{N}$, namely $[\![1,|S|]\!]$, $[\![1,|A|]\!]$ and $[\![1,|S\times A ]\!]$.

** Environment
   The environment matrix $\mathbf P$ is of size $|S\times A|\times|S|$, where the element at line =sa_index($s$,$a$)= and column =s\_index( $s'$)= is $p(s'|s,a)$, the probability of transitionning in state $s'$ upon taking action $a$ in state $s$.
** Policy
   A policy (a (usually deterministic) mapping from $S$ to $A$) can be represented in the very sparse matrix $\mathbf \pi$ of size $|S|\times|S\times A|$, where the element at line =s_index($s$)= and column =sa_index($s''$,$a$)= is $\delta(s=s'')\pi(a|s)$. This representation could be used to represent a larger framework of control policies, but we restrict ourselves to Markovian, stationary policies.
** Policy probability transition matrix
   As a direct consequence of the chosen matricial representation, the matrix $\mathbf{P^\pi}$ of size $|S|\times|S|$ where the element at line =s_index($s$)= and column =s_index($s'$)= is $p(s'|s,\pi(s))$ can be expressed by $\mathbf{P^\pi} = \mathbf{\pi}\mathbf{P}$.
   Note that function composition is left to right and not right to left as with functional notation.
** Reward representation
   Althoug a reward over $S$ pnly may have its advantages, we here use rewards over $S\times A$, which is jsut a generalization.
** Value function
   The value function, over $S$, can be computed from the matricial bellman evaluation equation :
   $\mathbf{V} = \mathbf{\pi R}+\gamma\mathbf{P^\pi V}$, we invert this in : $\mathbf{V} = (\mathbf{I}-\gamma\mathbf{P^\pi})^{-1}\mathbf{\pi R}$.
** Quality function
   The quality function can be computed from the value function thanks to : $\mathbf{Q} = \mathbf{R} + \gamma \mathbf{PV}$
   
* Code

We encode a $MDP = \{S,A,R,\gamma,P,\phi,\psi\}$ like this :
#+begin_src python :tangle DP.py
from numpy import *
from RWC import *
import scipy
import pdb


class MDP:
    def __init__( self ):
        self.card_SA = self.card_S*self.card_A
    def s_index( self, s ): 
        raise NotImplementedError, "Not implemented in MDP"
    def a_index( self, a ):
        raise NotImplementedError, "Not implemented in MDP"
    def sa_index( self, sa ):
        return self.s_index( sa[0] ) + self.card_S*self.a_index( sa[1] )
    def S( self ):
        raise NotImplementedError, "Not implemented in MDP"
    def A( self ):
        raise NotImplementedError, "Not implemented in MDP"
    def psi( self, s, a=None):
        raise NotImplementedError, "Not implemented in MDP"
    def phi( self, s,a ):
        raise NotImplementedError, "Not implemented in MDP"
    def P( self ):
        raise NotImplementedError, "Not implemented in MDP"
    def Ppi( self, pi ):
        return dot( pi, self.P() )
    def R( self ):
        "R is expected to be over $S\times A$"
        raise NotImplementedError, "Not implemented in MDP"
    def reward( self, s, a=None, sdash=None ):
        if a==None and sdash==None:
            return self.R()[self.s_index(s)]
        elif a!=None and sdash==None:
            return self.R()[self.sa_index([s,a])]
        raise NotImplementedError, "Not implemented in MDP"
    def simul( self, s, a ):
        index = self.sa_index( [s,a] )
        P = self.P()
        choices = [(s,P[index,self.s_index(s)]) for s in self.S()]
        return weighted_choice( choices )
    def control( self, s, pi ):
        index = self.s_index( s )
        choices = [(a,pi[index,self.sa_index([s,a])]) for a in self.A()]
        return weighted_choice( choices )
    def pi_plot( self, pi ):
        raise NotImplementedError, "Not implemented in MDP"
    def r_plot( self, R ):
        raise NotImplementedError, "Not implemented in MDP"
    def mean_simrun_length( self, pi ):    
        raise NotImplementedError, "Not implemented in MDP"
    def rho_0( self, s):
        return 1./self.card_S
    def s_0( self ):
        choices = [ (s, self.rho_0(s) ) for s in self.S()]
        return weighted_choice( choices )
    def D( self, control, M=1,L=1,rho=None, reward=None ):
        if rho==None:
            rho = self.rho_0
        answer = []
        for m in range(0,M):
            s = self.s_0()
            for l in range(0,L):
                a = control( s )
                s_dash = self.simul( s, a )
                r = 0
                if reward != None:
                    r = reward( s, a, s_dash )
                trans = [s,a,s_dash,r]
                answer.append( trans )
                s = s_dash
        return answer
    def DPSA(self, R=None):
        "Exact dynamic programming algorithm, the reward vector is over the state action space"
        if R==None:
            R = self.R()
        V = zeros((self.card_S,1))
        Pi = zeros((self.card_S,self.card_SA))
        oldPi = Pi.copy()
        while True: #Do..while
            changed = False
            V = linalg.solve( identity( self.card_S ) - self.gamma*dot(Pi,self.P()), dot( Pi, R) )
            Q = R + self.gamma*dot( self.P(),V)
            Pi = self.Q2Pi( Q )
            if( all( Pi == oldPi ) ):
                break
        return Pi
    def Q2Pi( self, Q ):
        pi = {}
 #       for s in self.S():
            #HERE, WE ARE HERE !!!!!!!!!!!
  #          old_a = Pi[ s_index( s )]
  #          old_V = V[ s_index( s )]
  #          qvalues_a = map( 
   #             lambda a:[QSA( Reward,V,s,s_index,a,Pactions,a_index,sa_index),a],
    #            [a for a in Actions] )
#            q,a = max( qvalues_a, key = lambda qa: qa[0] )
#            Pi[ s_index( s ) ] = a
#            V[ s_index( s ) ] = q
#            if Pi[ s_index( s ) ] != old_a or V[ s_index( s ) ] != old_V:
#                changed = True
#        if not changed:
#            break
#    Ppi = zeros( Pactions[0].shape )
#    for s in States:
#        Ppi[ s_index( s ) ] = Pactions[ int(Pi[ s_index( s ) ])][s_index( s )]
#    savetxt( filename, V, "%e", "\t" )
  #   return pi

#+end_src

#+begin_src python :tangle testDP.py
from DP import *
from RWC import *

class MoveNStay(MDP):
    card_S = 2
    card_A = 2
    def s_index( self, s):
        return s
    def a_index( self, a):
        return 0 if a=="stay" else 1
    def S(self):
        return [0,1]
    def A(self):
        return ["stay","move"]
    def P(self):
        return array([[0.75,0.25],
                      [0.25,0.75],
                      [0.25,0.75],
                      [0.75,0.25]])
    def pi( self ):
        return array([[.5,0,.5,0],
                     [0,0.25,0,0.75]])
    

MnS = MoveNStay()
D = MnS.D( lambda s:MnS.control(s,MnS.pi()),M=1,L=300)
print "<html><body>"
stay_cnt = 0.
stay_succ = 0.
move_cnt = 0.
move_succ = 0.
for trans in D:
    if trans[1] == 'stay':
        stay_cnt+=1.
        if trans[0] == trans[2]:
            stay_succ+=1.
    else:
        move_cnt+=1.
        if trans[0] != trans[2]:
            move_succ+=1.
    #print trans
    #print "<br/>" 

print "Move success rate bonjour : %f <br/>"%(move_succ/move_cnt)
print "Stay success rate : %f<br/>"%(stay_succ/stay_cnt)

Ppi = MnS.Ppi( MnS.pi() )
for row in Ppi:
    print row
    print"<br/>"
    

print "</body></html>"

#+end_src

#+srcname: testDP_make
#+begin_src makefile
testDP.py: DP.org
	tangle DP.org

testDPcompile: testDP.py DP.py 
	python testDP.py > testDP.html

testDPstartView: testDPcompile
	uzbl-browser testDP.html --named testDP&

testDPview: testDPcompile
	echo reload > /tmp/uzbl_fifo_testDP

testDPstopView:
	echo exit > /tmp/uzbl_fifo_testDP


#+end_src

    #+begin_src python :tangle DP.py

#FIXME does not work hen action have a dimension that is more than 1, and don't begin at 0
#I'm not fixing this now because this is the convention we use everywhere
def V2omega( R, V, States, s_index, Pactions, sa_index ):
    dicOmega = {} #We don't know the cardinal of the state space in advance
    for state in States:
        for action in range(0,len(Pactions)):
            Pa = Pactions[ action ]
            #Q(s,a) = R(s) + \gamma Pa(s)^TV
            #FIXME: Gamma est code en dur ici
            dicOmega[ sa_index( state, action ) ] = \
            R[ s_index( state ) ] + 0.9*dot( Pa[ s_index( state ), : ], V )
    size = max( dicOmega.keys() ) + 1
    omega = zeros(( size, 1 ))
    for i in range( 0, size ):
        omega[ i ] = dicOmega[ i ]
    return omega

#FIXME: try to use numpy.armgax() somehow instead of this
def greedy_policy( state, omega, phi, Actions, l=lambda sa:0 ):
    "returns \arg\max_a \omega^T\phi(s,a) + l(s,a)"
    s_besta = max( [[state, a] for a in Actions], key=lambda sa :
                       dot( omega.transpose(), phi( sa[0], sa[1] )) + l(sa))
    return s_besta[ 1 ]

def omega2pi( omega, phi, States, s_index, Pactions ):
    "Returns the probability matrix describing the greedy policy\
    with respect to the Q function described by \omega^T\phi"
    dicPi = {} #We don't know the cardinal of the state space in advance
    for state in States:
        action = greedy_policy( state, omega, phi, range(0, len(Pactions)))
        Pa = Pactions[ action ]
        dicPi[ s_index(state) ] = Pa[ s_index(state) ]
    size = max( dicPi.keys() ) + 1
    Pi = zeros(( size, size ))
    for i in range(0,size):
        Pi[ i, : ] = dicPi[ i ]
    return Pi

def DP( Reward, States, s_index, Actions, Pactions, a_index, sa_index, filename ):
    "Exact dynamic programming algorithm, the reward vector is over the states"
    RSA = zeros( len(States) * len(Actions) )
    for s in States:
        for a in Actions:
            RSA[ sa_index( s, a )] = Reward[ s_index( s )]
    return DPSA( RSA, States, s_index, Actions, Pactions, a_index, sa_index, filename )

def DPSA( Reward, States, s_index, Actions, Pactions, a_index, sa_index, filename ):
    "Exact dynamic programming algorithm, the reward vector is over the state action space"
    V = zeros( len( States ))
    Pi = zeros( len( States ))
    while True: #Do..while
        changed = False
        for s in States:
            old_a = Pi[ s_index( s )]
            old_V = V[ s_index( s )]
            qvalues_a = map( 
                lambda a:[QSA( Reward,V,s,s_index,a,Pactions,a_index,sa_index),a],
                [a for a in Actions] )
            q,a = max( qvalues_a, key = lambda qa: qa[0] )
            Pi[ s_index( s ) ] = a
            V[ s_index( s ) ] = q
            if Pi[ s_index( s ) ] != old_a or V[ s_index( s ) ] != old_V:
                changed = True
        if not changed:
            break
    Ppi = zeros( Pactions[0].shape )
    for s in States:
        Ppi[ s_index( s ) ] = Pactions[ int(Pi[ s_index( s ) ])][s_index( s )]
    savetxt( filename, V, "%e", "\t" )
    return Ppi
        
def QSA( R, V, s, s_index, a, Pa, a_index, sa_index ):
    return R[sa_index(s,a)] + 0.9 * dot( (Pa[ a_index(a) ])[s_index(s)], V )

    #+end_src

#FIXME changer le nom du snippet
  #+srcname: TT_code_make
  #+begin_src makefile
DP.py:DP.org
	$(call tangle,"DP.org")
  #+end_src
