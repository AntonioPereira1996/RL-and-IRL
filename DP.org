#+TITLE: Exact dynamic programming algorithm
    #+begin_src python :tangle DP.py
from numpy import *
import scipy
import pdb

#FIXME does not work hen action have a dimension that is more than 1, and don't begin at 0
#I'm not fixing this now because this is the convention we use everywhere
def V2omega( R, V, States, s_index, Pactions, sa_index ):
    dicOmega = {} #We don't know the cardinal of the state space in advance
    for state in States:
        for action in range(0,len(Pactions)):
            Pa = Pactions[ action ]
            #Q(s,a) = R(s) + \gamma Pa(s)^TV
            #FIXME: Gamma est code en dur ici
            dicOmega[ sa_index( state, action ) ] = \
            R[ s_index( state ) ] + 0.9*dot( Pa[ s_index( state ), : ], V )
    size = max( dicOmega.keys() ) + 1
    omega = zeros(( size, 1 ))
    for i in range( 0, size ):
        omega[ i ] = dicOmega[ i ]
    return omega

#FIXME: try to use numpy.armgax() somehow instead of this
def greedy_policy( state, omega, phi, Actions ):
    "returns \arg\max_a \omega^T\phi(s,a)"
    s_besta = max( [[state, a] for a in Actions], key=lambda sa : dot( omega.transpose(), phi( sa[0], sa[1] )))
    return s_besta[ 1 ]

def omega2pi( omega, phi, States, s_index, Pactions ):
    "Returns the probability matrix describing the greedy policy\
    with respect to the Q function described by \omega^T\phi"
    dicPi = {} #We don't know the cardinal of the state space in advance
    for state in States:
        action = greedy_policy( state, omega, phi, range(0, len(Pactions)))
        Pa = Pactions[ action ]
        dicPi[ s_index(state) ] = Pa[ s_index(state) ]
    size = max( dicPi.keys() ) + 1
    Pi = zeros(( size, size ))
    for i in range(0,size):
        Pi[ i, : ] = dicPi[ i ]
    return Pi

def DP( Reward, States, s_index, Actions, Pactions, a_index, sa_index, filename ):
    "Exact dynamic programming algorithm, the reward vector is over the states"
    RSA = zeros( len(States) * len(Actions) )
    for s in States:
        for a in Actions:
            RSA[ sa_index( s, a )] = Reward[ s_index( s )]
    return DPSA( RSA, States, s_index, Actions, Pactions, a_index, sa_index, filename )

def DPSA( Reward, States, s_index, Actions, Pactions, a_index, sa_index, filename ):
    "Exact dynamic programming algorithm, the reward vector is over the state action space"
    V = zeros( len( States ))
    Pi = zeros( len( States ))
    while True: #Do..while
        changed = False
        for s in States:
            old_a = Pi[ s_index( s )]
            old_V = V[ s_index( s )]
            qvalues_a = map( 
                lambda a:[QSA( Reward,V,s,s_index,a,Pactions,a_index,sa_index),a],
                [a for a in Actions] )
            q,a = max( qvalues_a, key = lambda qa: qa[0] )
            Pi[ s_index( s ) ] = a
            V[ s_index( s ) ] = q
            if Pi[ s_index( s ) ] != old_a or V[ s_index( s ) ] != old_V:
                changed = True
        if not changed:
            break
    Ppi = zeros( Pactions[0].shape )
    for s in States:
        Ppi[ s_index( s ) ] = Pactions[ int(Pi[ s_index( s ) ])][s_index( s )]
    savetxt( filename, V, "%e", "\t" )
    return Ppi
        
def QSA( R, V, s, s_index, a, Pa, a_index, sa_index ):
    return R[sa_index(s,a)] + 0.9 * dot( (Pa[ a_index(a) ])[s_index(s)], V )

    #+end_src

#FIXME changer le nom du snippet
  #+srcname: TT_code_make
  #+begin_src makefile
DP.py:DP.org
	$(call tangle,"DP.org")
  #+end_src
