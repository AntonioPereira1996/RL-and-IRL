% Created 2012-06-15 ven. 16:27
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage[integrals]{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{amsmath}
\providecommand{\alert}[1]{\textbf{#1}}

\title{Rapport Intermédiaire de Thèse, Janvier-Décembre 2011}
\author{Klein Edouard}
\date{\today}

\begin{document}

\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\vspace*{1cm}




\section{Introduction}
\label{sec-1}

  
  Ces dernières décennies, de plus en plus de tâches sont confiées à des systèmes automatiques. L'automatisation de plus en plus poussée et l'autonomie grandissante des systèmes en place réduit de manière générale les coûts et les risques associés. Diverses techniques peuvent être envisagées pour automatiser un système, qu'il s'agisse du thermostat d'un four, d'une machine-outil, d'un robot humanoïde ou même d'un système de dialogue par téléphone.\\
  
  L'approche la plus classique, antérieure même à l'avènement de l'informatique, est celle de l'automatique. Armé d'un modèle inverse du système à contrôler, l'automaticien parvient à créer un mécanisme soumettant le système à la consigne de l'opérateur. Le succès de cette méthode est indéniable comme le prouve sa vaste adoption encore aujourd'hui (une direction assistée est un système asservi au sens de l'automatique, par exemple). Néanmoins, les informations qu'il est nécessaire d'obtenir sur le système à contrôler restent parfois inconnues. Un modèle inverse est en effet une formulation mathématique du monde permettant de trouver l'action à effectuer pour transiter entre deux états donnés. Il n'est pas possible d'obtenir une telle généricité pour la plupart des systèmes.\\
  
  Une approche moins contraignante est la planification qui à partir d'informations portant sur les effets d'une action sur le système va élaborer un \emph{plan}. Cette suite d'action sera appliquée en séquence pour atteindre le but fixé. Il est beaucoup moins difficile d'établir un modèle mathématique (dit direct) d'un système permettant de prédire les effets d'une action que de créer un modèle inverse comme celui que nécessite l'automatique.

  On rentre ici dans le cadre de la prise de décision séquentielle. Un \emph{agent} est placé aux commandes du système qui devient alors l'\emph{environnement} de l'agent. A intervalles réguliers, l'agent va intervenir et appliquer une \emph{action} sur son environnement (que nous appelions \emph{système} dans les paragraphes précédents). Les outils de planifications trouvent, à l'aide du modèle direct, le chemin que devra suivre l'agent en appliquant action après action pour, sur le long terme, atteindre le but fixé.\\

  Un défaut notable des approches de planification est leur incapacité à gérer l'incertain. L'idée même de \emph{plan} suppose que tout se déroule sans accrocs. Bien des environnements de la vie réelle ne sont malheureusement pas aussi sympathiques. Il faut permettre à l'agent d'appliquer une bonne action, même si l'action précédente n'a pas atteint le but escompté ou prévu par le modèle. A cette fin, la notion de \emph{plan} est élargie en notion de \emph{politique}. A intervalles réguliers, l'agent ne se content plus d'appliquer en aveugle une action prédéfinie mais étudie l'ensemble des informations disponibles sur sa position au sein de l'environnement (appelé \emph{état})  avant de choisir l'action adaptée. Une politique ne se contente ainsi pas de tracer un chemin que l'agent doit suivre, mais propose une action cohérente pour chacun des états dans lequel l'agent peut être amené à se trouver. Il s'agit d'un mécanisme réactif, où l'agent est en interaction constante avec son environnement.

  La description de la tâche a effectuer est fournie à l'agent sous forme de récompenses, qu'il ne touche que si l'état dans lequel il se trouve est établi comme souhaitable par l'opérateur. On peut symétriquement ``punir'' l'agent en lui donnant une récompense négative lorsqu'il se trouve dans des états indésirables, par exemple ceux qui mettent l'intégrité du système en danger. Le critère maximisé par l'agent est la somme pondérée de cette récompense au long de tous les états qu'il traverse. Maximiser la somme pondérée et non la récompense directement force l'agent à apprendre un comportement cohérent sur le long terme c'est à dire à ne pas favoriser la maximisation immédiate et éphémère de la récompense seule.

  Des outils de plus en plus puissants ont été développés dans le cadre de l'\emph{apprentissage par renforcement} (AR) pour permettre à un agent d'apprendre une politique optimisant une récompense donnée. Il est maintenant possible d'apprendre une politique sans connaître même le modèle direct, l'agent le découvrant au fur et à mesure de ses interactions avec l'environnement. Mieux, l'agent peut apprendre une politique optimale (ou quasi optimale) en observant \emph{un autre agent} interagir avec l'environnement. Cela évite de placer un agent novice aux commandes d'un système coûteux. Au sein de l'équipe IMS, plusieurs chercheurs s'intéressent aux problématiques liées à l'\emph{apprentissage par renforcement}.
\section{Sujet de thèse}
\label{sec-2}

  Les outils dont nous venons de parler permettent l'apprentissage une fois la représentation du monde mise en place et une fois fournie une fonction de récompense définissant la tâche a effectuer. Cette dernière tâche n'est pas forcément la plus facile. Si, pour les tâches d'ordinaires dévolues à l'automatique, il semble facile de transposer une description textuelle d'une tâche (``Allonger le bras'') en une fonction de récompense (``Récompense donnée lorsque l'angle du coude prend la valeur 180°'') cette facilité s'estompe rapidement à mesure que la tâche devient complexe.\\

  Prenons l'exemple de la conduite. Une description textuelle de la tâche est déjà difficile à imaginer. Il est possible de partir de quelques règles du type ``Ne pas provoquer de collisions'', ``Ne pas freiner brusquement'', ``Ne pas changer de file à l'improviste''. Mais que faire lorsque ces règles entrent en conflit ? Si la voiture devant celle de l'agent pile brusquement, doit-il à tout prix éviter la collision ? Pour ce faire, faut-il qu'il change de file à l'improviste ou qu'il freine brusquement ? Il n'est pas facile de répondre \emph{a priori} à ce type de questions qui deviennent lorsque la tâche est complexe trop nombreuses pour trouver réponse satisfaisante.\\

  Nous nous trouvons maintenant dans une situation paradoxale où nous disposons d'outils extrêmement puissants et pratiques pour apprendre un comportement dans des environnements difficiles et aléatoires, mais où nous ne sommes pas capable de spécifier la tâche que nous voulons voir l'agent apprendre. Nous pouvons heureusement contourner cette difficulté par le biais de l'\emph{apprentissage par renforcement inverse} (ARI), cadre dans lequel s'inscrit cette thèse.\\

  De la même manière que les jeunes gens n'apprennent pas à conduire en lisant le manuel de leur auto mais en observant leurs parents et leurs moniteurs de conduite puis en se mettant derrière le volant, nous comptons apprendre la tâche à effectuer en observant un \emph{expert} la réaliser. Cette démarche exploite la capacité humaine à résoudre intuitivement et rapidement des conflits dont nous serions incapables d'analyser sur papier. Pour reprendre l'exemple précédent, un automobiliste saura après un rapide coup d'œil dans son rétroviseur s'il vaut mieux qu'il pile ou qu'il déboîte en urgence et effectuera sa manœuvre dans la foulée.\\

  Notre but est de dériver, du comportement d'un expert effectuant une tâche, une description de cette tâche sous la forme d'une fonction de récompense, ce qui permet ensuite l'utilisation des outils d'apprentissage par renforcement pour apprendre cette tâche à un agent. Cela ouvrirait le champ d'application de l'apprentissage par renforcement à des tâches encore inaccessibles car trop complexes pour être ``expliquées''.\\

  Les outils utilisés pour résoudre les problèmes de prise de décision séquentielle remontent jusque dans les années 50. L'apprentissage par renforcement inverse est en comparaison un problème assez jeune, posée pour la première fois en 1998, et qui a réellement pris son essor après 2004.

  Le problème de l'ARI tarde à trouver une solution à la hauteur de ce qui se fait dans le domaine de l'AR. Il s'agit notamment d'un problème dont la formulation mathématique rigoureuse laisse à désirer. Une récompense nulle dans tous les états laisse le champ libre à l'agent qui peut alors faire ce qu'il veut sans être ni pénalisé ni récompensé. Toutes les politiques sont donc optimales vis à vis de cette récompense nulle. Particulièrement, la politique de l'expert est optimale vis à vis de la récompense nulle. Une approche mathématique naïve ne permet de trouver comme récompense justifiant le comportement de l'expert que cette solution triviale et n'apportant absolument pas la réponse à notre problème.

  Un autre soucis de l'approche est la difficulté de l'appliquer à des cas réels tant elle nécessite d'information sur l'environnement. Aucun des algorithmes existants au début de la thèse ne peut se passer de disposer du modèle de l'environnement. L'interaction seule avec l'environnement est en théorie possible, mais en pratique impossible à mettre en place tant les interactions nécessaires sont nombreuses. Il faut alors avoir recours à un simulateur.
\section{Contributions au domaine}
\label{sec-3}

  La bibliographie de ce tout nouveau domaine est très fractionnée, avec des apports venue de différentes communautés. Il faut attendre 2000 pour qu'une publication pose formellement le problème. La contribution centrale intervient en 2004 et introduit la notion d'attribut moyen, sorte d'historique du passage dans l'espace d'état. Dans la foulée de cette publication, plusieurs travaux apparaissent qui utilisent cette notion d'attribut moyen. Notamment, on compare cet historique de l'expert à l'historique de l'agent, à qui l'on apprend par modification successives de la récompense à obtenir un historique proche de celui de l'expert. Bien que l'on obtienne parfois une fonction de récompense exploitable, cette approche relève plus de l'apprentissage apr imitation que de l'apprentissage par renforcement inverse au sens philosophique du terme.\\

  La première contribution au domaine consiste en un mécanisme de calcul de cet attribut moyen \cite{klein2011batch}. Inspiré d'algorithmes existants pour l'approximation de fonction de valeur, thème central en apprentissage par renforcement, cette contribution apporte une méthode de calcul permettant l'évaluation \emph{off-policy} de l'attribut moyen d'une politique. Sans l'anglicisme, cela signifie que l'on peut évaluer une grandeur relative à une politique en observant \emph{une autre politique} (comme par exemple celle de l'expert). Couplée à l'approche centrale de 2004, cette avancée a permis de partiellement lever l'un des principaux obstacles du domaine, en ceci que sur des problèmes simples il est désormais possible de retrouver une fonction de récompense non triviale justifiant le comportement de l'expert, à partir des seules données fournies par celui-ci. Plus besoin de modèle (ou de simulateur). Si l'on dispose d'agents autres que l'expert pouvant manipuler le système assez largement, alors il est possible d'exploiter ces données pour fournir résoudre le problème de l'ARI sans avoir à mettre l'agent en cours d'apprentissage au commandes du système.\\

  Une autre contribution de plus faible envergure consiste en un apport sur la définition mathématique formelle du problème \cite{klein2011dimensionality} permettant de réduire l'espace dans lequel on doit chercher les solutions.
\section{Perspectives}
\label{sec-4}

  
  Le problème de l'ARI n'est pas résolu, c'est à dire qu'il n'existe pas de solutions applicables facilement. Néanmoins certaines idées naissent en ce moment même dans la communauté qui rendent la résolution de moins en moins contraignante. Notre apport pour le calcul de l'attribut moyen s'inscrit dans cette direction. Nous pensons que couplé à un nouveau type d'approche ne se basant pas sur un agent imitant l'expert et améliorant petit à petit son imitation, nous pourrons arriver à résoudre l'ARI.

\bibliographystyle{alpha}
\bibliography{../../Biblio/Biblio.bib}

\end{document}
