#+LaTeX_CLASS: article
#+LATEX_HEADER:\usepackage[frenchb]{babel}
#+LATEX_HEADER:\usepackage[utf8]{inputenc}
#+LATEX_HEADER:\usepackage[T1]{fontenc}
#+LATEX_HEADER:\usepackage{bibtopic}
#+TITLE:Rapport Intermédiaire de Thèse en Informatique à l'Université de Lorraine (\'Ecole doctorale IAEM)\\
#+LaTeX: \begin{center} \large
/Découverte automatique d'attributs pour l'imitation/\\Encadrée par Matthieu Geist et Yann Guermeur\\Groupe de recherche IMS/MaLIS, Supélec et \'Equipe ABC, Loria Janvier-Décembre 2011
#+LaTeX: \end{center}
------

#+begin_abstract
L'adoption de systèmes automatisés autonomes peut réduire les coûts et les risques, cependant la complexité de la mise au point de tels systèmes gêne leur adoption généralisée. Des approches permettant à ces systèmes de prendre en charge des environnements de plus en plus difficiles font apparaître de nouveaux défis. L'un de ceux-ci est la définition de la consigne. A mesure que les tâches confiées aux systèmes automatiques se complexifient, la définition de ces dernières devient moins aisée. Une méthode prometteuse est d'observer un expert (par exemple un humain effectuant la tâche) et d'extraire automatiquement la description à partir d'une analyse de son comportement. Ce rapport situe le sujet de thèse parmis les approches existantes et explique les avancées déjà effectuées, qui ont donné lieu à trois publications en conférences internationales et une en conférence francophone.
#+end_abstract
------
\pagebreak
[TABLE-OF-CONTENTS]
#+begin_comment

   

 - Critères à checker
   - [X] Parler de l'automatique
     - [X] Il existe un boucle de rétro action qui de fait prend l'état en compte
     - [X] Marche très bien par exemple sur les machine outils
     - [X] Pêche par exemple pour les systèmes à dynamique non linéaire (jeux ?)
     - [X] Unidimensionnel
     - [X] Champ vaste et assez complet
   - [X] Parler de la planif
     - [X] On connait les états dans lequel le système se trouve, on établit un plan pour aller là où on veut, recherche exhaustive ou heuristique dans l'espace des états
     - [X] Si le modèle de l'incertitude est faux ça peut merder
   - [-] Enrichir le titre
     - [X] Rajouter un abstract
     - [ ] contextualiser : Pour qui, rappel du sujet, encadrants
   - [X] Passer le doc en Français
   - [X] Parler de l'apprentissage par renforcement (et de la programmation dynamique dans un premier temps puis différencier les deux)
     - [X] Introduire la récompense comme "critère local quant à la qualité du contrôle"
     - [X] On optimise le cumul des récompenses
     - [X] Politique vs. plan
     - [X] Algo en ligne avec oubli : adaptation
     - [X] Alog hors ligne : sécurité
     - Le problème qu'on cherche à résoudre est de faire évoluer des machines dans des environnements non maîtrisés, difficile à modéliser et où les êtres vivants s'en sortent sans pour autant être totalement optimaux. Faire des machines pouvant résoudre des problèmes génériques et pas ad hoc, au prix de perdre en efficacité sur chacune des tâches. 
   - [ ] Parler de l'ARI
     - et puis sur l'ARI : toutes les méthodes optimisent un critère (autom, planif, AR) et peu de gens se posent la question de la pertinence de la consigne. C'est un problème que se posait d'ailleurs déjà Kalman apparemment ! C'est donc pas  nouveau mais c'est moins étudié.
   - [X] Sujet de thèse
     - [X] Donner à un moment le vrai intitulé
   - [ ] Mettre plus de références
   - [ ] Séparer mes références des autres
#+end_comment


* Introduction
  
  Ces dernières décennies, de plus en plus de tâches sont confiées à des systèmes automatiques. L'automatisation de plus en plus poussée et l'autonomie grandissante des systèmes en place réduit de manière générale les coûts et les risques associés. Diverses techniques peuvent être envisagées pour automatiser un système, qu'il s'agisse du thermostat d'un four, d'une machine-outil, d'un robot ou même d'un système de dialogue par téléphone.\\
  
  Pour décrire ces techniques et situer cette thèse dans ce champ foisonnant nous emploierons un vocabulaire unifié tiré principalement du domaine de l'/apprentissage par renforcement/. Nous parlerons de l'entité "intelligente" aux commandes du système à contrôler comme de l'/agent/ dont nous adoptons le point de vue. Ainsi, le système à contrôler devient l'/environnement/, qui fournit à l'/agent/ de l'information sur sa configuration actuelle, que nous appelons son /état/. En retour, l'/agent/ applique une /action/ sur l'/environnement/ afin de changer son /état/. Nous traiterons sans perte de généralité dans cette introduction d'/agents/ prenant leurs décisions /séquentiellement/ : à un moment donné, l'/agent/ choisit l'/action/ qu'il juge appropriée au vu de l'/état/ dans lequel se trouve le système, ce qui le fait /transiter/ après un certain /pas de temps/ dans un autre /état/, où il choisira à nouveau une /action/ et ainsi de suite.\\

  Une idée très importante appraît ici, celle de la /rétroaction/. L'agent n'agit pas sur l'environnement en aveugle, mais utilise l'information dont il dispose sur l'/état/ du système pour choisir l'/action/ appropriée. L'existence de cette boucle dans le trajet de l'information est indispensable à l'autonomie d'un agent.\\

  L'approche la plus classique, dont les débuts sont de loin antérieurs à l'avènement de l'informatique, est celle de l'automatique. Une /consigne/ est fournie à l'agent dont les actions vont avoir pour but de minimiser l'/erreur/ entre la /consigne/ et l'/état/. Le succès de ce domaine est indéniable comme le prouve sa vaste adoption encore aujourd'hui ; une direction assistée est un système asservi au sens de l'automatique, par exemple. La /consigne/ est fournie par le conducteur qui tourne le volant, l'/agent/ s'occupe d'orienter les roues en conséquence, en appliquant la force nécessaire quelle que soit la vitesse du véhicule ou la nature du terrain sous les roues.\\
  Dans son expression la plus générale ce domaine est centenaire, les outils développés ont atteint une grande maturité. Les machines-outils des chaînes d'assemblage montrent par exemple la précision qu'il est possible d'atteindre malgré le poids des pièces ou des bras. Le problème dit du /contrôle optimal/ est, sous certaine contraintes, résolu par les outils de l'automatique ; il est possible d'implémenter l'agent répondant du mieux possible à certaines contraintes (de temps, de consommation d'énergie, etc.).\\
  L'application des outils issus de l'automatique devient cependant moins aisée lorsque l'environnement cesse d'être stationnaire. Cela se produit facilement dès qu'un humain apparaît dans l'environnement. La consigne peut également être difficile à spécifier, par exemple lorsque l'on souhaite créer un agent jouant un jeu comme le go, les échecs ou le backgammon. Le contrôle d'environnements ne pouvant se représenter par une seule variable peut également s'avérer problématique pour les outils les plus simples de ce domaine, traditionnellement destinés à contrôler une seule valeur. On peut alors par exemple utiliser plusieurs agents en parallèle, ou les hiérarchiser.\\

  Parallèlement, la /planification/ a été développée depuis plus d'un demi-siècle. Partageant des liens avec l'automatique, ce domaine traite plus facilement les types de problèmes que nous venons de mentionner. Etant donnée une représentation des états admissibles par l'environnement, un /plan/ est élaboré. Cette suite d'action sera appliquée en séquence par l'agent pour atteindre le but fixé. L'évolution de l'environnement n'a pas besoin d'être linéaire, et celui-ci peut-être représenté par plusieurs variables. L'agent va trouver le chemin l'amenant à l'état souhaité, et "voyager" d'état en état pour l'atteindre. Des contraintes sur le chemin peuvent également être exprimées, chemin le plus rapide, le moins coûteux en temps ou en ressources. Lorsque une recherche exhaustive dans l'ensemble des états admissibles est possible, le contrôle optimal peut être atteint par l'agent. Pour les environnements difficiles, l'utilisation d'heuristiques rend le calcul plus facile (voire même tout simplement possible) au prix d'une potentielle non optimalité du contrôle. Le recours à ces heuristique devient indispensable lorsque l'espace à explorer est infini. \\
  La planification permet de gérer les environnements incertains, dans le sens où une action de l'agent n'a pas un effet déterminé, mais peut l'emmener dans plusieurs états selon une loi de probabilité. Pour obtenir un contrôle performant, il faut cependant que le modèle de l'environnement prenne en compte cette incertitude sans trop d'erreurs. Cela peut s'avérer impossible si l'environnement est non stationnaire, c'est-à-dire changeant.\\

  Il est possible d'élargir la notion de /plan/ en notion de /politique/. L'agent ne se contente plus d'appliquer en aveugle une action prédéfinie mais étudie son état avant de choisir l'action adaptée. Une politique ne se contente ainsi pas de tracer un chemin que l'agent doit suivre, mais propose une action cohérente pour chacun des états dans lequel l'agent peut être amené à se trouver. Il s'agit d'un mécanisme réactif, où l'agent est en interaction constante avec son environnement. Cela permet d'espérer une quasi-optimalité, même si parfois une action n'a pas atteint le but escompté ou prévu par le modèle. La recherche d'une politique à partir d'un modèle de l'environnement touche au domaine de la /programmation dynamique/.\\
  La consigne que fournit l'opérateur est un critère local quant à la qualité du contrôle que l'on appelle /récompense/. Le critère maximisé par l'agent est la somme pondérée des récompenses obtenues le long de tous les états qu'il traverse. Maximiser la somme pondérée et non la récompense directement force l'agent à apprendre un comportement cohérent sur le long terme.\\
  
  Ce cadre a été étendu à celui de l'/apprentissage par renforcement/. Le but est d'obtenir des machines la même versatilité que les êtres vivants. Face à un environnement inconnu, animaux et humains arrivent généralement à en apprendre les caractéristiques et à évoluer en leur sein, même si leur manière de le faire n'est pas forcément optimale. Créer un agent capable de s'adapter à son environnement permettrait de réduire les coûts d'ingénierie.\\
  Face à des environnements difficiles à modéliser, il est possible de laisser l'agent intéragir avec l'environnement jusqu'à ce que, petit à petit, il parvienne à accomplir la tâche spécifiée par la récompense. Un facteur d'oubli peut permettre à l'agent d'apprendre en permanence et ainsi d'évoluer dans des environnements changeants. Il est également possible pour un agent d'apprendre à contrôler un environnement en observant les interactions d'un autre agent avec cet environnement. Cela peut s'avérer utile dans le cas de systèmes coûteux et fragiles, qu'on l'on ne place sous les ordres de l'agent qu'une fois que celui-ci a suffisamment appris.\\

  

* Sujet de thèse
  Les outils dont nous venons juste de parler permettent l'apprentissage une fois fournie une fonction de récompense définissant la tâche à effectuer. Même les tâches représentées par une "simple" consigne en automatique peuvent être difficile à mettre sous la forme d'une fonction de récompense, car il faut y inclure les contraintes liées à l'intégrité du système, que l'automaticien aura intégré dans le modèle.\\

  Prenons l'exemple de la conduite. Une description textuelle de la tâche est déjà difficile à imaginer. Il est possible de partir de quelques règles du type "Ne pas provoquer de collisions", "Ne pas freiner brusquement", "Ne pas changer de file à l'improviste". Mais que faire lorsque ces règles entrent en conflit ? Si la voiture devant celle de l'agent pile brusquement, doit-il à tout prix éviter la collision ? Pour ce faire, faut-il qu'il change de file à l'improviste ou qu'il freine brusquement ? Il n'est pas facile de répondre /a priori/ à ce type de questions qui deviennent, lorsque la tâche est complexe, trop nombreuses pour trouver réponse satisfaisante.\\

  L'/apprentissage par renforcement inverse/ (ARI), cadre dans lequel s'inscrit cette thèse, a pour objet de contourner cette difficulté : de la même manière que les jeunes gens n'apprennent pas à conduire en lisant le manuel de leur auto mais en observant leurs parents et leurs moniteurs de conduite puis en se mettant derrière le volant, nous comptons apprendre la tâche à effectuer en observant un /expert/ la réaliser. Cette démarche exploite la capacité humaine à résoudre intuitivement et rapidement des conflits qu'il serait difficile d'analyser sur papier. Pour reprendre l'exemple précédent, un automobiliste saura après un rapide coup d'œil dans son rétroviseur s'il vaut mieux qu'il pile ou qu'il déboîte en urgence et effectuera sa manœuvre dans la foulée.\\

  Notre but est de dériver, du comportement d'un expert effectuant une tâche, une description de cette tâche sous la forme d'une fonction de récompense, ce qui permet ensuite l'utilisation des outils d'apprentissage par renforcement pour apprendre cette tâche à un agent. Cela ouvrirait le champ d'application de l'apprentissage par renforcement à des tâches encore inaccessibles car trop complexes pour être "expliquées".\\
  L'intitulé de la thèse, /Découverte automatique d'attributs pour l'imitation/, isole une partie du problème : il s'agit d'extraire de la description de la suite d'/états/ traversés par l'expert les informations pertinentes à l'expression de la récompense.\\

  Si l'on exclut l'automatique (plus ancienne), les outils utilisés pour résoudre les problèmes de prise de décision séquentielle remontent jusque dans les années 50. L'apprentissage par renforcement inverse est en comparaison un problème assez jeune, posée pour la première fois en 1998 \cite{russell1998learning}, et qui a réellement pris son essor après 2004 \cite{abbeel2004apprenticeship}.\\
  Il s'agit d'un problème mathématiquement mal posé, dans le sens où il existe plus d'une fonction de récompense pour laquelle le comportement d'un expert est optimal. Comment sélectionner "la bonne" ? La situation empire lorsque la politique de l'expert n'est pas connue dans tout l'espace d'état, mais simplement observée tandis que l'expert agit, ce qui limite l'information disponible.\\
  
* Contributions au domaine
  Il faut attendre 2000 pour qu'une publication \cite{ng2000algorithms} pose formellement le problème. La contribution centrale intervient en 2004 \cite{abbeel2004apprenticeship} et introduit la notion d'/attribut moyen/, que suggérait déjà l'analyse de 2000. Après 2004, plusieurs travaux apparaissent qui utilisent cette notion d'attribut moyen (un état de l'art partiel mais unificateur : \cite{neu2009training}).\\
  L'attribut moyen est une mesure liée à la distribution des états que traverse un agent en suivant sa politique dans l'environnement. Cette mesure tient une place centrale dans beaucoup des algorithmes existants, notamment car deux agents ayant des attributs moyens similaires rempliront une certaine tâche (/i.e./, optimiseront une certaine récompense) de manière similaire.\\

  La première contribution au domaine consiste en un mécanisme de calcul de cet attribut moyen \cite{klein2011batch2}. Inspiré d'algorithmes existants pour l'approximation de fonction de valeur, thème central en apprentissage par renforcement, cette contribution apporte une méthode de calcul permettant l'évaluation /off-policy/ de l'attribut moyen d'une politique. Sans l'anglicisme, cela signifie que l'on peut évaluer une grandeur relative à une politique en observant /une autre politique/ (comme par exemple celle de l'expert). Testée en l'injectant dans l'approche centrale de 2004, cette idée a permis de résoudre les problème de l'apprentissage par renforcement inverse sur un problème jouet simple à partir des seules données issues de l'expert, mais n'a pas permis de complètement lever les obstacles imposés par la structure des algorithmes existants. Ceux-ci nécessitent en effet dans leur immense majorité de résoudre le problème direct (celui de l'apprentissage par renforcement) de manière répétée. Cela n'est pas toujours possible uniquement avec les données de l'expert, il faut une autre source d'information.\\
  Cette contribution permet néanmoins de s'affranchir d'un simulateur pour estimer une grandeur centrale dans la majorité des approches du domaine. Cela permet d'éviter les soucis liés à la modélisation de l'environnement ou à un trop grand besoin en données (coûteuses à générer).
  Une autre contribution de plus faible envergure consiste en un apport sur la définition mathématique formelle du problème \cite{klein2011dimensionality} permettant de réduire l'espace dans lequel on doit chercher les solutions.\\

  Il m'a été permis de participer à trois conférences en temps qu'auteur, j'ai pu rencontrer la communauté /Machine Learning/ au sens large à IJCAI [fn:: [[http://ijcai-11.iiia.csic.es/]]], ainsi qu'à ECML[fn:: [[http://www.ecmlpkdd2011.org/]]] qui précédait EWRL[fn:: [[http://ewrl.wordpress.com/past-ewrl/ewrl9-2011/]]] où j'ai pu présenter mes travaux à un public plus directement concerné du fait du sujet précis de ce /workshop/. Enfin j'ai rencontré la communauté francophone à JFPDA[fn:: [[https://zanuttini.users.greyc.fr/jfpda2011/]]]. Les échanges formels et informels permis par ces déplacements ont enrichi ma culture et ma reflexion.

  
* Perspectives
  
  Il serait intéressant de pouvoir se passer de la résolution répétée du problème direct. Tout d'abord parce que cela pose des difficultés lors de l'application des algorithmes à des problèmes complexes, ensuite car la démarche visant à simplement rendre similaire l'attribut moyen de l'expert et celui de l'agent tend à résoudre à problème d'imitation : faire pareil que l'expert. La nuance est d'importance avec la formulation originale de l'apprentissage par renforcement inverse, qui consiste à voir la récompense comme une description succincte mais complète de la tâche à effectuer, description que nous souhaitons extraire. Nos travaux actuels vont dans ce sens, avec des résultats préliminaires prometteurs.
  

# \bibliographystyle{alpha}
# \bibliography{../../Biblio/Biblio.bib}
 \begin{btSect}[alpha]{../../Biblio/BiblioKlein.bib}
 \section{Mes publications}
 \nocite{klein2011dimensionality,klein2011batch,klein2011batch2,klein2011batch3}
 \btPrintCited
 \end{btSect}
 \begin{btSect}[alpha]{../../Biblio/Biblio.bib}
 \section{References}
 \btPrintCited
 \end{btSect}
