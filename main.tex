\typeout{Batch, Off-Policy Apprenticeship Learning}
\documentclass{article}
\usepackage{ijcai11}
\usepackage{times}
% the following package is optional:
%\usepackage{latexsym} 
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{natbib}
\usepackage{graphicx}

\title{Batch, Off-Policy Apprencticeship Learning}
\author{Edouard Klein \and Matthieu Geist \and Olivier Pietquin \\
Supélec, IMS\\
Metz, France \\
\{edouard.klein|matthieu.geist|olivier.pietquin\}@supelec.fr}
\begin{document}
\maketitle
\tableofcontents
\begin{abstract}
We consider the IRL framework for apprenticeship learning. %1
Current IRL algorithms use a simple Monte-Carlo estimation to approximate the feature expectation of both the expert and the policy under evaluation. %2
In this paper, we propose another method for estimating the feature expectation based on the Least Square Temporal Difference (LSTD) family of algorithms. %3
This new method allows for batch, off-policy appreticeship learning in situations where the dynamics of the system remains unknown. %4
By testing it on a classical problem, we show that our method is empirically sound.%5
We also provide some theoretical results.%6
\end{abstract} 
\section{Introduction}
When a system can be empirically controlled by an expert whose behaviour is difficult to describe formally, apprenticeship learning is a way to let a machine control the system by mimicking the expert.\\

This is a restriction of the more general problem of task transfer, where the goal of the expert is to be guessed and then pursued with the machine own means.\\

\emph{Inverse Reinforcement Learning} is a framework created to tackle both of these problems. \citet{russell1998learning} first defined it in the terms used today, drawing from animal behavior research and more loosely from macroeconomics research : given an expert's sensory input and decisions, determine the reward (i.e. the \emph{goal}) optimized by that expert. This reward is then to be optimized via tools developped whithin the \emph{Reinforcement Learning} framework \citep{sutton1998reinforcement}.\\

\citet{ng2000algorithms} proposed new algorithms to solve the IRL problem, followed by an article \citep{abbeel2004apprenticeship} about the more specific problem of apprenticeship learning, in which imitation metrics are introduced. The method proposed in this work is built upon this paper, and compared against it.\\

The main limitation in \citet{abbeel2004apprenticeship}'s work is the need for a model of the environnement or, at least, a simulator. \\

FIXME:Autres approches qu'Abbeel, notamment celles qui cherchent à contourner le même problème.\\

With the LSTD \citep{bradtke1996linear} and LSPI \citep{lagoudakis2003least} algorithms, one can work out the Reinforcement Learning problem without the need for a model. We extend these capabilities to \citet{abbeel2004apprenticeship}'s algorithm by assessing the imitation metrics through LSTD. We also do not need a simulator, the data from the expert being used to feed LSPI.\\
\section{Background}
A \emph{Markov Decision Process} is a tuple $\{S,A,\gamma,R\}$ with :
\begin{itemize}
\item $S$ the state space,
\item $A$ the action space,
\item $\gamma$ the discount factor,
\item $R$ the reward function
\end{itemize}

A policy $\pi$ is a map from the states to the actions : $\pi:S\rightarrow A$.\\

The \emph{Reinforcement Learning} problem seeks the policy that maximize the expected discounted reward. When following a policy $\pi$ from state $s\in S$, the expected discounted reward is described by the \emph{value function} $V^\pi$ : \\
\begin{equation}
V^\pi(s) = E\left[\left.\sum\limits_{t=0}^{\infty}\gamma^tr_t\right|s_0=s,\pi\right]
\label{eqn:V}
\end{equation}

The state space $S$ is projected into a feature space generated by the function $\psi : S \rightarrow \mathbb{R}^p$. Both the state space and the action space are projected into a feature space generated by the function $\phi : S\times A \rightarrow \mathbb{R}^k$.\\

The value function is approximated via a matrix of coefficients $\omega$, such as :
\begin{equation}
\hat V^\pi(s) = \arg\max_{a\in A}\omega_\pi^T\phi(s,a)
\end{equation}

The \emph{expert} is an agent whose interactions with the world are recorded as a set of transitions : $D_E = \left\{\{s_i,a_i,s'_i,r_i\}_i\right\}$.\\

$D$ is a random transition source.\\

The feature expectation is a mapping from a policy $\pi$ to a function $\mu_\pi : S \rightarrow \mathbb{R}^p$ which gives for a state $s\in S$ the expected discounted value for the features of the states visited when following policy $\pi$ from state $s$ :
\begin{equation}
\mu_\pi(s) = E\left[\left.\sum\limits_{t=0}^{\infty}\gamma^t\psi(s_t)\right|s_0=s,\pi\right]
\label{eqn:mu}
\end{equation}\\
The feature expectation of the expert is denoted $\mu_E$.\\

The IRL problem seeks the reward the expert is maximizing. The imitation problem seeks the reward which, maximized by one of the many RL tools, will give a behavior as close to the expert's as possible.\\

\citet{abbeel2004apprenticeship}'s algorithm solves the imitation pproblem by approximating this reward with a matrix of coefficients $\theta$ such as :
\begin{equation}
\hat R(s) = \theta^T\phi(s)
\end{equation}

The quality of the imitation is assessed via the error term $||\mu_\pi(s_0)-\mu_E(s_0)||_2$. Given that the norm of $\theta$ is bounded, this error term also gives a feeling of the difference of performance under the estimated reward function $\hat R$ :
\begin{equation}
|V^\pi(s_0)-V^E(s_0)| \leq ||\theta||_2||\mu_\pi(s_0)-\mu_E(s_0)||_2
\end{equation}

The main shortcoming of this algorithm lies in its need for either a model of the environnement or at least a sample source $D$, in order to approximate the feature expectation of both the expert and the current policy via a Monte-Carlo estimator.
\section{LSTD-$\mu$}
The similarity between equations \ref{eqn:mu} and \ref{eqn:V} (both are Bellman equations) allows the use of the LSTD algorithm in order to estimate the feature expectation of a policy.\\

We introduce the matrix $\zeta$ such as :
\begin{equation}
\hat \mu_\pi (s) = \zeta^T\phi(s,\pi(s))
\end{equation}
$\zeta$ is given by :
\begin{equation}
\zeta = A^{-1}b
\end{equation}
with :
\begin{eqnarray}
FIXME: copier depuis le code
\end{eqnarray}

This approach is superior to the Monte-Carlo estimation suggested by \citet{abbeel2004apprenticeship}, as it allows for a batch, off policy estimation of $\mu_\pi$, even when no model of the environment is known and no simulator is available whereas the Monte-Carlo estimation needs either a model or a simulator, and only allow for an estimation of $\mu_\pi(s_0)$.\\

Coupled with the LSPI algorithm, this allows an adaptation of \citet{abbeel2004apprenticeship} algorithm to "real-life" settings where the only data available come from the expert's trajectories.\\

FIXME: Un mot sur pourquoi il est inutile de faire des itérations sur mu directement ?

FIXME: Dérouler les deux algorithmes complets côte à côte dans une forme lisible, car c'est un peu la galère en lisant le papier d'Abbeel.
\section{Analysis}
FIXME: Matthieu ?
\section{Experimental benchmark}
The experimental benchmark chosen here is the same as in \citep{ng2000algorithms}, a 5x5 grid world. The expert policy maximizes the following reward : 0 everywhere but in the upper-right square, where it is 1.\\

Both \cite{abbeel2004apprenticeship}'s algorithm (mc\_ANIRL) and our adaptation (lstd\_ANIRL) are tested side by side. We use the projection method in both algorithm. As a MDP solver, mc\_ANIRL is given LSPI with a (very) big matrix $D$ as a sample source. $\mu_\pi(s_0)$ and $\mu_E(s_0)$ are computed thanks to a (very) big Monte-Carlo estimation. Both are, for all intended purpose on this toy problem, as good as a perfect theoretical solver.\\

On the other hand lstd\_ANIRL uses LSPI and LSTD$\mu$, fed only with the expert's transitions. This corresponds to a real-life setting where data generation is expensive and the system can not be controlled by an untrained machine.\\

Figure \ref{fig:E} shows the performance of both algorithms with repect to the number of samples from the expert made available.\\

With respect to computing power, our algorithm needed far less iteration to get the job done.
\begin{figure*}
\includegraphics[width=\textwidth]{../Code/courbe_e.pdf}
\caption{$||\mu_E(s_0)-\mu_\pi(s_0)||_2$ with respect to the number of samples available from the expert. Our algorithm is not as good as \citet{abbeel2004apprenticeship} with very few samples, but make up for it very soon.}
\label{fig:E}
\end{figure*}
\section{Quality criterion}
FIXME: Discussion sur les critères utilisés, ouverture vers le transfert de tâche.
\begin{figure*}
\includegraphics[width=\textwidth]{../Code/courbe_a.pdf}
\caption{Different criteria with respect to the number of iterations. See how decorelated they can be.}
\label{fig:A}
\end{figure*}
\section{Future Work}
FIXME: Une seule itération, est-ce normal ? Abbeel reste-t-il meilleur sur un problème plus complexe ? Transfert de tâche ? Est-il nécessaire de choisir sa politique avec LSTDMu, puisqu'il finit en une seule fois ?
\section{Conclusion}
\bibliographystyle{named}
\bibliography{../Biblio/Biblio}
\end{document} 

