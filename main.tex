\typeout{Batch, Off-Policy Apprenticeship Learning}
\documentclass{article}
\usepackage{ijcai11}
\usepackage{times}
\usepackage{color}
% the following package is optional:
%\usepackage{latexsym} 
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{algorithm2e}
\usepackage{algorithm}
\usepackage{algorithmic}
\newcommand{\argmax}{\operatorname*{argmax}} %\operatorname* pour les op. pouvant admettre des limites...

\title{Batch, Off-Policy Apprencticeship Learning}
\author{Edouard Klein \and Matthieu Geist \and Olivier Pietquin \\
SupÃ©lec, IMS\\
Metz, France \\
\{edouard.klein$|$matthieu.geist$|$olivier.pietquin\}@supelec.fr}
\begin{document}
\maketitle
\begin{abstract}
We consider the IRL framework for apprenticeship learning. %1
Current IRL algorithms use a simple Monte-Carlo estimation to approximate the feature expectation of both the expert and the policy under evaluation. %2
In this paper, we propose another method for estimating the feature expectation based on the Least Square Temporal Difference (LSTD) family of algorithms. %3
This new method allows for batch, off-policy appreticeship learning in situations where the dynamics of the system remains unknown. %4
By testing it on a classical problem, we show that our method is empirically sound.%5
We also provide some theoretical results.%6
\end{abstract}
\section{Introduction}

Optimal control consist in having a machine controlling a system
with the goal of fulfilling a specific task, optimality being
defined as how well the task is performed. Various solutions to this
problem exist from automation to planification. Notably, the
reinforcement learning (RL) paradigm~\citep{sutton1998reinforcement} is a general
machine learning framework in which an agent learns to control
optimally a dynamic system through interactions with it. The task is
specified through a reward function, the agent objective being to
take sequential decisions so as to maximize the expected cumulative
reward.

However, defining optimality (and thus the reward function) can
itself be a challenge. If the system can be empirically controlled
by an expert, even though his/her behavior can be difficult to
describe formally, apprenticeship learning is a way to have a
machine controlling the system by mimicking the expert. Rather than
directly mimicking the expert with some supervised learning
approach, Inverse Reinforcement Learning (IRL)~\citep{ng2000algorithms}
consists in learning a reward function under which the policy
demonstrated by an expert is optimal. Mimicking the expert therefore
ends up to learning an optimal policy according to this reward
function. A significant advantage of such an approach is that
expert's actions can be guessed in states which have not been
encountered during demonstration. Firstly introduced in \citep{russell1998learning}, another advantage claimed by the author would be to
find a compact and complete representation of the task in the form
of the reward function.

There roughly exist three families of IRL algorithms:
feature-expectation-based~\cite{abbeel2004apprenticeship,syed2008apprenticeship,syed2008game,ziebart2008maximum},
marge-maximization-based~\cite{ratliff2006maximum,ratliff2007imitation,ratliff2007boosting,kolter2008hierarchical}
and approaches based on the parameterization of the policy by the
reward function~\cite{ramachandran2007bayesian,neu2007apprenticeship}. The first family
assumes a linearly parameterized reward function. This naturally
leads to a linearly parameterized value function, the associated
feature vector being the so-called feature expectation (defined as
the cumulative discounted reward's feature vector under a given
policy, see Section~\ref{sec:back} for a formal definition).
These approaches learn a reward function such that the feature
expectation of the optimal policy (according to the learnt reward
function) is close to the feature expectation of the expert policy.
This is a sufficient condition to have close value functions. The
second family expresses IRL as a constrained optimization problem in
which expert's examples have higher expected reward than all other
policies by a certain margin. Moreover, suboptimality of the expert
can be considered through the introduction of slack variables. The
last family parameterizes policies with a reward function. Assuming
that the expert acts according to a Gibbs policy (respectively to
the optimal value function related to the reward function which is
optimized), it is possible to estimate the likelihood of a set of
state-action pairs provided by the expert. The algorithms differs in
the way this likelihood is maximized.

This paper focuses on the first family of algorithms, and more
precisely on the seminal work of~\cite{abbeel2004apprenticeship}. All of them
rely on the computation of the feature expectation (which depends on
policies but not on rewards) of (i) the expert and (ii) some
intermediate policies. The expert's feature expectation is computed
using a simple Monte Carlo approach (which requires full
trajectories of the expert). Other feature expectations are either
computed exactly (which requires knowing analytically the dynamics
of the system) or with a Monte Carlo approach (which requires to
simulate the system). The contribution of this paper is LSTD-$\mu$,
a new temporal-difference-based algorithm to estimate these feature
expectations. It relaxes the predeceasing assumptions: transitions
of the expert are sufficient (rather than full trajectories) and nor
the model neither a simulator are necessary to compute intermediate
feature expectations. This paper focuses on the algorithm introduced
in~\cite{abbeel2004apprenticeship}, but we are confident that the proposed
approach can be extended to other algorithms based on feature
expectation computation~\cite{syed2008apprenticeship,syed2008game,ziebart2008maximum}.

The rest of this paper is organized as follows.
Section~\ref{sec:back} provides the necessary background,
notably the definition of feature expectation and its use in the
seminal IRL algorithm of~\cite{abbeel2004apprenticeship}.
Section~\ref{sec:lstdmu} presents LSTD-$\mu$, our main contribution.
Section~\ref{sec:exp} provides some preliminary experiments and
Section~\ref{sec:conclusion} opens perspectives.


\section{Background}
\label{sec:back}
A sequential decision problem is often framed as a Markov Decision
Process (MDP)~\citep{puterman1994markov}. A MDP is a tuple
$\{S,A,P,R,\gamma\}$ with $S$ being the state space, $A$ the action
space, $P\in\mathcal{P}(S)^{S\times A}$ the set of Markovian
transition probabilities, $R\in\mathbb{R}^S$ the reward function
(assumed to be absolutely bounded by 1) and $\gamma\in[0,1[$ a
discounting factor. A policy $\pi\in A^S$ maps states to action. The
quality of a policy is quantified by the associated value function
$V^\pi$, which associates to each state the expected and discounted
cumulative reward:
\begin{equation}
  V^\pi(s) = E[\sum_{t=0}^\infty \gamma^t R(s_t)|s_0=s, \pi]
\end{equation}
Dynamic programming aims at finding the optimal policy $\pi^*$, that
is one of the policies associated to the optimal value function,
$V^* = \argmax_\pi V^\pi$, which maximizes the value for each state.
If the model (that is transition probabilities and the reward
function) is unknown, learning the optimal control through
interactions is addressed by reinforcement learning.

For inverse reinforcement learning, the problem is reversed. It is
assume that an expert acts according to an optimal policy $\pi_E$,
this policy being optimal according to some unknown reward function
$R^*$. The goal of inverse reinforcement learning is to learn this
reward function from sampled trajectories of the expert. This is a
difficult and ill-posed problem~\citep{ng2000algorithms}. Apprenticeship
learning through IRL, which is the problem at hand in this paper,
has a somehow weaker objective: it aims at learning a policy
$\hat{\pi}$ which is (approximately) as good as the expert policy
$\pi_E$ under the unknown reward function $R^*$, for a known initial
state $s_0$ (this condition can be weakened by assuming a
distribution for initial states, this is not done here for clarity
of exposition). Now, the approach proposed in~\citep{abbeel2004apprenticeship} is
presented.

We assume that the true reward function belongs to some hypothesis
space $\mathcal{H}_\phi = \{\theta^T \phi(s),
\theta\in\mathbb{R}^p\}$, of which we assume the basis functions to
be bounded by 1: $|\phi_i(s)|\leq 1, \forall s\in S, 1\leq i \leq
p$. Therefore, there exists a parameter vector $\theta^*$ such that:
\begin{equation}
  R^*(s) = (\theta^*)^T \phi(s)
\end{equation}
In order to ensure that rewards are bounded, we impose that
$\|\theta\|_2\leq 1$. For any reward function belonging to
$\mathcal{H}_\phi$ and for any policy $\pi$, the related value
function $V^\pi(s)$ can be expressed as follows:
\begin{align}
  V^\pi(s) &= E[\sum_{t=0}^\infty \gamma^t \theta^T \phi(s_t)|s_0=s, \pi]
  \\
  &= \theta^T  E[\sum_{t=0}^\infty \gamma^t \phi(s_t)|s_0=s, \pi]
\end{align}
Therefore, the value function is also linearly parameterized, with
the same weights and with basis functions being grouped into the
so-called \emph{feature expectation} $\mu^\pi$:
\begin{equation}
  \mu^\pi(s) = E[\sum_{t=0}^\infty \gamma^t \phi(s_t)|s_0=s, \pi]
\end{equation}
Recall that the problem is to find a policy whose performance is
close to that of the expert's for the starting state $s_0$, on the
unknown reward function $R^*$. In order to achieve this goal, it is
proposed in~\citep{abbeel2004apprenticeship} to find a policy $\tilde{\pi}$ such
that $\|\mu^{\pi_E}(s_0)-\mu^{\tilde{\pi}}(s_0)\|_2\leq \epsilon$
for some (small) $\epsilon>0$. Actually, this ensures that the value
of the expert's policy and the value of the estimated policy (for
the starting state $s_0$) are close for \emph{any} reward function
of $\mathcal{H}_\phi$:
\begin{align}
  |V^{\pi_E}(s_0) - V^{\tilde{\pi}}(s_0)| &=
  |\theta^T(\mu^{\pi_E}(s_0)-\mu^{\tilde{\pi}}(s_0))|
  \label{eqn:vs0}
  \\
  &\leq \|\mu^{\pi_E}(s_0)-\mu^{\tilde{\pi}}(s_0)\|_2
\end{align}
This last equation uses the Cauchy-Schwartz inequality and the
assumption that $\|\theta\|_2\leq 1$. Therefore, the approach
described here does not ensure to retrieve the true reward function
$R^*$, but to act as well as the expert under this reward function
(and actually under any reward function, notably the unknown optimal
one).

Let us now describe the algorithm proposed in~\citep{abbeel2004apprenticeship} to
achieve this goal:
\begin{enumerate}
  \item Starts with some initial policy $\pi^{(0)}$ and compute
  $\mu^{\pi^{(0)}}(s_0)$. Set $j=1$;
  %
  \item Compute $t^{(j)} = \max_{\theta: \|\theta\|_2\leq 1}
  \min_{k\in\{0,j-1\}}\theta^T(\mu^{\pi_E}(s_0) -
  \mu^{\pi^{(k)}}(s_0))$ and let $\theta^{(j)}$ be the value
  attaining this maximum. At this step, one searches for the reward
  function which maximizes the distance between the value of the
  expert at $s_0$ and the value of \emph{any} policy computed so far
  (still at $s_0$). This optimization problem can be solved using a
  quadratic programming approach or a projection
  algorithm~\citep{abbeel2004apprenticeship};
  %
  \item if $t^{(j)}\leq \epsilon$, terminate. The algorithm outputs a
  set of policies $\{\pi^{(0)}, \dots, \pi^{(j-1)}\}$ among which
  the user chooses manually or automatically the closest to the
  expert (see~\citep{abbeel2004apprenticeship} for details on how to choose this
  policy\footnote{A simple solution, not mentioned in~\citep{abbeel2004apprenticeship},
  is to choose the policy of which the feature expectation is the
  closest to the one of the expert.}).
  Notice that the last policy is not necessary the best;
  %
  \item solve the MDP with the reward function $R^{(j)}(s) =
  (\theta^{(j)})^T\phi(s)$ and denote $\pi^{(j)}$ the associated
  optimal policy. Compute $\mu^{\pi^{(j)}}(s_0)$;
  %
  \item set $j\leftarrow j+1$ and go back to step 2.
\end{enumerate}
There remains three problems: computing the feature expectation of
the expert, solving the MDP and computing feature expectations of
intermediate policies.

As suggested in~\citep{abbeel2004apprenticeship}, solving the MDP can be done
approximately by using any appropriate reinforcement learning
algorithm. In this paper, we use the Least-Squares Policy Iteration
(LSPI) algorithm~\citep{lagoudakis2003least}. There remains to estimate
feature expectations. In~\citep{abbeel2004apprenticeship}, $\mu^{\pi_E}(s_0)$ is
estimated using a Monte Carlo approach over $m$ trajectories:
\begin{equation}
  \hat{\mu}_E(s_0) = \frac{1}{m} \sum_{h=1}^m \sum_{t=0}^\infty
  \gamma^i \phi(s_t^{(h)})
\end{equation}
This approach does not hold if only transitions of the expert are
available, or if trajectories are too long (in this case, it is
still possible to truncate them). For intermediate policies, it is
also suggested to estimate associated feature expectations using a
Monte Carlo approach (if they cannot be computed exactly). This is
more constraining than for the expert, as this assumes that a
simulator of the system is available. In order to address these
problems, we introduce a temporal-difference-based algorithm to
estimate these feature expectations.

\section{LSTD-$\mu$}
\label{sec:lstdmu}
Let us write the definition of the $i^\text{th}$ component of a
feature expectation $\mu^\pi(s)$ for some policy $\pi$:
\begin{equation}
  \mu_i^\pi(s) = E[\sum_{t=0}^\infty \gamma^t \phi_i(s_t)|s_0=s,\pi]
  \label{eqn:phi}
\end{equation}
This is exactly the definition of the value function of the policy
$\pi$ for the MDP considered with the $i^\text{th}$ basis function
$\phi_i(s)$ as the reward function. There exists many algorithms to
estimate a value function, any of them can be used to estimate
$\mu_i^\pi$. Based on this remark, we propose to use specifically
the least-squares temporal difference (LSTD)
algorithm~\citep{bradtke1996linear} to estimate each component of the
feature expectation (as each of these components can be understood
as a value function related to a specific and known reward
function).

Assume that a set of transitions $\{(s_t,r_t,s_{t+1})_{1\leq t \leq
n}\}$ sampled according to the policy $\pi$ is available. We assume
that value functions are searched for in some  hypothesis space
$\mathcal{H}_\psi = \{ \hat{V}_\xi(s) = \sum_{i=1}^q \xi_i \psi_i(s)
= \xi^T \psi(s), \xi\in\mathbb{R}^q\}$. As reward and value
functions are possibly quite different, another hypothesis space is
considered for value function estimation. But if $\mathcal{H}_\phi$
is rich enough, one can still consider
$\mathcal{H}_\psi=\mathcal{H}_\phi$. Therefore, we are looking for
an approximation of the following form:
\begin{equation}
  \hat{\mu}_i^\pi(s) = (\xi_i^*)^T \psi(s)
  \label{eqn:psi}
\end{equation}
The parameter vector $\xi_i^*$ is here the LSTD estimate:
\begin{equation}
  \xi_i^* = \left(\sum_{t=1}^n
  \psi(s_t)(\psi(s_t)-\gamma\psi(s'_{t}))^T\right)^{-1}
  \sum_{t=1}^n \psi(s_t) \phi_i(s_t)
\end{equation}
For apprenticeship learning, we are interested more particularly in
$\hat{\mu}^\pi(s_0)$. Let $\Psi = (\psi_i(s_t))_{t,i}$ be the
$n\times q$ matrix of values predictors, $\Delta\Psi = (\psi_i(s_t)
- \gamma\psi_i(s'_t))_{t,i}$ be the related $n\times q$ matrix and
$\Phi = (\phi_i(s_t))_{t,i}$ the $n\times p$ matrix of reward
predictors. It can be easily checked that $\hat{\mu}^\pi(s_0)$
satisfies:
\begin{equation}
  (\hat{\mu}^\pi(s_0))^T = \psi(s_0)^T (\Psi^T \Delta\Psi)^{-1} \Psi^T \Phi
\end{equation}
This provides an efficient way to estimate the feature expectation
of the expert in $s_0$.

There remains to compute the feature expectations of intermediate
policies, which should be done in an off-policy manner (that is
without explicitly sampling trajectories according to the policy of
interest). To do so, still interpreting each component of the
feature expectation as a value function, we introduce a state-action
feature expectation defined as follows (much as the classic
$Q$-function extends the value function):
\begin{equation}
  \mu^\pi(s,a) = E[\sum_{t=0}^\infty \gamma^t
  \phi(s_t)|s_0=s,a_0=a,\pi]
\end{equation}
Compared to the classic feature expectation, this definition adds a
degree of freedom on the first action to be chosen before following
the policy $\pi$. With a slightly different definition of the
related hypothesis space, each component of this feature expectation
can still be estimated using the LSTD algorithm (namely using the
LSTD-Q algorithm~\citep{lagoudakis2003least}). The clear advantage of
introducing the state-action feature expectation is that this
additional degree of freedom allows off-policy learning. Assuming
that $\mathcal{H}_\psi$ defines state-action feature vectors and
that a set of transitions $\{(s_t,a_t,r_t,s_{t+1})_{1\leq t \leq
n}\}$ sampled according to some behaviorial policy $\pi_0$ is
available, the parameter vector $\xi_i^*$ related to the
$i^\text{th}$ component of the state-action feature expectation
$\mu^\pi(s)$ (policy $\pi$ being different from policy $\pi_0$) is
given by:
\begin{equation}
  \xi_i^* = \left(\sum_{t=1}^n
  \psi(s_t,a_t)(\psi(s_t,a_t)-\gamma\psi(s'_{t},\pi(s'_t)))^T\right)^{-1}
  \sum_{t=1}^n \psi(s_t,a_t) \phi_i(s_t)
\end{equation}
Extending LSTD-$\mu$ to state-action LSTD-$\mu$ is done in the same
manner as LSTD is extended to LSTD-Q~\citep{lagoudakis2003least},
technical details are not provided here for the clarity of
exposition.

Given the (state-action) LSTD-$\mu$ algorithm, the apprenticeship
learning algorithm of~\citep{abbeel2004apprenticeship} (see
Section~\ref{sec:back}) can be easily extended to a batch and
off-policy setting. The solely available data is a set of
transitions sampled according to the expert policy. The
corresponding feature expectation for the starting state $s_0$ is
estimated with the LSTD-$\mu$ algorithm. At step~4 of this
algorithm, the MDP is (approximately) solved using
LSPI~\citep{lagoudakis2003least} (an approximate policy iteration
algorithm using LSTD-Q as the off-policy $Q$-function estimator).
The corresponding feature expectation at state $s_0$ is estimated
using the proposed state-action LSTD-$\mu$.

Before presenting some experimental results, let us stress that
LSTD-$\mu$ is simply the LSTD algorithm applied to a specific reward
function. Although quite clear, the idea of using a temporal
difference algorithm to estimate the feature expectation is new, as
far as we know. A clear advantage of the proposed approach is that
any theoretical result holding for LSTD also holds for LSTD-$\mu$,
such as convergence~\citep{nedic2003least} or
finite-sample~\citep{lazaric2010finiteLSTD} analysis for example.

\section{Experimental benchmark}
\label{sec:exp}
\subsection{Experiment description and results}
The experimental benchmark chosen here is the same as in \citep{ng2000algorithms}, a 5x5 grid world. The agent is in one of the case of the grid (whose coordinates is the state) and can choose at each step one of the four compass directions (the action). With probability 0.9, the agent moves in the intended direction. With probability 0.1, the direction is randomly drawn (and thus have probability 0.25 to match the original direction). The reward optimized by the expert is 0 everywhere but in the upper-right square, where it is 1. For every policy, an episode ends when the upper right square is attained, or after 20 steps. At the start of each episode, the agent is in the lower-left corner of the grid (the opposite of where the reward is).\\

Both the state and action spaces are finite and of small cardinality. Hence, the chosen feature functions $\phi$ and $\psi$ (see equations \ref{eqn:phi} and \ref{eqn:psi} for a remainder of their meaning) are the typical features of a tabular representation : 0 everywhere but at a given index $i$ for which the value is 1. There is a bijective mapping from the input space elements to the indexes $i$ for which the coordinate of the feature is 1.\\

Both \citet{abbeel2004apprenticeship}'s algorithm (mc\_ANIRL) and our adaptation (lstd\_ANIRL) are tested side by side. The MDP solver of mc\_ANIRL is LSPI with a (very) big matrix $D$ as a sample source. $\mu_\pi(s_0)$ and $\mu_E(s_0)$ are computed thanks to a (very) big Monte-Carlo estimation. Both these computations are  as good as perfect theoretical solvers for all intended purpose on this toy problem. We thus are in the case intended by \citet{abbeel2004apprenticeship}.\\
On the other hand lstd\_ANIRL is used as if it could not have access to the simulator. It uses LSPI and LSTD$\mu$, fed only with the expert's transitions. This corresponds to a real-life setting where data generation is expensive and the system can not be controlled by an untrained machine.\\

We want to compare the porformance of both versions of the algorithm with repect to the number of samples available from the expert, as these samples usually are the bottleneck. The discussion about the choice of the performance metric has its own dedicated subsection (subsection \ref{sec:perf}). We use here the $||\mu(s_0) - \mu_E(s_0)||_2$ error term.\\

Figure \ref{fig:E} shows, for some numbers of samples from the expert, the value of $||\mu(s_0)-\mu_E(s_0)||_2$ after the algorithm converged or attained the maximum number of iterations (we used 40). The best policy is found by lstd\_ANIRL after one iteration only\footnote{Precise reasons for what it happens are not clear now, but certainly have something to do with the fact that all the estimations are made along the same distribution of samples.} whereas in mc\_ANIRL convergence happens after at least ten iterations and the best policy is not always the last. The score of the best policy (not the last) is what is plotted here.\\

We can see that although lstd\_ANIRL is not as good as mc\_ANIRL when very few samples are available, both algorithm quickly converge to almost the same value (we don't know yet if the seemingly slightly better performance of lstd\_ANIRL is an artifact). The fact that lstd\_ANIRL can work in a batch, off-policy and model-free setting, while requesting less computing power than mc\_ANIRL makes it suitable to a range of task inaccessible to the mc\_ANIRL version (for example controlling a machine tool working on expensive materials).
\subsection{Discussion about the quality criterion}
Figure \ref{fig:A} and \ref{fig:B} illustrate the difficulty of choosing the quality criterion. These plots show four different quality ciretria during a run of mc\_ANIRL and several runs of lstd\_ANIRL, for which the abscissa can not be the number of iterations.\\

The $||\mu(s_0) - \mu_E(s_0)||_2$ term is widely discussed in \citep{abbeel2004apprenticeship}'s additional material. It bears an interesting relation with the difference between the expert's value function and the current value function in the initial state with respect \emph{to the current reward} (equation \ref{eqn:vs0}).\\

The fact that  the curves of  $||\mu(s_0) - \mu_E(s_0)||_2$ and $||\hat\mu(s_0) - \hat\mu_E(s_0)||_2$ are indistinguishable in figure \ref{fig:A} means that for it has access to a cheap simulator, mc\_ANIRL is as if it worked within an analitycal framework. This however can not be said of lstd\_ANIRL, for which the two curves are indeed different (Figure \ref{fig:B}). Not knowing the true value of $\mu_E(s_0)$ may be a problem for lstd\_ANIRL, as it can introduce an error int the halt criterion of the algorithm\footnote{This equation differs from what has been given as the second step of the algorithm in section \ref{sec:back}. This is because we implemented the version of the algorithm that uses a projection (see \citet{abbeel2004apprenticeship}'s supplementary material for an in-depth discussion of both versions). Although the mathematical expressions are different, both convey the same meaning and are exchangeable.}, $t = ||\bar\mu-\hat\mu_E(s_0)||$. After the algorithm has converged there is no way to know whether the policy is actually good or not other than actually testing it.\\

It shall be noted that although it plays its role, the halt criterion is not a good measure of the quality of the current policy in mc\_ANIRL either, as it can be low when the policy is bad. The best policy, however, can be easily chosen from the $||\mu(s_0) - \mu_E(s_0)||_2$ term, which mc\_ANIRL can compute. When this term is low, the objective performance (that is, $V^E(s_0)-V^\pi(s_0)$ with respect to the unknow true reward function) is low too. 
\section{Conclusion}
\label{sec:conclusion}
Given some transitions generated by an expert controlling a system and maximizing in the long run an unknown reward, we ported \citet{abbeel2004apprenticeship}'s approach to apprenticeship learning vie inverse reinforcement learning to a batch, model-free, off-policy setting at the cost of the need for a bigger number of samples from the expert. We believe this cost is not prohibitive as our approach only requires isolated samples which are often less difficult to get than whole trajectories as needed by the original approach. Furthermore, our method requires less computing power as it converges after only one iteration (that is, as far as we witnessed in our experimental setting).\\

The simple idea of using LSTD to estimate the feature expectation can be applied to other algorithms as well, for example \cite{abbeel2004apprenticeship,syed2008apprenticeship,syed2008game,ziebart2008maximum}.\\

The exact reason why LSTD$\mu$ converges so fast should be investigated and a bigger experiment should be run, possibly on a continuous problem.\\
\begin{figure}
\includegraphics[width=.5\textwidth]{../Code/courbe_e.pdf}
\caption{$||\mu_E(s_0)-\mu_\pi(s_0)||_2$ with respect to the number of samples available from the expert. Our algorithm is not as good as \citet{abbeel2004apprenticeship} with very few samples, but make up for it very soon.}
\label{fig:E}
\end{figure}
\label{sec:perf}
\begin{figure}
\includegraphics[width=.5\textwidth]{../Code/courbe_a.pdf}
\caption{Different criteria with respect to the number of iterations for a run of mc\_ANIRL.}
\label{fig:A}
\end{figure}
\begin{figure}
\includegraphics[width=.5\textwidth]{../Code/courbe_b.pdf}
\caption{Different criteria with respect to the number of samples from the expert, for several runs of lstd\_ANIRL.}
\label{fig:B}
\end{figure}
\bibliographystyle{named}
\bibliography{../Biblio/Biblio}
\end{document} 

