\typeout{Batch, Off-Policy Apprenticeship Learning}
\documentclass{article}
\usepackage{ijcai11}
\usepackage{times}
\usepackage{color}
% the following package is optional:
%\usepackage{latexsym} 
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{natbib}
\usepackage{graphicx}
\usepackage{subfigure}
%\usepackage{algorithm2e}
\usepackage{algorithm}
\usepackage{algorithmic}

\title{Batch, Off-Policy Apprencticeship Learning}
\author{Edouard Klein \and Matthieu Geist \and Olivier Pietquin \\
Supélec, IMS\\
Metz, France \\
\{edouard.klein$|$matthieu.geist$|$olivier.pietquin\}@supelec.fr}
\begin{document}
\maketitle
\begin{abstract}
We consider the IRL framework for apprenticeship learning. %1
Current IRL algorithms use a simple Monte-Carlo estimation to approximate the feature expectation of both the expert and the policy under evaluation. %2
In this paper, we propose another method for estimating the feature expectation based on the Least Square Temporal Difference (LSTD) family of algorithms. %3
This new method allows for batch, off-policy appreticeship learning in situations where the dynamics of the system remains unknown. %4
By testing it on a classical problem, we show that our method is empirically sound.%5
We also provide some theoretical results.%6
\end{abstract}
\section{Introduction}
The optimal control problem can be expressed as having a machine control a system, with the goal of fulfilling a specific task. Various solutions to this problem exist. Things can get hard to deal with, however, when precisely defining the task is in itself a challenge. When the system can be empirically controlled by an expert, even when his behaviour is difficult to describe formally, apprenticeship learning is a way to let a machine control the system by mimicking the expert.\\

When the task at hand can be formally described, the problem of optimal control can be solved by tools from the \emph{Reinforcement Learning} (RL) framework. An extensive introduction to this matter can be found in \citep{sutton1998reinforcement} : the control problem is projected into a sequential decision problem inside a \emph{Markov Decision Process}. The agent transits from \emph{state} to \emph{state}, the decision taking the form of choosing an \emph{action} at each visited state. For each \emph{transition}, the agent receives a \emph{reward}. The \emph{reward function} (a mapping from each state to a real value) defines the task. The agent's goal is to maximize the long-term reward by maximizing what is called the \emph{expected discounted reward} : the expectation of the sum of all rewards to be obtained, multiplied by a deacreasing-over-time factor (called the \emph{discount factor}). For a given \emph{policy} (a mapping from each state to an action) that the agent follows, the mapping from each state to the expected discounted reward is called the \emph{value function}. The \emph{optimal policy}, hopefully found by the agent, is the unique policy of which the value fuction is, for every state, superior or equal to all the other policie's value functions.\\

It is not necessary for a model of the environment or for a simulator to be available, as for example the LSTD (\citep{bradtke1996linear}) algorithm can approximate the value function in a batch, off-policy, model-free setting. Building on it, the LSPI (\citep{lagoudakis2003least}) algorithm can be used to actually solve the RL problem in such settings : given a set of random or arbitrary transitions and the corresponding rewards, find the optimal policy.\\

\citet{russell1998learning} first defined the \emph{Inverse Reinforcement Learning} (IRL) problem in the terms used today, drawing from animal behavior research and more loosely from macroeconomics research. What is looked for is not the policy as it is in the RL problem, but the reward, which defines the task and of which the expected discounted sum is maximized by an expert. IRL is a framework in which it could be possible to attack the apprenticeship learning problem mentionned earlier.\\

\citet{ng2000algorithms} first proposed an algorithm to solve the IRL problem, sheding light on a problem foreseen in \citep{russell1998learning} : the IRL problem is mathematically ill-defined as it admits a infinity of solutions, including degenerative ones (e.g. the null reward accepts any policy as an optimal policy).\\

This work was followed by an article \citep{abbeel2004apprenticeship} focussing on the more specific, but better defined problem of apprenticeship learning using the IRL framework. This work makes use of the IRL framework by using an approximator for the reward function, and looking for the parameters for which the approximated reward function is the best. This however only concerns the apprenticeship learning (a.k.a. imitation learning) because the metric used, although intended to discriminate the expert's performance and the current policy's performance \emph{under the current approximate reward}, can be seen as a measure of proximity of the different states visitation frequencies.

This metric, called the \emph{feature expectation}, is akin to the expected discounted reward discussed when describing the RL framework. It does however not sum the reward but the features of the visited states, features defined by the choice of the reward function approximator. It can be assessed either by analytical resolution (when the model is available) of a Monte-Carlo estimation (when a simulator is available).\\
 
The main limitation in \citet{abbeel2004apprenticeship}'s work (from now on reffered to as the ANIRL algorithm) is the need for a model of the environnement or, at least, a simulator. We build directly upon this work by presenting a new algorithm, LSTD$\mu$, which along with the use of LSPI and LSTD allows the use of the ANIRL algorithm in a batch, off-policy, model-free setting.\\

FIXME:Autres approches qu'Abbeel, notamment celles qui cherchent à contourner le même problème.\\

\section{Background}
\label{sec:back}
A \emph{Markov Decision Process} is a tuple $\{S,A,\gamma,R\}$ with :
\begin{itemize}
\item $S$ the state space,
\item $A$ the action space,
\item $\gamma$ the discount factor,
\item $R$ the reward function
\end{itemize}

A policy $\pi$ is a map from the states to the actions : $\pi:S\rightarrow A$.\\

The \emph{Reinforcement Learning} problem seeks the policy that maximize the expected discounted reward. When following a policy $\pi$ from state $s\in S$, the expected discounted reward is described by the \emph{value function} $V^\pi$ : \\
\begin{equation}
V^\pi(s) = E\left[\left.\sum\limits_{t=0}^{\infty}\gamma^tr_t\right|s_0=s,\pi\right]
\label{eqn:V}
\end{equation}

The state space $S$ is projected into a feature space generated by the function $\psi : S \rightarrow \mathbb{R}^p$. Both the state space and the action space are projected into a feature space generated by the function $\phi : S\times A \rightarrow \mathbb{R}^k$.\\

The value function is approximated via a matrix of coefficients $\omega$, such as :
\begin{equation}
\hat V^\pi(s) = \arg\max_{a\in A}\omega_\pi^T\phi(s,a)
\end{equation}

The \emph{expert} is an agent whose interactions with the world are recorded as a set of transitions : $D_E = \left\{\{s_i,a_i,s'_i,r_i\}_i\right\}$.\\

$D$ is a random transition source.\\

The feature expectation is a mapping from a policy $\pi$ to a function $\mu_\pi : S \rightarrow \mathbb{R}^p$ which gives for a state $s\in S$ the expected discounted value for the features of the states visited when following policy $\pi$ from state $s$ :
\begin{equation}
\mu_\pi(s) = E\left[\left.\sum\limits_{t=0}^{\infty}\gamma^t\psi(s_t)\right|s_0=s,\pi\right]
\label{eqn:mu}
\end{equation}\\
The feature expectation of the expert is denoted $\mu_E$.\\

The IRL problem seeks the reward the expert is maximizing. The imitation problem seeks the reward which, maximized by one of the many RL tools, will give a behavior as close to the expert's as possible.\\

\citet{abbeel2004apprenticeship}'s algorithm solves the imitation pproblem by approximating this reward with a matrix of coefficients $\theta$ such as :
\begin{equation}
\hat R(s) = \theta^T\phi(s)
\end{equation}

The quality of the imitation is assessed via the error term $||\mu_\pi(s_0)-\mu_E(s_0)||_2$. Given that the norm of $\theta$ is bounded, this error term also gives a feeling of the difference of performance under the estimated reward function $\hat R$ :
\begin{equation}
|V^\pi(s_0)-V^E(s_0)| \leq ||\theta||_2||\mu_\pi(s_0)-\mu_E(s_0)||_2
\label{eqn:vmu}
\end{equation}

The main shortcoming of this algorithm lies in its need for either a model of the environnement or at least a sample source $D$, in order to approximate the feature expectation of both the expert and the current policy via a Monte-Carlo estimator.
\section{LSTD-$\mu$}
The similarity between equations \ref{eqn:mu} and \ref{eqn:V} (both are Bellman equations) allows the use of the LSTD algorithm in order to estimate the feature expectation of a policy.\\

We introduce the matrix $\zeta$ such as :
\begin{equation}
\hat \mu_\pi (s) = \zeta^T\phi(s,\pi(s))
\end{equation}
$\zeta$ is given by :
\begin{equation}
\zeta = \tilde A^{-1}\tilde b
\end{equation}
with :
\begin{eqnarray}
\tilde A = \sum_{\{s,a,s'\} \in D}\phi(s,a)\left(\phi(s,a) - \gamma \phi(s',\pi(s'))\right)^T\\
\tilde b = \sum_{\{s,a,s'\} \in D} \phi(s,a)\psi(s)^T
\end{eqnarray}

This approach is superior to the Monte-Carlo estimation suggested by \citet{abbeel2004apprenticeship}, as it allows for a batch, off policy estimation of $\mu_\pi$, even when no model of the environment is known and no simulator is available whereas the Monte-Carlo estimation needs either a model or a simulator, and only allow for an estimation of $\mu_\pi(s_0)$.\\

Coupled with the LSPI algorithm, this allows an adaptation of \citet{abbeel2004apprenticeship} algorithm to "real-life" settings where the only data available come from the expert's trajectories.\\

FIXME: Un mot sur pourquoi il est inutile de faire des itérations sur mu directement ?
\begin{figure*}
 \begin{minipage}[t]{.45\textwidth}
 \begin{algorithm}[H]
   \caption{lstd\_ANIRL} 
   \label{alg:lstd_anirl} 
   \begin{algorithmic}[1]
     \STATE $\omega \leftarrow 0$
     {\color{blue}
     \STATE $D_\mu \leftarrow D $
     \STATE $D_\mu.r \leftarrow \psi(D.s)$
     \STATE $\zeta \leftarrow LSTD\mu( D_\mu, \pi_\omega )$
     \STATE $\mu \leftarrow \zeta^T\phi(s_0,\pi_\omega(s_0))$
     \STATE $D_E.r \leftarrow \psi(D_E.s)$
     \STATE $\zeta \leftarrow LSTD\mu( D_E )$ 
     \STATE $\mu_E \leftarrow \zeta^T\phi(D_E.s_0, D_E.a_0)$
     }
     \STATE $\theta \leftarrow {\mu_E - \mu\over ||\mu_E - \mu||_2}$
     \STATE $ \bar\mu \leftarrow \mu $
     \STATE $ t \leftarrow ||\mu_E - \bar\mu||_2 $
     \WHILE{$ t > \epsilon $}
     \STATE $D.r \leftarrow \theta^T\psi(D.s)$
     \STATE $ \omega \leftarrow LSPI( D, \omega_0 ) $
     {\color{blue}
     \STATE $ \zeta \leftarrow LSTD\mu( D_\mu, \pi_\omega )$   
     \STATE $ \mu \leftarrow \zeta^T\phi(s_0,\pi_\omega(s_0))$}
     \STATE $\bar\mu \leftarrow \bar\mu + { (\mu-\bar\mu)^T (\mu_E-\bar\mu) \over (\mu-\bar\mu)^T (\mu-\bar\mu) } (\mu-\bar\mu) $ 
     \STATE $\theta \leftarrow 
       {\mu_E - \bar\mu\over ||\mu_E - \bar\mu||_2}$
     \STATE $ t\leftarrow ||\mu_E - \bar\mu||_2$
     \ENDWHILE
   \end{algorithmic}
 \end{algorithm}
 \end{minipage}
 \hfill
 \begin{minipage}[t]{.45\textwidth}
   \begin{algorithm}[H]
   \caption{mc\_ANIRL} 
   \label{alg:mc_anirl} 
   \begin{algorithmic}[1]
     \STATE $\omega \leftarrow 0$
     {\color{blue}
     \STATE $D_\pi \leftarrow simulator( \omega )$
     \STATE $\mu \leftarrow \sum\limits_{s\in D_\pi.s}\psi(s)$
     \STATE $\mu_E \leftarrow \sum\limits_{s\in D_E.s}\psi(s)$
     }
     \STATE $\theta \leftarrow {\mu_E - \mu\over ||\mu_E - \mu||_2}$
     \STATE $ \bar\mu \leftarrow \mu $
     \STATE $ t \leftarrow ||\mu_E - \bar\mu||_2 $
     \WHILE{$ t > \epsilon $}
     \STATE $D.r \leftarrow \theta^T\psi(D.s)$  \COMMENT{The estimated reward is propagated in the data}
     \STATE $ \omega \leftarrow LSPI( D, \omega_0 ) $  \COMMENT{The MDP is solved}
     {\color{blue}
     \STATE $D_\pi \leftarrow simulator( \omega )$
     \STATE $\mu \leftarrow \sum\limits_{s\in D_\pi.s}\psi(s)$
     }
     \STATE $\bar\mu \leftarrow \bar\mu + { (\mu-\bar\mu)^T (\mu_E-\bar\mu) \over (\mu-\bar\mu)^T (\mu-\bar\mu) } (\mu-\bar\mu) $ \COMMENT{We use the projection method, and not the QP solver}
     \STATE $\theta \leftarrow 
       {\mu_E - \bar\mu\over ||\mu_E - \bar\mu||_2}$
     \STATE $ t\leftarrow ||\mu_E - \bar\mu||_2$
     \ENDWHILE
     \end{algorithmic}  
   \end{algorithm}
 \end{minipage}
 \hfill
 \caption{Both algorithms side by side, with specific parts in blue}
 \label{fig:alg}
\end{figure*}

%% \begin{figure*}
%%   \subfigure{
%%     \begin{algorithm}
%%       $A\leftarrow 0$
%%     \end{algorithm}
%%   }
  %% \qquad
  %% \subfigure{
    %% \begin{algorithm}[H]
    %%   \caption{Algo deux}
    %%   \begin{algorithmic}
    %%     $b\leftarrow 0$
    %%   \end{algorithmic}
    %% \end{algorithm}
    %%   } 
%% \end{figure*}

FIXME: Dérouler les deux algorithmes complets côte à côte dans une forme lisible, car c'est un peu la galère en lisant le papier d'Abbeel.
\section{Analysis}
FIXME: Matthieu ?
\section{Experimental benchmark}
The experimental benchmark chosen here is the same as in \citep{ng2000algorithms}, a 5x5 grid world. The agent is in one of the case of the grid (whose coordinates is the state) and can choose at each step one of the four compass directions (the action). With probability 0.9, the agent moves in the intended direction. With probability 0.1, the direction is randomly drawn (and thus have probability 0.25 to match the original direction). The reward optimized by the expert is 0 everywhere but in the upper-right square, where it is 1. For every policy, an episodes ends when the upper right square is attained, or after 20 steps. At the start of each episode, the agent is in the lower-left corner of the grid (the opposite of where the reward is).\\

Both the state and action spaces are finite and of small cardinality. Hence, the chosen feature functions $\phi$ and $\psi$ (see section \ref{sec:back}) are the typical features of a tabular representation : 0 everywhere but at a given index $i$ for which the value is 1. There is a bijective mapping from the input space elements ($S$ for $\psi$, $S \times A$ for $\phi$) to the indexes $i$ for which the coordinate of the feature is 1.\\

Both \citet{abbeel2004apprenticeship}'s algorithm (mc\_ANIRL) and our adaptation (lstd\_ANIRL) are tested side by side. The pseudo code of the implementation we used can be found figure \ref{fig:alg}.\\

The MDP solver of mc\_ANIRL is LSPI with a (very) big matrix $D$ as a sample source. $\mu_\pi(s_0)$ and $\mu_E(s_0)$ are computed thanks to a (very) big Monte-Carlo estimation. Both these computations are  as good as perfect theoretical solvers for all intended purpose on this toy problem. We thus are in the case intended by \citet{abbeel2004apprenticeship}.\\

On the other hand lstd\_ANIRL is used as if it could not have access to the simulator. It uses LSPI and LSTD$\mu$, fed only with the expert's transitions. This corresponds to a real-life setting where data generation is expensive and the system can not be controlled by an untrained machine.\\

We want to compare the porformance of both versions of the algorithm with repect to the number of samples available from the expert, as these samples usually are the bottleneck. The discussion about the choice of the performance metric has its own dedicated section (section \ref{sec:perf}). We use here the $||\mu(s_0) - \mu_E(s_0)||_2$ error term.\\

Figure \ref{fig:E} shows, for some numbers of samples from the expert, the value of $||\mu(s_0)-\mu_E(s_0)||_2$ after the algorithm converged or attained the maximum number of iterations (we used 40). The best policy is found by lstd\_ANIRL after one iteration only (this is being investigated), whereas in mc\_ANIRL convergence happens after at least ten iterations, and the best policy is not always the last. The score of the best policy (not the last) is what is plotted here.\\

We can see that although lstd\_ANIRL is not as good as mc\_ANIRL when very few samples are available, both algorithm quickly converge to almost the same value (we don't know yet if the seemingly slightly better performance of lstd\_ANIRL is an artifact). The fact that lstd\_ANIRL can work in a batch, off-policy and model-free setting, while requesting less computing power than mc\_ANIRL makes it suitable to a range of task inaccessible to the mc\_ANIRL version (for example controlling a machine tool working on expensive materials).
\section{Quality criterion}

Figure \ref{fig:A} and \ref{fig:B} illustrate the difficulty of choosing the quality criterion. These plots show four different quality ciretria during a run of mc\_ANIRL and several runs of lstd\_ANIRL, for which the abscissa can not be the number of iterations.\\

The $||\mu(s_0) - \mu_E(s_0)||_2$ term is widely discussed in \citep{abbeel2004apprenticeship}'s additional material. It bears an interesting relation with the difference between the expert's value function and the current value function with respect \emph{to the current reward} (equation \ref{eqn:vmu}).\\

The fact that  the curves of  $||\mu(s_0) - \mu_E(s_0)||_2$ and $||\hat\mu(s_0) - \hat\mu_E(s_0)||_2$ are indistinguishable in figure \ref{fig:A} means that for it has access to a cheap simulator, mc\_ANIRL is as if it worked within an analitycal framework. This however can not be said of lstd\_ANIRL, for which the two curves are indeed different (Figure \ref{fig:B}). Not knowing the true value of $\mu_E(s_0)$ may be a problem for lstd\_ANIRL, as it can introduce an error int the halt criterion of the algorithm, $t = ||\bar\mu-\hat\mu_E(s_0)||$. After the algorithm has converged (it systematically does after only one iteration, this is being investigated), there is no way to know whether the policy is actually good or not.\\

It shall be noted that although it plays its role, the halt criterion is not a good measure of the quality of the current policy in mc\_ANIRL either, as it can be low when the policy is bad. The best policy, however, can be easily chosen from the $||\mu(s_0) - \mu_E(s_0)||_2$ term, which mc\_ANIRL can compute. When this term is low, the objective performance (that is, $V^E(s_0)-V^\pi(s_0)$ with respect to the unknow true reward function) is low too. 
\begin{figure}
\includegraphics[width=.5\textwidth]{../Code/courbe_e.pdf}
\caption{$||\mu_E(s_0)-\mu_\pi(s_0)||_2$ with respect to the number of samples available from the expert. Our algorithm is not as good as \citet{abbeel2004apprenticeship} with very few samples, but make up for it very soon.}
\label{fig:E}
\end{figure}
\label{sec:perf}
\begin{figure}
\includegraphics[width=.5\textwidth]{../Code/courbe_a.pdf}
\caption{Different criteria with respect to the number of iterations for a run of mc\_ANIRL.}
\label{fig:A}
\end{figure}
\begin{figure}
\includegraphics[width=.5\textwidth]{../Code/courbe_b.pdf}
\caption{Different criteria with respect to the number of samples from the expert, for several runs of lstd\_ANIRL.}
\label{fig:B}
\end{figure}
\section{Future Work}
FIXME: Une seule itération, est-ce normal ? Abbeel reste-t-il meilleur sur un problème plus complexe ? Transfert de tâche ? Est-il nécessaire de choisir sa politique avec LSTDMu, puisqu'il finit en une seule fois ?
\section{Conclusion}
\bibliographystyle{named}
\bibliography{../Biblio/Biblio}
\end{document} 

