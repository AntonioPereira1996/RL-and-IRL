#+TITLE: Code for the SL cascading approach to IRL

FIXME: Faire un header propre pour ces trucs

La méthode, décrite ailleurs consiste à :

$<s_i,a_i>$ -[classifieur]-> $q(s,a) = \omega^T\phi(s,a)$ 

$D_E = <s_i,q(s_i,a_i)-\gamma q(s'_i,a'_i)>$
$D_C = <s_i,q(s_i,a_i)-\gamma q(s'_i,a*)>$

$D_E$ -[regresseur]-> $\hat\R_E(s) = \theta_E^T\psi(s)$
$D_C$ -[regresseur]-> $\hat\R_C(s) = \theta_C^T\psi(s)$

Donc en résumé, en entrée l'on prend en entrée un set de transitions $<s_i,a_i,eoe>$ venant de l'expert, ainsi que deux features $\phi$ et $\psi$ et l'on renvoie deux matrices $\theta_E$ et $\theta_C$ définissant une récompense.

#+begin_src python :tangle Cascading.py
from LAFEM import *
from numpy import *
from a2str import *
import scipy.linalg

def run( D, psi, phi, Actions ):
    "Run the cascading algorithm"
    omega_q = classif_score( map( lambda sasre:sasre[0:2], D ), phi, Actions )
    q = lambda s,a : dot( omega_q.transpose(), phi( s, a ) )
    D_E = shape_for_E_regression( D, q, psi )
    D_C = shape_for_C_regression( D, q, psi, Actions )
    theta_E = regression( D_E )
    theta_C = regression( D_C )
    return [theta_E,theta_C]

def classif_score( D, phi, Actions ):
    "Returns the omega matrix so that \omega^T\phi(s,a) allows for classification of elements s with labels a"
    #Il s'avere que SCIRL, notre algo d'IRL peut etre adapte pour
    #faire de la classification, il suffit de lui fournir, en lieu et place de $\mu_E$
    #n'importe quelle feature
    class IRL2classif_hack( LAFEM ):
        data = D
        Threshold = 0.1 #Sensible default
        T = 40 #Sensible default
        A = Actions
        dicPi_E = {}
        def __init__( self ):
            s_0 = D[0][0]
            a_0 = D[0][1]
            self.theta_0 = zeros( phi( s_0, a_0 ).shape )
            #FIXME factoriser ce qui suit (optimisation de la fonction l et fonction l naive)
            for sa in D:
                self.dicPi_E[ l2str( sa[0] )] = sa[1]
        def l( self, s, a ):
            return 0 if all( self.dicPi_E[ l2str( s )] == a ) else 1
        def mu_E( self, s, a ):
            return phi( s, a )
        def alpha( self, t):
            return 3./(t+1) #Sensible default
    classifier = IRL2classif_hack()
    return classifier.run()

def regression( data ):
    "When given data of the form [[x,y]...] returns \omega to that \omega^Tx = y, almost"
    #FIXME: La dimension de Y (1) est codee en dur; Avant de regler ce probleme il faut se mettre d'accord partout, dans l'integralite du code, sur des conventions vecteurs lignes, vecteurs colonnes, matrices etc. Je suggere que tout soit des matrices, comme dans la GSL en C.
    #The formula is omega = (X^TX)^{-1}X^TY
    X = zeros(( len( data ), len(data[0][0]) ))
    Y = zeros(( len( data ), 1 ))
    for i in range(0,len(data)):
        X[i,:] = data[i][0]
        Y[i,:] = data[i][1]
    #FIXME Achtung regularisation, lambda = 0.1
    #FIXME Le coefficient de regularisation devrait etre reglable
    return dot( dot( scipy.linalg.inv( dot( X.transpose(), X ) +0.1*identity( X.shape[1] )), X.transpose() ) , Y)

def shape_for_E_regression( D, q, psi ):
    "Given data of the form [[s,a,s',eoe],...,[s,a,s',eoe]], returns the matrix [[psi(s)^T,q(s,a)-gamma*q(s',a')],...]"
    def hatR( sasa ):
        s = sasa[0]
        a = sasa[1]
        ss = sasa[2]
        aa = sasa[3]
        return [ psi( s ).transpose()[0], (q(s,a) - 0.9*q( ss, aa ))[0]] #FIXME: hard coded gamma
    return map( hatR, shape_as_sasa( D ) )

def shape_for_C_regression( D, q, psi, Actions ):
    "Given data of the form [[s,a,s',eoe],...,[s,a,s',eoe]], returns the matrix [[psi(s)^T,q(s,a)-gamma*q(s',a*)],...]"
    def hatR( sasa ):
        s = sasa[0]
        a = sasa[1]
        ss = sasa[2]
        astar = max( [ [ss,x] for x in Actions ], key=lambda sa : q( sa[0], sa[1] ) )[1]
        return [ psi( s ).transpose()[0], (q(s,a) - 0.9*q( ss, astar ))[0]] #FIXME: hard coded gamma
    return map( hatR, shape_as_sasa( D ) )


def shape_as_sasa( D ):
    "Given data of the form [[s,a,s',eoe],...,[s,a,s',eoe]], returns data of the form [[s,a,s',a'],...]"
    episodes = []
    answer = []
    start_index = 0
    while start_index < len( D ):
        end_index = (i for i in range(start_index,len(D))if D[i][3] == 0).next() #till next eoe
        episodes.append( D[start_index:end_index+1] )
        start_index = end_index+1
    for traj in episodes:
        s = map( lambda sase: sase[0], traj )
        a = map( lambda sase: sase[1], traj )
        ss = map( lambda sase: sase[2], traj ) #ss stands for s'
        aa = map( lambda sase: sase[1], traj[1:] ) #and aa for a'
        #So, we now have :
        # s     a     ss    aa
        # s_1   a_1   s_2   a_2
        # ...
        # s_N-1 a_N-1 s_N   a_N
        # s_N   a_N   s_N+1 
        # The last line only has s,a,s' and no a'.
        # If s_N = s_N+1, s_N==s_N+1 was certainly an absorbing state
        # we can therefore add a_N+1 = a_N
        # If not, then we have a truncated trajectory, and as there is
        # no way to know what a_N+1 was, we drop s_N, a_N, s_N+1
        if( all( s[-1] == ss[-1] ) ):
            try:
                aa.extend(a[-1])
            except TypeError: # a contains only one element
                aa.append(a)
        else:
            s = s[:-1]
            a = a[:-1]
            ss = ss[:-1]
        answer.extend( map( lambda x,y,z,t : [x,y,z,t], s, a, ss, aa ) )
    return answer

#+end_src
#+srcname: Cascading_code_make
#+begin_src makefile
Cascading.py: Cascading.org
	$(call tangle,"Cascading.org")
#+end_src
