\documentclass[11pt]{article}
\pagestyle{empty}
\usepackage[francais]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}

\usepackage{graphicx}
\usepackage{fancybox}

\setlength{\textwidth}{160mm}

\setlength{\oddsidemargin}{0mm}
% \setlength{\evensidemargin}{30mm}

\begin{document}
\section*{Résumé}
Humains et animaux évoluent dans des environnement incertains et dont la dynamique peut être non linéaire. Donner à une machine ou un robot la capacité d'accomplir une tâche dans un tel environnement constitue le problème du contrôle optimal. Résoudre ce problème d'une manière qui ne soit pas \emph{ad-hoc} à un environnement particulier est un problème ouvert.

Un paradigme efficace pour la résolution générique du problème du contrôle optimal est l'Apprentissage par Renforcement (AR), qui à partir d'un signal de récompense défini dans un Processus Décisionnel de Markov (PDM), trouve (éventuellement de manière approchée) par interactions avec l'environnement le comportement maximisant le cumul des récompenses sur le long terme. La conception par l'opérateur humain de ce signal de récompense reste une tâche ardue nécessitant d'une part des connaissances en AR et d'autre part une expertise quant au problème que la machine doit résoudre.

Afin de contourner la difficulté que représente la conception de la fonction de récompense, nous étudions les techniques dites d'Apprentissage par Renforcement Inverse (ARI) qui extraient la récompense de démonstrations de la tâche à effectuer accomplies par un expert. Une fois la récompense extraite des démonstrations expertes, elle peut être optimisée par un algorithme d'AR fournissant ainsi une politique de contrôle imitant le comportement de l'expert.

Ces méthodes rentrent dans le cadre plus large de l'apprentissage par imitation, où l'on trouve notamment les algorithmes d'imitation supervisée qui n'utilisent pas de description de la tâche à effectuer (rôle joué en AR par la fonction de récompense) mais apprennent l'association démontrée par l'expert entre la situation de la machine dans l'environnement et les actions qu'il effectue. Cet apprentissage, qui correspond au problème de la classification multi-classe, ne tient pas compte de la structure temporelle présente dans les données. Il présente l'avantage de ne nécessiter que des données expertes au prix de difficultés à généraliser le comportement de l'expert dans les zones de l'environnement où celui-ci n'a pas fourni de démonstrations.

L'ARI demande à l'inverse des données plus difficiles à réunir mais, grâce à l'optimisation de la fonction de récompense par un algorithme d'AR une fois celle-ci extraite des données, promet une meilleur capacité de généralisation. Les méthode d'ARI de l'état de l'art obéissent pour la plupart à un schéma itératif qui compare de manière répétée la politique optimale pour la récompense courante à la politique de l'expert, par le biais de l'attribut moyen. Le calcul répété d'une politique optimale et de son attribut moyen rendent ces approches inopérantes en l'absence d'un simulateur de l'environnement considéré. Une méthode d'ARI plus moderne ne nécessite pour fonctionner que les données expertes et des données représentatives (i.e. tirées par une politique aléatoire) du MDP afin d'effectuer de l'échantillonnage préférentiel.

Notre thèse porte sur la définition d'algorithmes d'ARI efficaces en échantillons. Le but est de bénéficier de la souplesse d'utilisation des algorithmes supervisés, qui ne nécessitent pour fonctionner que des données faciles à réunir dans la pratique, tout en conservant la bonne capacité de généralisation des algorithmes d'ARI. A cet effet nous fournissons un algorithme d'estimation de l'attribut moyen qui permet de faire fonctionner les algorithmes itératifs de la littérature sans avoir besoin d'un simulateur, celui-ci étant remplacé par des données représentatives du MDP.

Nous proposons ensuite deux nouveaux algorithmes d'ARI. Le premier introduit la dynamique temporelle du MDP dans une méthode de classification structurée par le biais de l'attribut moyen (qui peut être évalué par l'algorithme d'estimation que nous proposons). L'attribut moyen intervient dans la définition de la fonction de score du classifieur. Le second algorithme, plus souple, cascade une méthode de classification avec une étape de régression, qui en inversant l'équation de Bellman permet d'obtenir une fonction de récompense à partir de la fonction de score du classifieur.

Ces deux nouvelles méthodes disposent de garanties théoriques portant sur l'optimalité de la politique experte vis-à-vis de la fonction de récompense renvoyée par l'algorithme d'ARI. Armant chacune d'une heuristique qui lui est propre, nous illustrons empiriquement leur capacité à déduire une fonction de récompense à partir de démonstrations expertes uniquement. Les performances de nos algorithmes sont supérieures à celles d'algorithmes d'imitation supervisée et à un algorithme d'ARI moderne disposant de données non expertes supplémentaires. 

\end{document}
