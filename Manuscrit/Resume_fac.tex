\documentclass[9pt]{article}
\pagestyle{empty}
\usepackage[francais]{babel}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{fullpage}
\usepackage{graphicx}
\usepackage{fancybox}

\setlength{\textwidth}{160mm}

\setlength{\oddsidemargin}{0mm}
% \setlength{\evensidemargin}{30mm}

\begin{document}
\section*{Résumé}
Ce manuscrit s'intéresse au problème du contrôle optimal, qui consiste à trouver un comportement maximisant un critère défini par l'opérateur. Il traite plus particulièrement des méthodes permettant de trouver le contrôle optimal de manière générique et non \emph{ad-hoc} à un environnement particulier, lorsque le modèle dynamique de cet environnement est inconnu.

Un paradigme efficace pour la résolution générique et sans modèle du problème du contrôle optimal est l'Apprentissage par Renforcement (AR). Les critères de réussite de la tâche sont fournis par l'opérateur sous la forme d'un signal de récompense défini dans un Processus Décisionnel de Markov (PDM). Les algorithmes d'AR trouvent (éventuellement de manière approchée) par interactions avec l'environnement le comportement maximisant le cumul des récompenses sur le long terme. La conception par l'opérateur humain de ce signal de récompense reste une tâche ardue nécessitant d'une part des connaissances en AR et d'autre part une expertise quant au problème que la machine doit résoudre.

Afin de contourner cette étape difficile, les techniques dites d'Apprentissage par Renforcement Inverse (ARI) sont apparues. Elles extraient la récompense de démonstrations de la tâche à effectuer, démonstrations accomplies par un expert. Cette récompense peut alors être optimisée par un algorithme d'AR fournissant ainsi une politique de contrôle imitant le comportement de l'expert.

Ces méthodes rentrent dans le cadre plus large de l'apprentissage par imitation, où l'on trouve notamment les algorithmes d'imitation supervisée qui n'utilisent pas de description de la tâche à effectuer (rôle joué en AR par la fonction de récompense) mais apprennent l'association démontrée par l'expert entre la situation de la machine dans l'environnement et les actions qu'il effectue. Cet apprentissage, qui correspond au problème de la classification multi-classe, ne tient pas compte de la structure temporelle présente dans les données. Il présente l'avantage de ne nécessiter que des données expertes au prix de difficultés à généraliser le comportement de l'expert dans les zones de l'environnement où celui-ci n'a pas fourni de démonstrations.

L'ARI demande à l'inverse des données plus difficiles à réunir mais, grâce à l'optimisation de la fonction de récompense par un algorithme d'AR une fois celle-ci extraite des données, promet une meilleure capacité de généralisation. Les méthodes d'ARI de l'état de l'art obéissent pour la plupart à un schéma itératif qui compare de manière répétée la politique optimale pour la récompense courante à la politique de l'expert, par le biais de l'attribut moyen. Le calcul répété d'une politique optimale et de son attribut moyen rendent ces approches inopérantes lorsque le modèle est inconnu. Une méthode d'ARI plus moderne ne nécessite pour fonctionner que les données expertes et des données tirées par une politique aléatoire afin d'effectuer de l'échantillonnage préférentiel.

Cette thèse porte sur la définition d'algorithmes d'ARI efficaces en échantillons. Le but est de bénéficier de la souplesse d'utilisation des algorithmes supervisés, qui ne nécessitent pour fonctionner que des données faciles à réunir dans la pratique, tout en conservant la bonne capacité de généralisation des algorithmes d'ARI. A cet effet ce document fournit un algorithme d'estimation de l'attribut moyen qui permet de faire fonctionner les algorithmes itératifs de la littérature sans avoir besoin d'un simulateur, celui-ci étant remplacé par des données représentatives du PDM.

Deux nouveaux algorithmes d'ARI sont ensuite proposés. Tous deux reposent sur un constat de similarité entre le rôle de la fonction de qualité de l'expert et celui de la fonction de score d'un classifieur. Le premier algorithme introduit la dynamique temporelle du PDM dans une méthode de classification structurée par le biais de l'attribut moyen (qui peut être évalué par l'algorithme d'estimation précédent). L'attribut moyen intervient dans la définition de la fonction de score du classifieur. Le second algorithme, plus souple, cascade une méthode de classification avec une étape de régression, qui en inversant l'équation de Bellman permet d'obtenir une fonction de récompense à partir de la fonction de score du classifieur.

Ces deux nouvelles méthodes disposent de garanties théoriques portant sur l'optimalité de la politique experte vis-à-vis de la fonction de récompense renvoyée par l'algorithme d'ARI. Armée chacune d'une heuristique qui lui est propre, leur capacité à déduire une fonction de récompense à partir de démonstrations expertes uniquement est illustrée empiriquement. Les performances de ces algorithmes sont supérieures à celles d'algorithmes d'imitation supervisée et à un algorithme d'ARI moderne disposant de données non expertes supplémentaires. 

\end{document}
