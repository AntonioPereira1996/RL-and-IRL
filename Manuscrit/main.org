#+TITLE:Plan
* Options and headers :noexport:
#+OPTIONS: tags:0
# (setq org-export-latex-hyperref-format "\\ref{%s}")
#+LaTeX_CLASS: article 
#+LaTeX_CLASS_OPTIONS: [frenchb]


#+LATEX_HEADER: \usepackage{natbib}
#+latex_header: \usepackage{stmaryrd}
#+LaTeX_header:\usepackage[utf8]{inputenc}
#+LaTeX_header:\usepackage[T1]{fontenc}
#+LaTeX_header:\usepackage{babel}
#+LaTeX_header:\newglossary[angl]{anglicisme}{aot}{atn}{Anglicismes}
#+LaTeX_header:\newcommand{\newangl}[3]{\newglossaryentry{#1}{type=anglicisme,name={\emph{#2}},description={#3}}}
#+LaTeX_header:\makeglossaries

** Acronymes
#+LaTeX_header:\newacronym{mdp}{PDM}{Processus Décisionnel de Markov}
#+LaTeX_header:\newacronym{irl}{ARI}{Apprentissage par Renforcement Inverse}
#+LaTeX_header:\newacronym{dp}{PD}{Programmation Dynamique}
#+LaTeX_header:\newacronym{rl}{AR}{Apprentissage par Renforcement}
#+LaTeX_header:\newacronym{lspi}{LSPI}{\emph{Least Square Policy Iteration}}
#+LaTeX_header:\newacronym{pirl}{PIRL}{\emph{Projection Inverse Reinforcement Learning}, algorithme proposé dans \cite{abbeel2004apprenticeship}}
#+LaTeX_header:\newacronym{mmp}{MMP}{\emph{Maximum Margin Planning}}
#+LaTeX_header:\newacronym{pm}{PM}{\emph{Policy Matching}}
#+LaTeX_header:\newacronym{mwal}{MWAL}{\emph{Multiplicative Weights for Apprenticeship Learning}}
#+LaTeX_header:\newacronym{maxent}{MaxEnt}{\emph{Maximum Entropy}}
#+LaTeX_header:\newacronym{relent}{RelEnt}{\emph{Relative Entropy}}
#+LaTeX_header:\newacronym{lpal}{LPAL}{\emph{Linear Programming for Apprenticeship Learning}}
#+LaTeX_header:\newacronym{birl}{BIRL}{\emph{Bayesian Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{gpirl}{GPIRL}{\emph{Gaussian Processes Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{firl}{FIRL}{\emph{Feature Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{irlgp}{FIRL}{\emph{Inverse Reinforcement Learning with Gaussian Processes}}
#+LaTeX_header:\newacronym{lstdmu}{LSTD-$\mu$}{\emph{Least Square Tenporal Differences feature expectations}}
#+LaTeX_header:\newacronym{lstd}{LSTD}{\emph{Least Square Tenporal Differences}}
#+LaTeX_header:\newacronym{scirl}{SCIRL}{\emph{Structured Classification for Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{cascading}{CSI}{\emph{Cascaded Supervised learning for Inverse reinforcement learning}}
#+LaTeX_header:\newacronym{cnn}{CNN}{\emph{Convolutional Neural Network}}
#+LaTeX_header:\newacronym{svm}{SVM}{\emph{Support Vector Machine} (Etonnament bien "traduit" en français par Séparateur à Vaste Marge)}
#+LaTeX_header:\newacronym{gmm}{GMM}{\emph{Gaussian Mixture Model}}
#+LaTeX_header:\newacronym{alvinn}{ALVINN}{\emph{Autonomous Land Vehicle In a Neural Network}}
#+LaTeX_header:\newacronym{churps}{CHURPs}{\emph{Compressed Heuristic Universal Reaction Planners}}
#+LaTeX_header:\newacronym{knn}{$k$-NN}{$k$ \emph{Nearest Neighboors}}

#+LaTeX_header:\newangl{batch}{batch}{Par paquet}
#+LaTeX_header:\newangl{offpolicy}{off-policy}{Hors ligne}
#+LaTeX_header:\newangl{gridworld}{gridworld}{Echiquier, damier}
#+LaTeX_header:\newangl{rewardshaping}{reward shaping}{Transformation de la récompense ne changeant pas les politiques optimales}
#+LaTeX_header:\newangl{mixing}{mixing}{Mixante}
#+LaTeX_header:\newangl{boosting}{boosting}{Ajout de nouveaux attributs}

* TAF :noexport:
** TODO Articuler les parties problème jouets
* Axe de recherche 
** Contrôle optimal 
** Trouver la consigne à partir du contrôle 
*** Intérêt intrinsèque 
    Approches économiques, biologiques ou psychologiques.
*** Imitation 
    Cadre général de  l'imitation, mentionner ici les approches sans formalisme \gls{mdp}, si il y en a qu'il est pertinent de mentionner.
** Annonce du plan 
* Formalisme mathématique, notations				       
** Imitation non \gls{irl} 
# <<hier:nonari>>
# Goal:Introduire uniquement le formalisme nécessaire à l'imitation par classification.
# Goal:Ce serait bien qu'on ressente le besoin des notions du \gls{mdp}, notamment la récompense
# Requires:Agent artificiel, environnement, tâche
# Ensures: État, action, politique, classifieur, erreur de classification, politique de l'expert, traces sa, généralisation, attributs, classif structurée de taskar
# ?s policy '\pi' 'Une politique'
# ?s statespace '\mathcal{S}' 'Espace d{\apos}état'
# ?s actionspace '\mathcal{A}' 'Espace d{\apos}action'
# ?cs 2 twosetsfunctions '{#2}^{#1}' 'B^A' 'Ensemble des applications de $A$ dans $B$.'
*** Formalisme
   D'un point de vue mathématique, un agent, qu'il soit artificiel ou humain, qu'il s'agisse de l'expert qu'il soit cours d'apprentissage ou même qu'il agisse de manière aléatoire (utile pour l'exploration), implémente une politique. Une politique $\policy$ est formellement définie comme une application d'un espace d'état vers un espace d'action :
\begin{equation}
\policy \in \twosetsfunctions{\statespace}{\actionspace}.
\end{equation}
  
# ?s state 's' 'État'
   Cette politique encode le comportement de l'agent : dans un état $\state \in \statespace$, l'agent choisira l'action $\policy(\state) \in \actionspace$. L'on constate que ce formalisme implique que pour choisir son action, l'agent n'utilise que les informations stockées dans l'état. Il faudra donc en pratique veiller à ce que celui-ci contienne toutes les informations utiles à la prise de décision, c'est à dire par exemple pour un système physique, non seulement les valeurs courantes des capteurs, mais aussi peut-être certaines valeurs passées afin de pouvoir calculer des taux de variation. 
# Exemple: could use an exemple (pendule ?)
# snippet: La notion d'agent artificiel déborde sur l'espace d'état, qui n'est lui même du coup pas directement lié à l'environnement. Un agent, ce n'est pas seulement une politique, mais aussi la définition de l'espace d'état et d'action, qui ne sont qu'une vue (plus ou moins bonne selon l'ingénierie) de la réalité
   
   Les espaces d'état et d'action sont mal nommés puisqu'il s'agit en général non d'espaces vectoriels mais de parties d'espaces vectoriels : la plage de valeurs que peut prendre une composante est rarement illimitée. Il s'agit même parfois de parties finies. C'est dans les problèmes qui nous préoccupent quasiment toujours le cas pour l'espace d'action. Nous considérons par défaut qu'il s'agit d'un ensemble fini de faible cardinal.

*** Classification
# ?s expertpolicy '\pi^E' 'Politique de l{\apos}expert'
   Il est possible de voir le problème de l'imitation comme celui de la recherche d'une politique correspondant point par point (ou au mieux en fonction des contraintes en mémoire et en temps de l'agent) à celle de l'expert. Il est en effet évident que si deux politiques sont égales alors elles accomplissent la même tâche avec le même degré d'efficacité. Même lorsque la politique de l'expert (notée $\expertpolicy$) est intégralement connue cette formulation n'est pas forcément dénuée de sens, l'on peut en effet souhaiter remplacer l'expert par un agent moins coûteux mais donc probablement plus limité. Apprendre par cœur (ou apprendre au mieux avec quelques erreurs) la politique de l'expert est alors sensé.

   Bien souvent cependant il est impossible de connaître intégralement la politique de l'expert, ne fut-ce que parce que l'espace d'état est trop grand. Il faut alors se contenter d'exemples sur un certain nombre $\nbsamples$ d'états :
# ?cs 1 satrace 'D_{sa}^{#1}' 'D_{sa}^{\policy}' 'Trace de type $s,a$ obtenue en suivant la politique $\policy$'
# ?s nbsamples 'N' 'Nombre d{\apos}exemples dans une trace'
# ?s action 'a' 'Une action'
# ?s datasetindex 'i' 'Entier indexant une base de données'
\begin{equation}
\satrace{\expertpolicy} = \{(\state_{\datasetindex},\action_{\datasetindex}=\expertpolicy(\state_{\datasetindex})) | \datasetindex \in \llbracket 0;\nbsamples-1\rrbracket\}.
\end{equation}

   Le problème de l'imitation se trouve ainsi réduit à celui de la classification. Étant donné que l'espace d'action est fini et de faible cardinal, chaque action est vue comme un label à appliquer à un état. La démonstration de l'expert fournit la base d'entraînement.

# ?s spacedistrib '\rho' 'Loi de probabilité ou fonction de poids'
# ?s classifpolicy '\pi^C' 'Politique issue d{\apos}un classifieur'
# ?s empiricalclassiferror '\epsilon_C^{empirical}' 'Erreur empirique de classification'
# ?cs 1 classiferror '\epsilon_C^{#1}' '\epsilon_C^{\spacedistrib}' 'Erreur théorique de classification sur la distribution $\spacedistrib$'
# ?cs 1 indicatorfunc '\mathds{1}(#1)' '\mathds{1}' 'Fonction indicatrice'
# ?cs 3 weightedexpectation '\E_{#1 \sim #2} \left[ #3\right]' '\E_{x \sim \rho} \left[ f(x)\right]' 'Espérance de $f(x)$ pour $x$ tiré selon $\rho$'
La classification est un problème plus subtil que celui de naïvement apprendre par cœur la base d'entraînement. Ce que nous cherchons à optimiser n'est pas la performance sur la base d'entraînement fournie, mais la performance sur l'espace d'état en général. Plus précisément, certains états nous intéressent plus que d'autres. Pour une justification intuitive, il suffit de penser aux jeux de plateau, où bien agir dans les quelques états qui apparaissent souvent en début de partie est beaucoup plus intéressant que bien agir dans un état bizarre que l'on ne rencontrera peut-être jamais ; d'où par exemple le travail sur les ouvertures aux échecs. Pour mesurer l'importance accordée à un état, l'on définit une fonction de poids qui somme à un : $\spacedistrib \in \twosetsfunctions{\statespace}{[0,1]}$ telle que $\int_{\statespace} \spacedistrib(s) ds = 1$ ou $\sum_{\state \in \statespace} \spacedistrib(\state) = 1$ selon que l'on se trouve en cas fini ou continu. La mesure de l'erreur d'une politique de classification se basant uniquement sur la base d'entraînement :
\begin{equation}
\empiricalclassiferror = {1\over \nbsamples}\sum_{(\state_{\datasetindex},\action_{\datasetindex}) \in \satrace{\expertpolicy}} \indicatorfunc{\classifpolicy(\state_{\datasetindex}) \neq \action_{\datasetindex}}
\end{equation}
est potentiellement différente de celle que l'on cherche réellement à optimiser :
\begin{eqnarray}
\classiferror{\spacedistrib} &=& \sum_{\state \in \statespace} \spacedistrib(\state)\indicatorfunc{\classifpolicy(\state) \neq \expertpolicy(\state)}\\
&=& \weightedexpectation{\state}{\spacedistrib}{ \indicatorfunc{\classifpolicy(\state) \neq \expertpolicy(\state)}}.
\end{eqnarray}
Les problèmes de sur-apprentissage apparaissent lorsque l'on minimise l'erreur empirique au détriment de l'erreur de classification $\classiferror{\spacedistrib}$. Une des difficultés étant malheureusement que l'on ne peut qu'estimer $\classiferror{\spacedistrib}$.

Le choix de la distribution $\spacedistrib$ sur laquelle il nous importe d'optimiser la classification est également un paramètre important. On peut comme on l'a vu intuitivement favoriser les états de départ. Il est possible de tenter d'estimer à partir d'une base d'exemple la vraie distribution des états qui seront soumis au contrôle de l'agent. On peut également pour certains environnements accorder plus d'importance à certains états critiques où une erreur aurait des conséquences fâcheuses.

*** Attributs
# ?s featurestateactionspace '\Phi' 'Espace d{\apos}attributs état-action'
# ?s featurestateactionfunc '\phi' 'Fonction d{\apos}attributs état-action'
# ?s dimphi 'd_{\phi}' 'Dimension de l{\apos}espace d{\apos}attributs état-action'
    Ce que l'on appelle la capacité de généralisation d'un classifieur est son aptitude à minimiser l'erreur théorique $\classiferror{\spacedistrib}$ à l'aide de données tirées selon une distribution potentiellement différente de $\spacedistrib$. Cette capacité de généralisation est affectée par la manière dont l'espace d'état apparaît au classifieur. Afin d'obtenir une description exploitable du problème, l'on va souvent choisir de travailler non pas directement dans l'espace d'état-action $\statespace\times\actionspace$ mais dans un espace d'attributs $\featurestateactionspace$ qui est l'image de l'espace d'état-action $\statespace\times\actionspace$ par une fonction vectorielle d'attribut $\featurestateactionfunc \in \twosetsfunctions{\statespace \times \actionspace}{\reals^{\dimphi}}$ :
    \begin{equation}
    \featurestateactionspace = \featurestateactionfunc(\statespace\times\actionspace)
    \end{equation}
Illustrons ce propos par l'étude d'une approche de classification qui utilise une fonction de score linéairement paramétrée sur l'espace d'attribut état-action $\featurestateactionspace$ : \cite[Chapitre 10]{taskar2005learning}.

# ?s classifscorefunc 'q' 'Fonction de score pour la classification'
# ?s reals '\mathbb{R}' 'Le corps des réels'
Le principe quasi-ubiquitaire en classification[fn::Les classifieurs à arbres de décision formant un contre-exemple notable \citep{safavian1991survey}.] de la fonction de score est le suivant : à chaque couple état-action une fonction $\classifscorefunc$ associe un score. Pour associer une action à un état, le classifieur passe simplement en revue toutes les actions (on voit donc l'intérêt d'un petit espace d'action) et choisit celle qui associée à cet état obtient le score le plus haut :
\begin{eqnarray}
\classifscorefunc &\in& \twosetsfunctions{\statespace \times \actionspace}{\reals},\\
\forall \state, \classifpolicy(\state) &=& \arg\max_{\action \in \actionspace} \classifscorefunc(\state,\action).
\end{eqnarray}
Apprendre une bonne fonction de score permet donc de résoudre le problème de classification. L'approche proposée dans \citep{taskar2005learning} prend le parti d'une fonction de score paramétrée linéairement. Il va de soi que rien ne garantit qu'une fonction de score linéaire sur l'espace d'état-action soit en mesure de donner un bon classifieur, dès lors le recours à une fonction d'attribut choisie avec soin est indispensable :
# ?s paramclassif '\omega' 'Vecteur de paramètres pour la classification'
# ?cs 1 transpose '#1^T' 'X^T' 'Transposée de la matrice ou du vecteur $X$'
\begin{equation}
q(\state,\action) = \transpose{\paramclassif}\featurestateactionfunc(\state,\action)
\end{equation}
Ce que nous cherchons maintenant est donc un bon vecteur de paramètres $\paramclassif$. Une telle recherche serait vaine si les attributs choisis ne permettaient pas d'exprimer une bonne fonction de score.

# ?s featurestatefunc '\psi' 'Fonction d{\apos}attribut sur l{\apos}espace d{\apos}état'
# ?s dimpsi 'd_{\psi}' 'Dimension de l{\apos}espace d{\apos}attributs sur l{\apos}espace d{\apos}état'
# ?cs 1 card '\left|#1\right|' '|A|' 'Cardinal de l{\apos}ensemble A'
Si l'on dispose d'une fonction d'attribut $\featurestatefunc \in \twosetsfunctions{\statespace}{\reals^{\dimpsi}}$ sur l'espace d'état, une technique classique pour obtenir une fonction d'attribut sur l'espace d'état-action consiste à exploser la représentation sur les différentes actions. D'un vecteur de dimension $\dimpsi$, l'on passe à un vecteur de dimension $\dimphi = \card{A}\dimpsi$ (où $\card{\cdot}$ dénote le cardinal d'un ensemble) en définissant :
# ?cs 2 kronecker '{\delta}^{#1}_{#2}' '\delta^i_j' 'Symbole de Kronecker, vaut $1$ si $i=j$, $0$ sinon'
\begin{equation}
\featurestateactionfunc(\state,\action) = \begin{pmatrix}
\kronecker{\action}{\action_1}\featurestatefunc(\state)\\
\vdots\\
\kronecker{\action}{\action_{\card{\actionspace}}}\featurestatefunc(\state)\\
\end{pmatrix}
\end{equation}
avec $\kronecker{i}{j}$ le symbole de Kronecker.

Le choix d'une bonne fonction d'attributs sur l'espace d'état est extrêmement problème-dépendant, néanmoins dans les cas les plus sympathiques deux techniques simples donnent de bons résultats. Dans le cas d'un espace d'état fini de taille raisonnable, il est possible de définir une fonction d'attribut binaire en associant un unique indice à chaque état. Le vecteur d'attribut d'un état est nul partout sauf en l'indice associé à l'état :
\begin{equation}
\featurestatefunc(\state) = \begin{pmatrix}
\kronecker{\state}{\state_1}\\
\vdots\\
\kronecker{\state}{\state_{\card{\statespace}}}\\
\end{pmatrix}.
\end{equation}
Un avantage de ce schéma est qu'il permet une représentation exacte de la fonction de score. En effet le produit $q(\state,\action) = \transpose{\paramclassif}\featurestateactionfunc(\state,\action)$ revient à isoler la composante de $\paramclassif$ correspondant à l'unique indice associé au couple $(\state, \action)$. Les deux gros désavantages sont l'incapacité de ce schéma de passer à l'échelle et l'absence totale de structure : l'on aura beau disposer d'énormément d'information sur les "voisins" d'un élément de l'espace, tant que l'on aura pas vu précisément cet élément dans la base d'exemple, c'est la valeur par défaut de la coordonnée correspondante dans $\paramclassif$ qui sera utilisée.

# ?cs 1 gaussperdim 'g_{#1}' 'g_i' 'Nombre de gaussiennes pour la dimension $i$ dans un vecteur d{\apos}attribut basé sur un réseau de gaussiennes'
# ?s dimstate 'd_{\mathcal{S}}' 'Dimension de l{\apos}esapce d{\apos}état'
# ?s gaussiancenter 'm' 'Centre d{\apos}une gaussienne'
# ?s gaussianvar '\sigma' 'Variance d{\apos}une gaussienne'
# ?cs 3 gaussian '\mathfrak{G}^{#1}_{#2}(#3)' '\mathfrak{G}^{m}_{\sigma}' 'Fonction gaussienne de centre $m$ et de variance $\sigma$'
# ?cs 2 component '{#1}^{#2}' 'X^j' 'Composante $j$ du vecteur $X$'
# ?s dimindex 'j' 'Entier indexant les dimensions d{\apos}un espace'
# ?s dimindexbis 'k' 'Entier indexant les dimensions d{\apos}un espace'
Pour les espaces continus, une paramétrisation usuelle consiste à paver l'espace de gaussiennes. L'on assigne un nombre $\gaussperdim{\dimindex}$ à chacune des dimensions $0 < \dimindex \leq \dimstate$ de l'espace d'état et l'on construit un maillage de $\dimpsi = \prod_{\dimindex=1}^{\dimstate}\gaussperdim{\dimindex}$ points $m_{\dimindexbis}, 0<\dimindexbis\leq\dimpsi$ répartis à équidistance dans l'espace qui seront les centres des $\dimpsi$ composantes gaussiennes de la fonction d'attribut. La variance pour une dimension $\dimindex$ peut être choisie par exemple comme 
\begin{equation}
\gaussianvar^{\dimindex} = { \max(\component{\state}{\dimindex})-\min(\component{\state}{\dimindex})\over 2 \gaussperdim{\dimindex}}.
\end{equation}
En notant :
\begin{equation}
\gaussian{\gaussiancenter}{\gaussianvar}{s} = \exp\left(-\sum_{\dimindex=1}^{\dimstate}{\component{s}{\dimindex}-\component{\gaussiancenter}{\dimindex}\over 2\component{\gaussianvar}{\dimindex}}\right),
\end{equation}
on obtient finalement la fonction d'attribut suivante :
\begin{equation}
\featurestatefunc(s) = \begin{pmatrix}
\gaussian{\gaussiancenter_{1}}{\sigma}{s}\\
\vdots\\
\gaussian{\gaussiancenter_{\dimpsi}}{\sigma}{s}\\
\end{pmatrix}.
\end{equation}
Contrairement à la fonction d'attribut binaire précédente, celle-ci possède une structure spatiale. Les scores de deux états proches subiront l'influence de la même composante du vecteur de paramètre. Quand le nombre de dimension augmente, le nombre de gaussiennes du réseau explose. Cette technique n'échappe donc pas à la malédiction de la dimension.

Considérant maintenant que nous disposons d'un vecteur d'attribut permettant de continuer, examinons la fonction de coût que \citet{taskar2005learning} se propose de minimiser (l'expression est modifiée :
# ?cs 1 best '{#1}^{*}' 'x^*' 'Element issu d{\apos}un $\arg\max_x$'
# ?s margin '\mathfrak{l}' 'Fonction de marge dans le classifieur à marge'
# ?s structuredcost 'J' 'Fonction de coût de la classification structurée'
\begin{eqnarray}
\structuredcost(\classifscorefunc) &=& {1\over \nbsamples} \sum_{\datasetindex=1}^{\nbsamples} \classifscorefunc(\state_{\datasetindex},\best{\action}_{\datasetindex}) + \margin(\state_{\datasetindex},\best{\action}_{\datasetindex}) - \classifscorefunc(\state_{\datasetindex},\action_{\datasetindex})\\
\structuredcost(\paramclassif) &=& {1\over \nbsamples} \sum_{\datasetindex=1}^{\nbsamples} \transpose{\paramclassif}\featurestatefunc(\state_{\datasetindex},\best{\action}_{\datasetindex}) + \margin(\state_{\datasetindex},\best{\action}_{\datasetindex}) - \transpose{\paramclassif}\featurestatefunc(\state_{\datasetindex},\action_{\datasetindex})\\
\textrm{avec : }\best{\action}_{\datasetindex} &=& \arg\max_{\action \in \actionspace} \classifscorefunc(\state_{\datasetindex},\action) + \margin(\state_{\datasetindex},\action)\\
 &=& \arg\max_{\action \in \actionspace} \transpose{\paramclassif}\featurestatefunc(\state_{\datasetindex},\action) + \margin(\state_{\datasetindex},\action).
\end{eqnarray}

La fonction de marge $\margin$ est là pour donner à ce classifieur sa capacité de généralisation. En effet, on constate que si $\margin$ est uniformément nulle, alors parvenir à minimiser parfaitement la fonction de coût revient à apprendre par cœur la base d'exemple, c'est à dire à probablement subir les effets du sur-apprentissage. Fixer 
\begin{equation}
\margin(\state_{\datasetindex},\action) = \begin{cases}
0 &\textrm{si }\action = \action_{\datasetindex}\\
1 &\textrm{si }\action \neq \action_{\datasetindex}
\end{cases}
\end{equation}
permet d'introduire un marge assez simple : minimiser la fonction de coût apprend une fonction de score qui donne aux choix de l'expert un score supérieur aux score des autres choix. \citet{taskar2005learning} précise qu'il est possible d'adapter la marge $\margin$ en fonction de la qualité des choix alternatifs, un bon choix correspondant à une petite marge. Nous verrons qu'en pratique la marge binaire que nous venons de suggérer fonctionne assez bien.

# ?s subgrad '\nabla' 'Sous gradient d{\apos}une fonction'
Nous empruntons à \cite{ratliff2006maximum} une technique simple pour minimiser cette fonction de coût, basée sur une descente de sous gradient. Le sous gradient de la fonction de coût est :
\begin{equation}
\subgrad\structuredcost(\paramclassif) = \sum_{\datasetindex=1}^{N}\featurestatefunc(\state_{\datasetindex},\best{\action}_{\datasetindex}) - \featurestatefunc(\state_{\datasetindex},\action_{\datasetindex})
\end{equation}

Nous avons présenté cette technique de classification plus en détail à des fins d'illustration de l'importance du choix des attributs, et également car nous la retrouverons plus loin dans le manuscrit lorsque nous nous intéresserons aux techniques d'\gls{irl}.

Il existe bien d'autres moyen de faire de la classification, comme par exemple les \gls{svm}.
*** \gls{svm}
# Goal: Faire une description rapide des MCSVM et de leurs avantages.
# Goal: Introduire la notion de noyau, de kenel-trick et faire le lien avec les attributs (poil au cul)
# Requires: attribut, classification
# Ensures noyau, kernel-trick, SVM

Nous aurons également à faire avec des \gls{svm} dans la suite du manuscrit. Une description exhaustive des techniques d'apprentissage supervisé propices à l'imitation dépasserait le cadre de cette thèse. \citet{hastie2005elements,vapnik1998statistical} sont des ouvrages de référence. Nous allons voir comment ces techniques ont été appliquées en pratique à l'apprentissage supervisé de la politique de l'expert.
*** Imitation par apprentissage supervisé de la politique
# Goal:Faire l'état de l'art des techniques d'imitation par apprentissage supervisé
# Goal:Ce serait bien qu'on ressente le besoin des notions du \gls{mdp}, notamment la récompense (bis)
# Requires:Classifieur, attributs, classif de taskar, (boosting?)
# Ensures: Boosting, ratliff2007imitation, 
Apprendre la politique de l'expert de manière supervisée à l'aide d'une base d'exemples peut s'avérer efficace, comme le démontrent plusieurs approches. Dans \citep{ratliff2007imitation}, les auteurs utilisent le classifieur à marge décrit plus haut
# checkref
# ?s featurestateactionhypothesisspace '\mathcal{H}_{\Phi}' 'Espace d{\apos}hypothèse où choisir une nouvelle composante pour l{\apos}attribut état action'
# ?cs 2 scalarprod '\left\langle\left.{#1}\right | {#2}\right\rangle' '\langle x|y\rangle' 'Produit scalaire de $x$ et $y$'
pour apprendre une politique experte sur un problème de locomotion quadrupède et sur un problème de manipulation d'objets. Le choix des attributs est simplifié grâce à une technique de \gls{boosting} similaire à \cite{friedman2001greedy,mason1999functional} : on choisit dans un espace d'hypothèse la fonction qui, ajoutée comme composante supplémentaire de la fonction d'attribut, aiderait le mieux à minimiser la fonction de coût. Mathématiquement, on choisit la fonction dont le produit scalaire avec le gradient de la fonction de coût est minimal : la nouvelle composante $\featurestateactionfunc_{\dimphi +1}$ est choisie dans l'espace d'hypothèse $\featurestateactionhypothesisspace$ selon 
\begin{equation}
\featurestateactionfunc_{\dimphi +1} = \arg\max_{\featurestateactionfunc\in \featurestateactionhypothesisspace} \scalarprod{\featurestateactionfunc}{-\subgrad \structuredcost}.
\end{equation}

Le boosting permet de déplacer de manière intelligente le problème du choix des attributs, sans le régler totalement. Il reste en effet à construire l'espace d'hypothèse $\featurestateactionhypothesisspace$ où choisir les nouveaux attributs. Un espace trop simple ne permettrait pas de minimiser efficacement la fonction de coût, tandis qu'un espace trop centré sur les données permettrait de la minimiser totalement, mais sans doute au prix d'un sur-apprentissage aux conséquences fâcheuses. C'est donc cet espace qui doit être calibré et construit afin de donner au classifieur ses capacités de généralisation. \citet{ratliff2007imitation} proposent d'utiliser un réseau de neurones.

Plus brutale, l'approche de \citet{lecun2006off} utilise un \gls{cnn} à 6 couches pour apprendre une association directe entre une image (stéréo) d'entrée et un angle de braquage (la tâche à apprendre est la conduite d'un véhicule en terrain libre). Le problème de la généralisation est résolu en exigeant une base d'entraînement couvrant au maximum l'espace d'état. Les auteurs ne cachent pas la difficulté de réunir une telle base qui doit réunir des conditions de terrain et d'illumination variées tout en exigeant un comportement extrêmement cohérent et prédictible de la part de l'opérateur humain et ce sur un grand nombre de trajectoires (il faut réunir près d'une centaine de milliers d'échantillons). En contrepartie de ces efforts, la technique proposée est robuste et ne nécessite aucun travail d'ingénierie au niveau des attributs, puisque la politique apprise associe directement la sortie du capteur à la consigne de l'actuateur du robot. Bien que cela soit moins problématique aujourd'hui avec l'augmentation de puissance des équipements embarqués, elle semble également plus rapide (dans l'exploitation, non dans l'apprentissage) que l'état de l'art de l'époque. Elle améliore les résultats notamment par rapport à \gls{alvinn} \citep{pomerleau1993knowledge} en ceci que la résolution des caméras peut être augmentée sans trop grande explosion du réseau grâce à l'usage de la convolution et non d'un réseau complètement connecté, et que la tâche apprise est plus difficile, il s'agit de conduire en terrain libre et non de suivre une route.

Nous venons de voir deux techniques différents permettant d'apprendre une politique à partir d'un base de données inerte, de manière supervisée, avec une intervention humaine minimale : soit l'on dispose de suffisamment de données pour qu'un apprentissage par cœur de la base corresponde à un apprentissage performant sur tout l'espace d'état, soit l'on construit des attributs (ou si l'on utilise du \gls{boosting} un espace d'hypothèse où les choisir) tels que l'apprentissage au mieux (en minimisant une fonction de coût exprimée sur les données) ne soit pas un apprentissage par cœur, mais un apprentissage généralisant sur tout l'espace d'état. Apprendre une politique de manière locale, c'est à dire en se concentrant trop sur une base de données lacunaire, n'est pas satisfaisant. Cela donne un résultat fragile, l'agent sera en effet pris au dépourvu s'il a à contrôler le système dans une configuration différente de celle sur laquelle il a été entraîné : il ne dispose ni d'information relative au comportement de l'expert dans une telle situation, ni d'information sur la tâche à accomplir qui lui permettraient de déduire ce que pourrait être ce comportement.

Brisant la contrainte de la base de données inerte, l'idée de demander ces échantillons de manière interactive a été proposée afin de minimiser la quantité de données nécessaire à l'apprentissage de la politique experte. Un exemple d'une telle approche est décrit en \citep{chernova2007confidence}. Des \gls{gmm} sont appris à partir d'une base de données experte de départ, puis l'agent applique la politique apprise tout en demandant à l'expert, lorsque l'incertitude est trop grande, de lui fournir un échantillon supplémentaire. Cette approche permet de limiter la redondance de la base d'entraînement et de guider l'échantillonnage vers les zones intéressantes de l'espace d'état, ce qui est également une solution au problème de la généralisation : quand l'agent ne sait pas généraliser, il demande à l'expert.

L'apprentissage direct de la politique experte est parfois intégré à dans un cadre plus large, où les notions de hiérarchie et de but apparaissent.

La classification par arbre de décision a été appliquée à l'apprentissage d'un plan de vol dans \citep{sammut1992learning}. L'application est impressionnante, piloter un avion, même en simulation, n'est pas une mince affaire puisqu'il fut en temps réel prendre en compte un grand nombre de facteurs pour décider d'une action parmi un éventail assez large. Les facteurs limitants de l'apprentissage automatique sont les mêmes que pour l'approche précédente : un grand nombre d'échantillons est requis (du même ordre de grandeur, de l'ordre de la centaine de milliers) et un comportement cohérent est exigé de l'expert humain (à un point tel que les démonstrations de deux experts ne peuvent être mélangées en une seule base d'entraînement). De plus, l'aspect automatique de l'approche reste limité à l'apprentissage d'une politique par phase de vol. La détection de la phase de vol courante et donc le choix de la politique de contrôle à appliquer est effectué par des règles d'origine humaine. Pour chaque phase, la politique apprise n'est robuste qu'à de petites variations dans les états rencontrés.

De fait, cette approche a été le point de départ de nombreuses améliorations. Le travail présenté par \citet{stirling1995churps} (appelé \gls{churps}) consiste a déduire un contrôleur à partir d'une description du modèle d'évolution du système et du but à atteindre. Pour automatiser la création de ces descriptions, tâche réclamant un travail difficile car nécessitant de décrire des mécanismes précis à l'aide d'un langage contraignant, \citet{bain2000framework} proposent d'utiliser les données de l'expert. Les règles complexes ainsi apprises étant ajoutées à l'espace d'action, il est possible d'apprendre de manière automatique un classifieur plus concis que celui de \citet{sammut1992learning}, et nécessitant moins de données expertes. L'architecture proposée utilise la logique du premier ordre et donc le raisonnement symbolique. Cela permet d'introduire explicitement des connaissances expertes dans le système. Ces connaissances peuvent être acquises semi-automatiquement : les prédicats sont bâtis à la main et les paramètres sont appris grâce aux données de vol comme le proposent \citet{srinivasan1998inductive}. La sémantique des symboles (ici, virage, altitude, trajectoire de vol, etc.) est très liée au problème concerné. Retrouver la puissance des techniques d'apprentissage symbolique sur un autre problème nécessite d'effectuer de nouveau le difficile travail de définition des symboles et prédicats. Un autre élément gênant est la difficulté d'exprimer la tâche à accomplir en utilisant un langage symbolique. Dans une approche hybride symbolique/automatique, \citet{shiraz1997combining} proposent à l'expert soit de décrire la tâche symboliquement, soit d'en démontrer l'exécution. Les phases les plus délicates (par exemple l'atterrissage) n'ont pu être décrites et ont été démontrées. La facilité d'exploitation des règles symbolique rentre en conflit avec la difficulté qu'il y a à les définir, à l'inverse la relative facilité de génération d'une base d'exemple se heurte à la difficulté qu'il y a à généraliser à partir de celle-ci.

Une autre approche utilise les notions de hiérarchie et de but, mais de manière quelque peu différente. Plutôt que d'utiliser la logique des prédicats, ce sont les principes de programmation impérative qui se voient assistés par l'apprentissage supervisé. Dans \cite{saunders2006teaching}, ce sont les \gls{knn} qui sont utilisés pour l'apprentissage supervisé d'une politique. Les attributs sont construits à la main à partir des valeurs de sortie des capteurs du robot, et portent une sémantique forte et explicite (distance, angle), donc pratique pour l'exploitation par un opérateur humain. Les problèmes de généralisation de l'apprentissage supervisé sont contournés par l'intégration dans un cadre beaucoup plus riche : l'opérateur humain peut élargir l'espace d'action à volonté, soit en définissant une séquence d'actions qui seront exécutées en série de manière déterministe, soit en proposant des exemples du comportement souhaité en précisant ou non un état-but correspondant à la situation dans laquelle on souhaite voir le robot une fois la politique exécutée. Ces exemples servent alors à l'apprentissage d'une politique de manière supervisée, cette politique est ajoutée en tant qu'action et son exécution pourra être déclenchée dans le cadre d'une autre politique, de niveau d'abstraction plus grand. Cette hiérarchisation des comportements permet de limiter l'effort humain, d'optimiser l'utilisation des exemples et de rapidement mettre en place des comportements complexes par la création de nouveaux niveau d'abstraction. 

L'apprentissage supervisé est dans les approches que nous venons de citer utilisé comme sous routine d'un système beaucoup plus large, dans lequel l'expertise humaine explicite reste le moyen central permettant la généralisation des comportements.


# Méthode de regroupement des actions : on apprend plus une politique en la copiant mais on essaie de comprendre comment fonctionne l'expert.
# ?? Moultes autres approches, labyrinthiques, exemples ultra rapide, se référer à blip et blop pour un survey
# ^(saunders2006teaching) citation [22] semble en proposer un survey. (saunders2006teaching) en propose lui-même un bon
# L'idée est bonne, mais (problèmes liés à l'approche). Ce qu'il faudrait c'est comprendre le but de l'expert, et essayer d'isoler ça.
# FIXME: La notion de but apparâit plusieurs fois
# Trucs que je sais pas où foutre :
# saunders2006learning, sec 3 : si on observe l'expert, on a pas accès à ses sensations ni à ses ordres directement, et ils correspondent pas à ceux de l'expert. Quoiqu'en changeant l'espace d'action (tour, fou etc.) , on devrait y arriver.  #correspondance problem
# 
# Trucs que j'ai pas lu, mais qu'il faudrait peut-être lire et mettre dans ce chapitre ou ailleurs, mais dont j'espère qu'ils sont de moindre importance et que donc c'est pas grave si je n'en parle pas
# (argall2009survey) T. Inamura, M. Inaba, H. Inoue, Acquisition of probabilistic behavior decision model based on the interactive teaching method, in: Proceedings of the Ninth International Conference on Advanced Robotics, ICAR’99, 1999.
# En fait toute la section 4.1 de argall2009survey mériterait d'être explorée ici, mais c'est long et chiant et il se fait tard.
# Faudrait aussi se faner schaal et son gros survey, mais c'est vraiment mal écrit, et je pense pas que je jeu en vaille la chandelle. Il faudrait penser à le citer, cependant.
# FIXME: normalement ce paragraphe est complètement explosé en plusieurs autres
# Ya bentivegna2004learning qui sert à rien mais qu'on peut rajouter si ya besoin de parler pour ne rien dire (problème dépendant)
# ya coates2008learning qui est impressionnant mais qui rentre dans aucune case
# ya  konidaris2011cst que je sais pas où foutre
# ya  leon2011teaching que je sais pas ou ranger non plus
# Quelque part il faudrait rajouter  montana2011towards
# Et natarajan2011imitation, c'est du supervisé, ou pas ?
# J'ai l'impression de m'embarquer dans un labyrinthe sans fin, avec toujours plus de papiers à résumer. Il est impossible d'être exhaustif en si peu de temps.
# FIXME Citer les deux surveys 

# Paragrapge de transition : Les sytèmes qui facilitent la vie aux humains en utilisant le supervisé en sous routine ont en filigrane la notion de but. S'il était possible de formaliser cette notion afin d'en faire profiter l'apprentissage automatique, on serait les rois du pétrole
** Cadre des \glspl{mdp} pour la prise de décision séquentielle
# Snippet: La classification ne se soucie pas de l'objectif de l'expert. Quid si une action mal choisie fait dérailler l'agent sur une partie totalement inconnue de l'espace d'état ?
*** TODO justifier le cadre, dire pourquoi on introduit ces notions
   Probabilités de transitions, fonction de récompense, fonction de valeur, politique optimale. \gls{dp}.

   Approximation de la fonction de valeur, \gls{rl}. L'\gls{rl} permettant d'apprendre le contrôle par interaction avec le système, il possède quelques avantages sur le \gls{dp}, comme la possibilité de s'adapter à un milieu changeant pour certains algorithmes ou de manière plus générale l'absence de besoin de connaître les probabilités de transition.

   Pour appliquer l'\gls{rl} au monde réel, il est nécessaire d'exploiter efficacement les échantillons. Les échantillons sont très faciles à obtenir si on dispose d'un simulateur, et sont les seules données accessible sur certains systèmes. Pouvoir les exploiter en \gls{batch} et \gls{offpolicy} permet de contrôler beaucoup de types de systèmes différents (qui peut le plus peut le moins). \gls{lspi} \cite{lagoudakis2003least}.
** Définition de l'\gls{irl} 
*** Définition du problème 
   Cette section pose les notations qu'on utilisera dans tous le manuscrit, on ne cite pas l'état de l'art tout de suite (puisque les notions et notations entrent parfois en conflit, notamment en ce qui concerne l'attribut moyen) sauf Russell cela est reporté au chapitre d'après. Il s'agit de donner les acteurs et de préciser le problème que l'on résout, on ne rentre pas dans la description des problèmes qu'on va rencontrer tout de suite. C'est une section assez courte mais plutôt dense.

   Dans le cadre de l'\gls{rl}, trouver la consigne à partir du contrôle devient l'\gls{irl}. Il s'agit de retrouver la fonction de récompense à partir d'une politique optimale.

   Nuances importantes : le MDP est un modèle qui a des limites. Il faut que les états soient markoviens, il faut en pratique que l'espace d'action ne soit pas trop large, on suppose que l'expert agit de manière optimale pour une récompense sur ce MDP. Il faut donc que l'expert agisse effectivement de manière optimale (un humain peut faire des erreurs) et que la fonction de récompense existe (au sens philosophique, en math il y en a au moins toujours une), et soit exprimable sur l'espace d'état etc. Le problème qu'on étudie est bien celui de trouver la fonction de récompense (qui existe) d'un expert (qui est vraiment optimal).

*** Attribut moyen 
    L'approximation linéaire de la fonction de récompense induit (par le biais de la fonction de valeur) l'apparition de l'attribut moyen. C'est, comme nous allons le voir au chapitre suivant, une notion centrale en renforcement inverse. C'est un fonction vectorielle qui porte la structure temporelle du MDP contraint par la politique (bref de la chaine de Markov sous-jacente) (illustrer graphiquement sur le \gls{gridworld}, par exemple).

    Deux politiques ayant des attributs moyens similaires auront des valeurs similaires quelle que soit la récompense (exprimée dans le schéma d'approximation linéaire) considérée. En revanche, il est possible d'avoir deux attributs moyens complètement différents et d'avoir la même valeur vis à vis de la "vraie" fonction de récompense (illustration sur le \gls{gridworld}, passage en haut à gauche et passage en bas à droite).
* État de l'art et problématique
** Fonction de récompense 

   Récompense état ou état-action ? En changeant l'espace d'état, les deux sont équivalents. Le \gls{rewardshaping}   \cite{ng1999policy} étudie les changements que l'on peut appliquer à une récompense sans changer les politiques optimales.

** Premières formulation du problème 
   Mentionné pour la première fois dans \cite{russell1998learning}. La formulation informée (qui connait les probabilités de transition) du problème \cite{ng2000algorithms} n'est pas bien posée. Deux solutions sont malgré tout proposées dont une fait déjà usage de l'attribut moyen, mais pas sous ce nom. La solution informée n'est pas celle qui nous intéresse pour les mêmes raisons qui nous font préférer l'\gls{rl} au DP (voir plus haut). La solution approchée est également problématique (FIXME:préciser pourquoi exactement).

# Rem Olivier: Quand on fait des critiques purs
   Détail important : dans l'\gls{rl}, on cherche le point fixe d'un opérateur attractif, il existe une solution et elle est unique. Dans l'\gls{irl} la solution n'est pas unique et il existe des solutions dégénérées. Résoudre le problème de l'\gls{irl} risque donc de s'avérer plus complexe que de résoudre le problème de l'\gls{rl}.

** Méthodes nécessitant la résolution répétée d'un MDP
# Rem olivier:  En gros, on trouve une récompense et on regarde, par optimisation, si le mu de la politique pour R est proche de celui de la politique de l'expert. Ça nécessite de résoudre l'\gls{rl} et de connaître la politique de l'expert partout. 
   \cite{abbeel2004apprenticeship} propose \gls{pirl}, un algorithme qui sert de base à beaucoup d'autres solutions par la suite. L'idée est, par itérations successives sur la récompense, de rapprocher l'attribut moyen de l'agent et celui de l'expert. Pour une certaine notion de distance entre attributs moyens.  Cette approche fondatrice impose cependant des contraintes assez fortes :
   - il faut de manière répétée résoudre un MDP
   - il faut de manière répétée estimer l'attribut moyen d'une politique arbitraire et de la politique experte
   - la sortie est une politique avec du $\beta$\gls{mixing}, pas une récompense

     
   \cite{ratliff2006maximum} propose \gls{mmp}, une approche dont on peut tordre la formulation pour la comparer à \gls{pirl} \cite{neu2009training}, mais qui associe non pas des états à des actions mais des politiques à des MDP. Cela pose encore d'autres problèmes comme la nécessité de résoudre de multiples \glspl{mdp} de manière tractable, et de formuler le problème du contrôle non pas comme un MDP, mais comme de multiples \glspl{mdp} "compatibles" entre eux. Cet algorithme peut apprendre des attributs par \gls{boosting} \cite{ratliff2007boosting}.
# Utiliser l'articulation sans oute présente (sais pas exactement, pas encore lu) dans ratliff2009learning pour tous les papiers de ratliff

   La technique proposée dans \cite{neu2007apprenticeship} (\gls{pm}) est plus robuste que celles décrites jusqu'à présent aux changements d'échelles des attributs ou au bruit dans les attributs. Elle souffre des même types de contraintes que \gls{pirl} ; comme \gls{pirl}, elle raisonne sur des politiques et non sur des récompenses.

 # Rem olivier: Puis faut la dynamique
   Basé sur la théorie des jeux, l'algorithme \gls{mwal} de \cite{syed2008game} tombe sur un os[fn::Os à \gls{mwal}, humour.]. Plus rapide à l'exécution que \gls{pirl} et capable de gérer la non optimalité de l'expert, il exige cependant énormément de connaissances à priori sur le problème : il faut que le vecteur de paramètres de l'approximation de la récompense soit positif. Il souffre des mêmes soucis de résolution répétée du MDP et d'estimation de l'attribut moyen que \gls{pirl}. Ce papier mentionne les problèmes liés aux raisonnements sur des politiques mixées et non des récompenses, sans pour autant les résoudre explicitement.

   Ce travail est étendu dans \cite{syed2008apprenticeship}, en formulant la résolution du MDP comme un programme linéaire, on peut trouver une politique stationnaire, sans \gls{mixing} dans \gls{pirl} et \gls{mwal}. En formulant l'\gls{irl} comme un programme linéaire également, les auteurs trouvent \gls{lpal}, qui retourne un politique (non une récompense). La formulation de ce programme linéaire met en jeu des grandeurs qui correspondent à l'attribut moyen de politiques optimales pour des récompenses arbitraires.

   \gls{maxent} de \cite{ziebart2008maximum} raisonne également sur les récompenses et non les politiques. La formulation probabiliste de la méthode est intéressante et donne un critère (l'entropie) pour choisir entre deux politiques qui jusqu'à présent étaient équivalentes (même valeur ou même attribut moyen). Structurellement, cependant, les mêmes défauts réapparaissent. Le calcul des quantités en jeu implique toujours la résolution répétée d'un MDP et l'estimation de l'attribut moyen de politiques arbitraire.

   Une approche bayésienne (\gls{birl}) de \cite{ramachandran2007bayesian} (et curieusement donnant le même algorithme qu'une approche non officiellement \gls{irl} et plus ancienne, \cite{chajewska2001learning}) diffère dans l'exposition du raisonnement, mais reste très similaire dans l'implémentation, puisque'il faut toujours calculer des politiques optimales et obtenir des échantillons de cette politique (pour calculer des postérieurs bayésiens et non plus des attributs moyens).

   La plupart de ces approches sont résumées dans \cite{neu2009training}. Ces différentes contributions ont le mérite d'observer le problème sous plusieurs angles, de se placer aux limites du problème (expert non optimal, attributs bruités, etc.) et d'aborder des questions fondamentales (notion de distance entre politiques ou récompenses). Le manque d'harmonisation du domaine de l'\gls{irl} (encore jeune) se fait sentir. Chacun redéfinit le problème à sa manière. Toutes ces approches sont malgré cela structurellement très similaires (résolution répétée de MDP et approximation de l'attribut moyen). Les approches les plus tardives font apparaître les difficultés soulevées par la recherche d'un politique mixée, d'une politique au lieu d'une récompense, de l'absence d'un critère commun, de la résolution répétée du MDP et de l'approximation de l'attribut moyen. Si de bonnes solutions aux deux premiers problèmes sont proposées, les autres soucis ne trouvent en revanche pas de réponse.
   
   Citer les applications (acrobatie hélico p.e.) et expliquer comment ils ont contourné le problème (c'est dur à contourner, mais c'est faisable). Signaler que  ça serait bien pour ces applications si on disposait d'algorithmes n'ayant pas ces contraintes.

** Méthodes ne nécessitant pas la résolution répétée d'un MDP 
# Rem olivier : faudrait voir si la définition de la métrique n'impose pas de résolution ou quelque chose de similaire. Me souviens plus exactement. 
   \cite{lopes2009active} : définir une métrique dans un MDP reste difficile.

   \gls{gpirl} \cite{levine2011nonlinear} dans la lignée de \gls{firl} \cite{levine2010feature} débloque plusieurs problèmes d'un coup. On ne suppose plus que la fonction de récompense est linéaire dans les attributs. On peut l'apprendre et faire de la sélection de attribut par la même occasion. FIXME: Il y avait une raison pour laquelle ce n'était pas tractable, la retrouver.

   "\gls{irlgp}" \cite{qiao2011inverse} et \cite{jin2010gaussian} ne sont pas tractables non plus FIXME:retrouver pourquoi.


     \gls{mdp} soluble linéairement : \cite{dvijotham2010inverse}. Il faut des \gls{mdp} solubles linéairement.

   \gls{relent} fait sauter la plupart des contraintes qui nous préoccupent (argumentaire à rapprocher de \gls{maxent}) \cite{boularias2011relative}. Il suffit d'avoir des échantillons experts et des échantillons aléatoires. On est dans le cadre définit avant de commencer l'état de l'art, et non plus dans le cadre mal défini des approches qui on suivi \gls{pirl}.

* \gls{lstdmu} 
# <<hier:lstdmu>>
** Principe 
  On rappelle que l'attribut moyen est une grandeur centrale en renforcement (dit en [[Attribut moyen]]). Les algorithmes qui l'utilisent ne présupposent pas de moyen de le calculer. La méthode de base consiste à faire jouer un simulateur et faire une estimation de Monte-Carlo.

  L'attribut moyen est par sa définition une fonction de valeur vectorielle. \gls{lstd} peut donc être adapté pour l'approximer.
** Avantages 
  Les avantages que \gls{lstd} possède pour l'approximation de fonction de valeur : \gls{batch}, /offline/ et /sample-efficient/ sont transférés à l'approximation de l'attribut moyen.

  On peut ainsi estimer l'attribut moyen d'une politique arbitraire sans utiliser de simulateur et sans connaître les probabilités de transition.

** Illustration							       
# rem olivier : Faire la liste des expériences en disant ce qu'elles montrent. C'est nécessaire pour trouver une structure cohérente aux parties expérimentales dans les 3 chapitres de contribution.
   En utilisant \gls{pirl} avec LSPI et \gls{lstdmu}$\mu$, on peut porter PIRL en mode /batch/ avec une perte de performance minimale, et que l'on peut mitiger en fonction de la quantité de données non expertes disponibles. Cela évite d'avoir à se servir d'un simulateur, qui n'est pas toujours disponible. En le modifiant pour qu'il sorte une récompense et non une politique on peut corriger les "erreurs de jeunesse" des algorithmes dans la lignée de \gls{pirl}. Ça reste non idéal cependant.
   
* \gls{scirl}
# snippet: tiré de ratliff2009learning, l'extrait suivant est exactement ce que SCIRL fait : Unfortunately, such a system is inherently myopic. This form of imitation learning assumes that all information necessary to make a decision at the current state can be easily encoded into a single vector of features that enables prediction with a relatively simple, learned function. However, this is often particularly difficult because the features must encode information regarding the future long-term consequences of taking an action. Under this framework, much of the work that we would like learning to handle is pushed onto the engineer in the form of feature extraction.
** Liens entre classification et \gls{rl}
  La classification peut-être utilisée pour faire de l'imitation (fait mentionné en sous-section \ref{hier:nonari}). Cela à l'avantage de ne nécessiter que des données de l'expert. Mais cela ne tient pas compte de la structure du MDP. La plupart des classifieurs apprennent une fonction de score [fn::Les arbres sont une exception.]. De fait la règle de décision du classifieur et la règle de décision d'un agent optimal dans un \gls{mdp} (équation présentée en [[Cadre des \glspl{mdp} pour la prise de décision séquentielle]]) sont similaires. On peut donc dresser un parallèle entre la fonction de score du classifieur et la fonction de qualité de l'expert.

  \gls{scirl} et \gls{cascading} (décrit en \ref{hier:cascading}) utilisent cette similarité pour introduire la structure du MDP dans (ou à la suite de) une méthode de classification. On espère ainsi pouvoir faire de l'\gls{irl} (trouver une récompense, pas apprendre une politique par copie) tout en profitant des avantages offerts par la méthode supervisée (efficacité en termes de données, implémentations /off-the-shelf/, etc.).

  Si l'on utilise un classifieur où cette fonction de score/qualité est approximée par un schéma linéaire, alors on retombe sur l'attribut moyen. Il faut encore approximer celui-ci, mais cela est courant dans la littérature, et surtout c'est précisément le problème résolu par \gls{lstdmu} (en [[hier:lstdmu]]). 

** Description 
** Validation théorique 
** Validation pratique rapide (problèmes jouets) 
*** TODO Donner la structure : qu'est-ce que chaque application démontre ?
** Mise en relief de l'influence des routines 
** Conclusion 
   SCIRL règle quelques problèmes des premiers algorithmes d'IRL et a de meilleures performances que \gls{maxent}. Il est théoriquement simple à expliquer et résout bien le problème de l'\gls{irl} tel qu'on l'a formulé. Il peut fonctionner avec uniquement des données de l'expert, ce qui ouvre la porte à des applications réelles.

* SCIRLBoost 
** Problème de la définition des fonctions de base  
** Description de l'algorithme 
** Validation sur les problèmes jouets 
* Cascading 
# <<hier:cascading>>
** Description 
*** Description 
*** Différences de concept avec SCIRL 
** Validation théorique 
** Validation pratique rapide (problèmes jouets) 
** Mise en relief de l'influence des routines 
** Comparaison pratique rapide de SCIRL et Cascading 
** Conclusion 
   Cascading a des performances similaires à SCIRL et est encore plus flexible, puisque des méthodes supervisées non paramétriques (ou à détermination automatique de paramètres) peuvent être employées, ce qui en plus de résoudre les problèmes structurels de PIRL, résout les problèmes plus fondamentaux de l'approximation linéaire de la fonction de valeur ou du choix des attributs.
* (Validation expérimentale) 
* Rappel des contributions 
* Perspectives de recherche 
* Bibliographie 
\bibliographystyle{plainnat}
\bibliography{../../Biblio/Biblio}
* Glossaire 
\printglossaries
