#+TITLE:Plan
* Options and headers :noexport:
** tikz
#+LATEX_HEADER:\usepackage{tikz} 
#+LATEX_HEADER:\usetikzlibrary{shapes.arrows}
#+LATEX_HEADER:\usetikzlibrary{calc}
#+LATEX_HEADER:\usetikzlibrary{decorations.pathreplacing}
#+LATEX_HEADER:\tikzset{for this and nested ones/.style={#1,every picture/.style={#1}}}
** Misc.
#+OPTIONS: tags:0
# (setq org-export-latex-hyperref-format "\\autoref{%s}")
#+LaTeX_CLASS: article 
#+LaTeX_CLASS_OPTIONS: [frenchb]

#+LATEX_HEADER:\hypersetup{ % config hyperref pour virer les box/color affreux...
#+LATEX_HEADER:    colorlinks,%
#+LATEX_HEADER:    citecolor=black,%
#+LATEX_HEADER:    filecolor=black,%
#+LATEX_HEADER:    linkcolor=black,%
#+LATEX_HEADER:    urlcolor=black
#+LATEX_HEADER:} 
#+LATEX_HEADER: \usepackage{natbib}
#+LATEX_HEADER: \usepackage{subfigure}
#+latex_header: \usepackage{stmaryrd}
#+LaTeX_header:\usepackage[utf8]{inputenc}
#+LaTeX_header:\usepackage[T1]{fontenc}
#+LaTeX_header:\usepackage[frenchb]{babel}
#+LaTeX_header:\usepackage{amsthm}
#+LaTeX_header:\newtheorem{proposition}{Proposition}
#+LaTeX_header:\newglossary[angl]{anglicisme}{aot}{atn}{Anglicismes}
#+LaTeX_header:\newcommand{\newangl}[3]{\newglossaryentry{#1}{type=anglicisme,name={\emph{#2}},description={#3}}}
#+LaTeX_header:\makeglossaries
#+LaTeX_header:\usepackage[french,ruled,vlined,noend,linesnumbered]{algorithm2e}


** Acronymes
#+LaTeX_header:\newacronym{mdp}{PDM}{Processus D{é}cisionnel de Markov}
#+LaTeX_header:\newacronym{irl}{ARI}{Apprentissage par Renforcement Inverse}
#+LaTeX_header:\newacronym{dp}{PD}{Programmation Dynamique}
#+LaTeX_header:\newacronym{rl}{AR}{Apprentissage par Renforcement}
#+LaTeX_header:\newacronym{lspi}{LSPI}{\emph{Least Square Policy Iteration}}
#+LaTeX_header:\newacronym{pirl}{PIRL}{\emph{Projection Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{mmp}{MMP}{\emph{Maximum Margin Planning}}
#+LaTeX_header:\newacronym{pm}{PM}{\emph{Policy Matching}}
#+LaTeX_header:\newacronym{mwal}{MWAL}{\emph{Multiplicative Weights for Apprenticeship Learning}}
#+LaTeX_header:\newacronym{maxent}{MaxEnt}{\emph{Maximum Entropy}}
#+LaTeX_header:\newacronym{relent}{RelEnt}{\emph{Relative Entropy}}
#+LaTeX_header:\newacronym{lpal}{LPAL}{\emph{Linear Programming for Apprenticeship Learning}}
#+LaTeX_header:\newacronym{birl}{BIRL}{\emph{Bayesian Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{gpirl}{GPIRL}{\emph{Gaussian Processes Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{firl}{FIRL}{\emph{Feature Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{irlgp}{FIRL}{\emph{Inverse Reinforcement Learning with Gaussian Processes}}
#+LaTeX_header:\newacronym{lstdmu}{LSTD-$\mu$}{\emph{Least Square Tenporal Differences feature expectations}}
#+LaTeX_header:\newacronym{lstd}{LSTD}{\emph{Least Square Tenporal Differences}}
#+LaTeX_header:\newacronym{scirl}{SCIRL}{\emph{Structured Classification for Inverse Reinforcement Learning}}
#+LaTeX_header:\newacronym{cascading}{CSI}{\emph{Cascaded Supervised learning for Inverse reinforcement learning}}
#+LaTeX_header:\newacronym{cnn}{CNN}{\emph{Convolutional Neural Network}}
#+LaTeX_header:\newacronym{svm}{SVM}{\emph{Support Vector Machine} (Etonnament bien "traduit" en français par Séparateur à Vaste Marge)}
#+LaTeX_header:\newacronym{gmm}{GMM}{\emph{Gaussian Mixture Model}}
#+LaTeX_header:\newacronym{alvinn}{ALVINN}{\emph{Autonomous Land Vehicle In a Neural Network}}
#+LaTeX_header:\newacronym{churps}{CHURPs}{\emph{Compressed Heuristic Universal Reaction Planners}}
#+LaTeX_header:\newacronym{knn}{$k$-NN}{$k$-\emph{plus proches voisins}}
#+LaTeX_header:\newacronym{gp}{PG}{Processus Gaussiens}
** Anglicismes
#+LaTeX_header:\newangl{batch}{batch}{Par paquet}
#+LaTeX_header:\newangl{offpolicy}{off-policy}{Signifie que la politique qui contrôle le système n'est pas celle qui est évaluée}
#+LaTeX_header:\newangl{onpolicy}{on-policy}{\`A la différence du {\it on-policy}, la politique évaluée est celle qui contrôle le système}
#+LaTeX_header:\newangl{gridworld}{gridworld}{Echiquier, damier}
#+LaTeX_header:\newangl{rewardshaping}{reward shaping}{Transformation de la récompense ne changeant pas les politiques optimales}
#+LaTeX_header:\newangl{mixing}{mixing}{Mixante}
#+LaTeX_header:\newangl{boosting}{boosting}{Ajout de nouveaux attributs}
#+LaTeX_header:\newangl{mountaincar}{mountain-car}{Problème jouet où une voiture doit sortir d'un creux}

* TAF :noexport:
** TODO Articuler les parties problème jouets
** Nettoyage
*** DONE Virer barres à droite graphes psi et mu, utiliser même échelle pour tout le monde (échelle monomodale, d'ailleurs)
*** DONE trouver un moyen de mettre automatiquement les noms (section, equation, etc.) devant les refs (autoref?)
*** TODO Introduire le mountain-car de belle manière
*** DONE virer l'extension glossaire de la table des matières
*** TODO Mettre sous forme ODF et envoyer à JCK
*** DONE Mettre les labels sous forme # <<>>
** Trucs à ajouter je ne sais où
*** Contre exemple politique, tache
    Je peux décrire une politique sur un espace d'état, mais pas la récompense, exemple. le mettre au début de 2.3.1 ou ailleurs.
*** Echiquier et changement d'espace d'état
    Avec la tour : on peut optimiser de manière différente
    Avec le fou : on peut optimiser mieux si on a pas le même espace d'état
* Axe de recherche 
# Mountain car stuff
#+begin_latex
\newsavebox\tuture
\begin{lrbox}{\tuture}
   \begin{tikzpicture}
    \fill[yellow] (0,0) -- (1.2,0) -- (1.2,.5) -- (0,.5) --cycle;
    \draw[black] (0,0) -- (1.2,0) -- (1.2,.5) -- (0,.5) --cycle;
    \node[shape=circle,draw=black,fill=green,scale=1] at (0.3,0) {};
    \node[shape=circle,draw=black,fill=green,scale=1] at (.9,0) {};
    \node[red] (carMark) at (.6,.2) {\bf x};
  \end{tikzpicture}
\end{lrbox}
\newcommand{\speedarrow}[1]{
  \begin{tikzpicture}
    \node [single arrow,fill=blue!50,minimum height=#1,single arrow head indent=3] at(0,0) {};
  \end{tikzpicture}
}
#+end_latex
** Contrôle optimal 
** Trouver la consigne à partir du contrôle 
*** Intérêt intrinsèque
# <<hier:intrinseque>> 
    Approches économiques, biologiques ou psychologiques.
*** Imitation 
# <<hier:general>>
    Cadre général de  l'imitation, mentionner ici les approches sans formalisme \gls{mdp}, si il y en a qu'il est pertinent de mentionner.
    Citer les revues : \cite{argall2009survey,schaal2003computational}, expliquer le cadre large, le problème de l'observabilité de l'expert et de l'adéquation expert/agent (mentionner le changement d'espace d'état?).
** Problèmes jouets
Pour accompagner les explications nous étudierons FIXME: deux ? problèmes jouets. Ces problèmes sont suffisamment simples pour qu'une interprétation des grandeurs en jeu soit possible, mais suffisamment complexes pour présenter un défi aux approches existantes.
*** Problème du \glstext{mountaincar}
Dans le \gls{mountaincar}, une voiture peu puissante placée dans un creux doit gravir une côte trop pentue pour être surmontée du premier coup. La solution du problème consiste à reculer, donc à s'éloigner de l'objectif, en remontant la pente opposée puis à profiter de l'élan pour gravir la pente. Le processus peut-être répété si besoin. Cette propriété contre intuitive de devoir s'éloigner de l'objectif pour pouvoir l'atteindre fait de ce problème jouet un bon test pour les algorithmes d'\gls{rl}. Il a été proposé pour la première fois par \citet{moore1990efficient} et a été popularisé lors son introduction dans le livre de \citet{sutton1998reinforcement}.

La \autoref{fig:mcstatespace} décrit les variables qui représentent l'information dont dispose l'agent pour ce problème. Il y a bien sûr la position de la voiture : le point le plus à gauche a une coordonnée de $-1.2$, l'objectif, situé le plus à droite, se trouve à la coordonnée $0.6$. Le point le plus bas est à $-0.5$. La deuxième information est la vitesse de la voiture, elle peut aller de $-0.07$ à $0.07$ où un chiffre positif désigne naturellement un déplacement vers la droite et négatif vers la gauche. La vitesse est plafonnée, et si la voiture atteint l'extrémité gauche, sa vitesse est annulée et elle reste en place.
\begin{figure}
\begin{tikzpicture}[for this and nested ones={scale=.4},transform shape]
\input{Figures/Mountain_car_state_space.tex}
\end{tikzpicture} 
\caption[Espace d'état du \glstext{mountaincar}]{L'espace d'état du \glstext{mountaincar} : la position de la voiture est représentée en abscisse et sa vitesse en ordonnée. Représenté sur la figure de gauche par des points : un exemple d'une trajectoire d'une bonne politique de contrôle où l'expert poursuit la montée de la pente opposée à l'objectif avant de la redescendre et de parvenir à gravir la pente à droite. La couleur des points indique l'action effectuée par l'expert : en bleu, il accélère vers la gauche, en rouge vers la droite. En fond, le nombre de pas de temps nécessaire à cet expert pour parvenir à l'objectif. La forme en escargot est caractéristique de ce problème comme cela est expliqué \autoref{fig:mountain_car_mu}.}
\label{fig:mcstatespace}
\end{figure}
# *** \glstext{gridworld}
# C'est le problème du \gls{gridworld} qui fut utilisé pour l'illustration des premiers algorithmes d'\gls{irl} par \citet{ng2000algorithms}, il apparaît ensuite sous différentes variantes dans d'autres contributions. Il s'agit d'un damier carré de $5$ cases de côté. L'agent peut s'y déplacer d'une case à la fois dans l'une des 4 directions cardinales. La tâche a accomplir est, partant de la case en bas à gauche, d'atteindre la case en haut à droite. Pour complexifier les choses, les actions de l'agent ont un effet stochastique : dans $70\%$ des cas, il va se déplacer dans la direction choisie, mais chacune des trois autres directions a une probabilité $0.1$ d'être celle dans laquelle l'agent bouge. Si le mouvement sélectionné fait sortir l'agent du damier, alors il restte en place.

# Comme le problème est discret, il peut être résolu par \gls{dp}, ce qui permet de connaître les grandeurs de manière exacte, et donc de quantifier la qualité des approximations. Il est également intéressant de par les différents axes de symétrie. Dans l'idéal, une approche statistique devrait être capable de découvrir ces symétries à partir des données.

** Annonce du plan 
# <<hier:annonceplan>>   
Truc que j'ai pas encore dit : le truc limitant, d'après moi, ce sont les données. Du coup j'examine les algorithmes en fonction de leur complexité en échantillons.
* Formalisme mathématique, notations
  Il est maintenant temps de formaliser ce que nous venons de voir et d'introduire les notions qui serviront les explications à suivre.
** Imitation non \glsentrytext{irl} 
# <<hier:nonari>>
# Goal:Introduire uniquement le formalisme nécessaire à l'imitation par classification.
# Goal:Ce serait bien qu'on ressente le besoin des notions du \gls{mdp}, notamment la récompense
# Requires:Agent artificiel, environnement, tâche
# Ensures: État, action, politique, classifieur, erreur de classification, politique de l'expert, traces sa, généralisation, attributs, classif structurée de taskar
L'\gls{irl} est un moyen d'obtenir une description de la tâche en observant un expert réaliser cette tâche, et donc une méthode intéressante pour imiter cet expert. L'on a vu qu'obtenir une description de la tâche présentait un intérêt intrinsèque, différent de l'imitation, mais la majorité des applications de l'\gls{irl} relevant de l'imitation, c'est cet angle que nous allons principalement explorer. Intéressons nous tout d'abord à l'imitation au sens large, commençant par les méthodes ne mettant pas l'\gls{irl} en jeu.
# ?s policy '\pi' 'Une politique'
# ?s statespace '\mathcal{S}' 'Espace d{\apos}état'
# ?s actionspace '\mathcal{A}' 'Espace d{\apos}action'
# ?cs 2 twosetsfunctions '{#2}^{#1}' 'B^A' 'Ensemble des applications de $A$ dans $B$'
*** Formalisme
   D'un point de vue mathématique, un agent, qu'il soit artificiel ou humain, qu'il s'agisse de l'expert qu'il soit en cours d'apprentissage ou même qu'il agisse de manière aléatoire (utile pour l'exploration), implémente une politique. Une politique $\policy$ est formellement définie comme une application d'un espace d'état vers un espace d'action[fn::Une autre manière de formaliser les choses se repose sur les politiques stochastiques définies dans $\twosetsfunctions{\statespace\times\actionspace}{[0;1]}$ (ignorons le formalisme des espaces probabilisés pour cette discussion). Cela complexifie un peu l'analyse, et la perte de généralité lorsque l'on se cantonne aux politiques déterministe est minimale.] :
\begin{equation}
\policy \in \twosetsfunctions{\statespace}{\actionspace}.
\end{equation}
  
# ?s state 's' 'État'
   Cette politique encode le comportement de l'agent : dans un état $\state \in \statespace$, l'agent choisira l'action $\policy(\state) \in \actionspace$. L'on constate que ce formalisme implique que pour choisir son action, l'agent n'utilise que les informations contenues dans l'état. Il faudra donc en pratique veiller à ce que celui-ci contienne toutes les informations utiles à la prise de décision, c'est-à-dire par exemple pour un système physique, non seulement les valeurs courantes des capteurs, mais aussi peut-être certaines valeurs passées afin de pouvoir calculer des taux de variation. 
# Exemple: could use an exemple (pendule ?)
# snippet: La notion d'agent artificiel déborde sur l'espace d'état, qui n'est lui même du coup pas directement lié à l'environnement. Un agent, ce n'est pas seulement une politique, mais aussi la définition de l'espace d'état et d'action, qui ne sont qu'une vue (plus ou moins bonne selon l'ingénierie) de la réalité
   
   Les espaces d'état et d'action ne sont généralement pas des espaces vectoriels complets, mais des parties d'espaces vectoriels : la plage de valeurs que peut prendre une composante est rarement illimitée. Il s'agit même parfois de parties finies. C'est dans les problèmes qui nous préoccupent quasiment toujours le cas pour l'espace d'action. Nous considérons par défaut qu'il s'agit d'un ensemble fini de faible cardinal.

*** Classification
# <<hier:classification>>    

# ?s expertpolicy '\pi^E' 'Politique de l{\apos}expert'
   Il est possible de voir le problème de l'imitation comme celui de la recherche d'une politique correspondant pour chaque état (ou au mieux en fonction des contraintes en mémoire et en temps de l'agent) à celle de l'expert. Il est en effet évident que si deux politiques sont identiques alors elles accomplissent la même tâche avec le même degré d'efficacité. Même lorsque la politique de l'expert (notée $\expertpolicy$) est intégralement connue cette formulation n'est pas forcément dénuée de sens, l'on peut en effet souhaiter remplacer l'expert par un agent moins coûteux mais donc probablement plus limité. Apprendre par cœur (ou apprendre au mieux avec quelques erreurs) la politique de l'expert est alors sensé.

   Bien souvent cependant il est impossible de connaître intégralement la politique de l'expert, ne fut-ce que parce que l'espace d'état est trop grand. Il faut alors se contenter d'exemples sur un certain nombre $\nbsamples$ d'états :
# ?cs 1 satrace 'D_{sa}^{#1}' 'D_{sa}^{\policy}' 'Trace de type $s,a$ obtenue en suivant la politique $\policy$'
# ?s nbsamples 'N' 'Nombre d{\apos}exemples dans une trace'
# ?s action 'a' 'Une action'
# ?s datasetindex 'i' 'Entier indexant une base de données'
\begin{equation}
\satrace{\expertpolicy} = \{(\state_{\datasetindex},\action_{\datasetindex}=\expertpolicy(\state_{\datasetindex})) | \datasetindex \in \llbracket 0;\nbsamples-1\rrbracket\}.
\end{equation}

   Le problème de l'imitation se trouve ainsi réduit à celui de la classification. Étant donné que l'espace d'action est fini et de faible cardinal, chaque action est vue comme un label à appliquer à un état. La démonstration de l'expert fournit la base d'entraînement.

# ?s spacedistrib '\rho' 'Loi de probabilité ou fonction de poids'
# ?s classifpolicy '\pi^C' 'Politique issue d{\apos}un classifieur'
# ?s empiricalclassiferror '\epsilon_C^{emp}' 'Erreur empirique de classification'
# ?cs 1 classiferror '\epsilon_C^{#1}' '\epsilon_C^{\spacedistrib}' 'Erreur théorique de classification sur la distribution $\spacedistrib$'
# ?cs 1 indicatorfunc '\mathds{1}(#1)' '\mathds{1}' 'Fonction indicatrice'
# ?cs 2 expectationknowing '\E \left[\left. #1\right|#2\right]' '\E \left[\left. f(x)\right| x \sim \rho \right]' 'Espérance de $f(x)$ pour $x$ tiré selon $\rho$'
La classification est un problème plus subtil que celui de naïvement apprendre par cœur la base d'entraînement. Ce que nous cherchons à optimiser n'est pas la performance sur la base d'entraînement fournie, mais la performance sur l'espace d'état en général. Plus précisément, certains états nous intéressent plus que d'autres. Pour une justification intuitive, il suffit de penser aux jeux de plateau, où bien agir dans les quelques états qui apparaissent souvent en début de partie est beaucoup plus intéressant que bien agir dans un état improbable que l'on ne rencontrera peut-être jamais ; d'où par exemple le travail sur les ouvertures aux échecs. Pour mesurer l'importance accordée à un état, l'on définit une fonction de poids homogène à une densité de probabilité[fn::Pour éviter la pédanterie mathématique, nous allons identifier loi de probabilité et densité de probabilité, ce qui nous amènera à écrire des choses comme $s\sim\rho$, même si $\rho$ est défini comme une densité et non comme une loi.] et qui donc somme à un : $\spacedistrib \in \twosetsfunctions{\statespace}{[0,1]}$ telle que $\sum_{\state \in \statespace} \spacedistrib(\state) = 1$. La mesure de l'erreur d'une politique de classification $\classifpolicy$ se basant uniquement sur la base d'entraînement (avec $\indicatorfunc$ la fonction indicatrice) :
\begin{equation}
\empiricalclassiferror = {1\over \nbsamples}\sum_{(\state_{\datasetindex},\action_{\datasetindex}) \in \satrace{\expertpolicy}} \indicatorfunc{\classifpolicy(\state_{\datasetindex}) \neq \action_{\datasetindex}}
\end{equation}
est potentiellement différente de celle que l'on cherche réellement à optimiser :
\begin{eqnarray}
\classiferror{\spacedistrib} &=& \sum_{\state \in \statespace} \spacedistrib(\state)\indicatorfunc{\classifpolicy(\state) \neq \expertpolicy(\state)}\\
&=& \expectationknowing{ \indicatorfunc{\classifpolicy(\state) \neq \expertpolicy(\state)}}{\state\sim\spacedistrib}.
\end{eqnarray}
Les problèmes de sur-apprentissage apparaissent lorsque l'on minimise l'erreur empirique au détriment de l'erreur de classification $\classiferror{\spacedistrib}$. Une des difficultés étant malheureusement que l'on ne peut qu'estimer $\classiferror{\spacedistrib}$.

Généralement pour la classification, la distribution $\spacedistrib$ où l'on cherche à minimiser l'erreur de classification est la même que celle selon laquelle on échantillonne les données. Il y a alors "juste" à résoudre les différences entre l'erreur empirique et l'erreur théorique puis à minimiser cette dernière.

Il est possible de rendre les choses plus intéressantes en choisissant une autre distribution $\spacedistrib$ sur laquelle il nous importe d'optimiser la classification. On peut, comme on l'a vu intuitivement favoriser les états de départ. Il est possible de tenter d'estimer à partir d'une base d'exemple la vraie distribution des états qui seront soumis au contrôle de l'agent (qui peut être différente de la distribution à laquelle l'expert est confronté). On peut également pour certains environnements accorder plus d'importance à certains états critiques où une erreur aurait des conséquences fâcheuses.

*** Attributs
# <<hier:attributs>>
# ?s featurestateactionspace '\Phi' 'Espace d{\apos}attributs état-action'
# ?s featurestateactionfunc '\phi' 'Fonction d{\apos}attributs état-action'
# ?s dimphi 'd_{\phi}' 'Dimension de l{\apos}espace d{\apos}attributs état-action'
    Ce que l'on appelle la capacité de généralisation d'un classifieur est son aptitude à minimiser l'erreur théorique $\classiferror{\spacedistrib}$ à l'aide de données. Cette capacité de généralisation est affectée par la manière dont l'espace d'état apparaît au classifieur. Afin de pouvoir représenter le classifieur et d'obtenir une description exploitable du problème, l'on va souvent choisir de travailler non pas directement dans l'espace d'état-action $\statespace\times\actionspace$ mais dans un espace d'attributs $\featurestateactionspace$ de dimension $\dimphi$ qui est l'image de l'espace d'état-action $\statespace\times\actionspace$ par une fonction vectorielle d'attribut $\featurestateactionfunc \in \twosetsfunctions{\statespace \times \actionspace}{(\reals^{\dimphi})}$ :
    \begin{equation}
    \featurestateactionspace = \featurestateactionfunc(\statespace\times\actionspace).
    \end{equation}
Illustrons ce propos par l'étude d'une approche de classification qui utilise une fonction de score linéairement paramétrée sur l'espace d'attribut état-action $\featurestateactionspace$ : \cite[Chapitre 10]{taskar2005learning}.

# ?s classifscorefunc 'q' 'Fonction de score pour la classification'
# ?s reals '\mathbb{R}' 'Le corps des réels'
Le principe quasi-ubiquitaire en classification[fn::Les arbres de décision formant un contre-exemple notable \citep{safavian1991survey}.] de la fonction de score est le suivant : à chaque couple état-action une fonction $\classifscorefunc$ associe un score. Pour associer une action à un état, le classifieur passe simplement en revue toutes les actions (on voit donc l'intérêt d'un petit espace d'action) et choisit celle qui associée à cet état obtient le score le plus haut :
\begin{eqnarray}
\classifscorefunc &\in& \twosetsfunctions{\statespace \times \actionspace}{\reals},\\
\forall \state, \classifpolicy(\state) &=& \arg\max_{\action \in \actionspace} \classifscorefunc(\state,\action).
\end{eqnarray}
Apprendre une bonne fonction de score permet donc de résoudre le problème de classification. L'approche proposée dans \citep{taskar2005learning} prend le parti d'une fonction de score paramétrée linéairement. Il va de soi que rien ne garantit qu'une fonction de score linéaire sur l'espace d'état-action soit en mesure de donner un bon classifieur (c'est-à-dire offrant une faible erreur de classification), une condition nécessaire est le recours à une fonction d'attribut choisie avec soin :
# ?s paramclassif '\omega' 'Vecteur de paramètres pour la classification'
# ?cs 1 transpose '{#1}^{T}' 'X^T' 'Transposée de la matrice ou du vecteur $X$'
\begin{equation}
q_{\paramclassif}(\state,\action) = \transpose{\paramclassif}\featurestateactionfunc(\state,\action).
\end{equation}
Ce que nous cherchons maintenant est donc un bon vecteur de paramètres $\paramclassif$. Une telle recherche serait vaine si les attributs choisis ne permettaient pas d'exprimer une bonne fonction de score.

# ?s featurestatefunc '\psi' 'Fonction d{\apos}attribut sur l{\apos}espace d{\apos}état'
# ?s dimpsi 'd_{\psi}' 'Dimension de l{\apos}espace d{\apos}attributs sur l{\apos}espace d{\apos}état'
# ?cs 1 card '\left|#1\right|' '|A|' 'Cardinal de l{\apos}ensemble A'
Si l'on dispose d'une fonction d'attribut $\featurestatefunc \in \twosetsfunctions{\statespace}{\reals^{\dimpsi}}$ sur l'espace d'état, une technique classique pour obtenir une fonction d'attribut sur l'espace d'état-action consiste à distribuer la représentation sur les différentes actions. D'un vecteur de dimension $\dimpsi$, l'on passe à un vecteur de dimension $\dimphi = \card{A}\dimpsi$ (où $\card{\cdot}$ dénote le cardinal d'un ensemble) en définissant :
\begin{equation}
\featurestateactionfunc(\state,\action) = \begin{pmatrix}
\indicatorfunc{\action=\action_1}\featurestatefunc(\state)\\
\vdots\\
\indicatorfunc{\action=\action_{\card{\actionspace}}}\featurestatefunc(\state)\\
\end{pmatrix}.
\end{equation}

Le choix d'une bonne fonction d'attributs sur l'espace d'état est extrêmement dépendant du problème, néanmoins dans de nombreux cas deux techniques simples donnent de bons résultats. Dans le cas d'un espace d'état fini de taille raisonnable, il est possible de définir une fonction d'attribut binaire en associant un unique indice à chaque état. Le vecteur d'attribut d'un état est nul partout sauf en l'indice associé à l'état :
\begin{equation}
\featurestatefunc(\state) = \begin{pmatrix}
\indicatorfunc{\state=\state_1}\\
\vdots\\
\indicatorfunc{\state=\state_{\card{\statespace}}}\\
\end{pmatrix}.
\end{equation}
Un avantage de ce schéma est qu'il permet une représentation exacte de la fonction de score. En effet le produit $q_{\paramclassif}(\state,\action) = \transpose{\paramclassif}\featurestateactionfunc(\state,\action)$ revient à isoler la composante de $\paramclassif$ correspondant à l'unique indice associé au couple $(\state, \action)$. Les deux gros désavantages sont l'incapacité de ce schéma à passer à l'échelle et l'absence totale de structure : l'on aura beau disposer d'énormément d'information sur les "voisins" d'un élément de l'espace, tant que l'on aura pas vu précisément cet élément dans la base d'exemple, c'est la valeur par défaut de la coordonnée correspondante dans $\paramclassif$ qui sera utilisée.

# ?cs 1 gaussperdim 'g_{#1}' 'g_i' 'Nombre de gaussiennes pour la dimension $i$ dans un vecteur d{\apos}attribut basé sur un réseau de gaussiennes'
# ?s dimstate 'd_{\mathcal{S}}' 'Dimension de l{\apos}espace d{\apos}état'
# ?s gaussiancenter 'm' 'Centre d{\apos}une gaussienne'
# ?s gaussianvar '\sigma' 'Variance d{\apos}une gaussienne'
# ?cs 3 gaussian '\mathfrak{G}^{#1}_{#2}(#3)' '\mathfrak{G}^{m}_{\sigma}' 'Fonction gaussienne de centre $m$ et de variance $\sigma$'
# ?cs 2 component '{#1}^{#2}' 'X^j' 'Composante $j$ du vecteur $X$'
# ?s dimindex 'j' 'Entier indexant les dimensions d{\apos}un espace'
# ?s dimindexbis 'k' 'Entier indexant les dimensions d{\apos}un espace'
# FIXME: Vérifier sigma et G lorsque j'écrirai le code permettant de dessiner les features
Pour les espaces continus, une paramétrisation usuelle consiste à paver l'espace de gaussiennes. L'on assigne un nombre $\gaussperdim{\dimindex}$ à chacune des dimensions $0 < \dimindex \leq \dimstate$ de l'espace d'état et l'on construit un maillage de $\dimpsi = \prod_{\dimindex=1}^{\dimstate}\gaussperdim{\dimindex}$ points $m_{\dimindexbis}, 0<\dimindexbis\leq\dimpsi$ répartis à équidistance dans l'espace qui seront les centres des $\dimpsi$ composantes gaussiennes de la fonction d'attribut. La variance pour une dimension $\dimindex$ peut être choisie par exemple comme (avec $\component{\state}{\dimindex}$ la $\dimindex$-ième composante de $\state$) :
\begin{equation}
\gaussianvar^{\dimindex} = { \max(\component{\state}{\dimindex})-\min(\component{\state}{\dimindex})\over 2 \gaussperdim{\dimindex}}.
\end{equation}
En notant :
\begin{equation}
\gaussian{\gaussiancenter}{\gaussianvar}{s} = \exp\left(-\sum_{\dimindex=1}^{\dimstate}{(\component{s}{\dimindex}-\component{\gaussiancenter}{\dimindex})^2\over 2(\component{\gaussianvar}{\dimindex}})^2\right),
\end{equation}
on obtient finalement la fonction d'attribut suivante après l'ajout d'une composante constante :
\begin{equation}
\featurestatefunc(s) = \begin{pmatrix}
\gaussian{\gaussiancenter_{1}}{\sigma}{s}\\
\vdots\\
\gaussian{\gaussiancenter_{\dimpsi}}{\sigma}{s}\\
1
\end{pmatrix}.
\end{equation}
Contrairement à la fonction d'attribut binaire précédente, celle-ci possède une structure spatiale. Les scores de deux états topologiquement proches subiront l'influence de la même composante du vecteur de paramètre. Quand le nombre de dimensions augmente, le nombre de gaussiennes du réseau explose. Cette technique n'échappe donc pas à la malédiction de la dimension. Une illustration de ce type d'attributs sur l'espace d'état bidimensionnel du \gls{mountaincar} (décrit en \autoref{hier-annonceplan}) est présentée \autoref{fig:mountain_car_psi}.
\begin{figure}
\begin{tikzpicture}[scale=1.75]
%\draw [help lines] (0,0) grid (6,-6);
\node at (0,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_6x0}};
\node at (0,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_5x0}};
\node at (0,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_4x0}};
\node at (0,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_3x0}};
\node at (0,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_2x0}};
\node at (0,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_1x0}};
\node at (0,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_0x0}};
					  
\node at (1,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_6x1}};
\node at (1,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_5x1}};
\node at (1,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_4x1}};
\node at (1,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_3x1}};
\node at (1,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_2x1}};
\node at (1,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_1x1}};
\node at (1,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_0x1}};
					  
\node at (2,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_6x2}};
\node at (2,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_5x2}};
\node at (2,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_4x2}};
\node at (2,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_3x2}};
\node at (2,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_2x2}};
\node at (2,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_1x2}};
\node at (2,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_0x2}};
					  
\node at (3,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_6x3}};
\node at (3,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_5x3}};
\node at (3,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_4x3}};
\node at (3,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_3x3}};
\node at (3,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_2x3}};
\node at (3,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_1x3}};
\node at (3,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_0x3}};
					  
\node at (4,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_6x4}};
\node at (4,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_5x4}};
\node at (4,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_4x4}};
\node at (4,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_3x4}};
\node at (4,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_2x4}};
\node at (4,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_1x4}};
\node at (4,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_0x4}};
					  
\node at (5,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_6x5}};
\node at (5,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_5x5}};
\node at (5,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_4x5}};
\node at (5,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_3x5}};
\node at (5,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_2x5}};
\node at (5,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_1x5}};
\node at (5,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_0x5}};
					  
\node at (6,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_6x6}};
\node at (6,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_5x6}};
\node at (6,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_4x6}};
\node at (6,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_3x6}};
\node at (6,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_2x6}};
\node at (6,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_1x6}};
\node at (6,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_psi_0x6}};
\end{tikzpicture}
\caption{Attributs gaussiens sur le problème du \gls{mountaincar}. L'espace d'état est pavé de $7\times 7 = 49$ gaussiennes dont les centres sont répartis à équidistance. Les variances sont les mêmes pour toutes les gaussiennes et dépendent de la plage de valeur sur une dimension. Toute fonction aux variations raisonnables peut être approximée de manière correcte par une somme pondérée de ces gaussiennes. Cette représentation est à comparer à la \autoref{fig:mountain_car_mu} où l'attribut moyen qui, lui, prend la dynamique temporelle du problème en compte, est présenté.}
\label{fig:mountain_car_psi}
\end{figure}

# ?s slack '\zeta' 'Variable d{\apos}ajustement'
Considérant maintenant que nous disposons d'un vecteur d'attribut permettant de continuer, examinons le problème \citet{taskar2005learning} se proposent de résoudre. Il s'agit d'un problème d'optimisation convexe sous contraintes :
\begin{eqnarray}
&\min\limits_{\slack \in \reals_{+}} {1\over \nbsamples}\sum\limits_{\datasetindex=1}^{\nbsamples}\slack_{\datasetindex}\\
&\forall \datasetindex, \classifscorefunc_{\paramclassif}(\state_{\datasetindex},\action_{\datasetindex}) \geq \max\limits_{\action \in \actionspace}(\classifscorefunc_{\paramclassif}(\state_{\datasetindex},\action) + \margin(\state_{\datasetindex},\action)) - \slack_{\datasetindex}.
\end{eqnarray}
La fonction objectif cherche naturellement à réduire les variables d'ajustement $\slack_{\datasetindex}$ tandis que les contraintes sont telles que le score associé par la fonction de score $\classifscorefunc_{\paramclassif}$ à un couple $(\state_{\datasetindex}, \action_{\datasetindex})$ correspondant à une décision experte est supérieur ou égal au meilleur score. Et non pas juste supérieur au score seul, mais supérieur d'une certaine marge $\margin(\state_{\datasetindex},\action)$ qui donne à ce classifieur sa capacité de généralisation. En effet, l'on constate que si $\margin$ est uniformément nulle, alors parvenir à minimiser parfaitement la fonction de coût revient à apprendre par cœur la base d'exemple, c'est à dire à probablement subir les effets du sur-apprentissage. Fixer 
\begin{equation}
\margin(\state_{\datasetindex},\action) = \begin{cases}
0 &\textrm{si }\action = \action_{\datasetindex}\\
1 &\textrm{si }\action \neq \action_{\datasetindex}
\end{cases}
\end{equation}
permet de donner aux choix de l'expert un score strictement supérieur aux scores des autres choix. \citet{taskar2005learning} précisent qu'il est possible d'adapter la marge $\margin$ en fonction de la qualité des choix alternatifs, un bon choix correspondant à une petite marge. Nous verrons qu'en pratique la marge binaire que nous venons de suggérer fonctionne assez bien.

Résoudre ce problème n'est pas évident en l'état, il est possible d'en obtenir une formulation plus simple (qui revient à celle employée dans \citep{ratliff2007imitation}) en constatant que lorsque les variables d'ajustement sont minimisées, elles sont égales aux violations des contraintes :
\begin{equation}
\slack_{\datasetindex} =  \max\limits_{\action \in \actionspace}(\classifscorefunc_{\paramclassif}(\state_{\datasetindex},\action) + \margin(\state_{\datasetindex},\action)) - \classifscorefunc_{\paramclassif}(\state_{\datasetindex},\action_{\datasetindex}).
\end{equation}
En posant :
\begin{eqnarray}
\best{\action}_{\datasetindex} &=& \arg\max_{\action \in \actionspace} \classifscorefunc_{\paramclassif}(\state_{\datasetindex},\action) + \margin(\state_{\datasetindex},\action)\\
 &=& \arg\max_{\action \in \actionspace} \transpose{\paramclassif}\featurestateactionfunc(\state_{\datasetindex},\action) + \margin(\state_{\datasetindex},\action),
 \end{eqnarray}
et en faisant monter les contraintes dans la fonction objectif, on obtient alors une simple fonction de coût à minimiser :
# ?cs 1 best '{#1}^{*}' 'x^*' 'Element issu d{\apos}un $\arg\max_x$'
# ?s margin '\mathfrak{l}' 'Fonction de marge dans le classifieur à marge'
# ?s structuredcost 'J' 'Fonction de coût de la classification structurée'
\begin{eqnarray}
\structuredcost(\paramclassif) &=& {1\over \nbsamples} \sum_{\datasetindex=1}^{\nbsamples} \classifscorefunc_{\paramclassif}(\state_{\datasetindex},\best{\action}_{\datasetindex}) + \margin(\state_{\datasetindex},\best{\action}_{\datasetindex}) - \classifscorefunc_{\paramclassif}(\state_{\datasetindex},\action_{\datasetindex})\\
 &=& {1\over \nbsamples} \sum_{\datasetindex=1}^{\nbsamples} \transpose{\paramclassif}\featurestateactionfunc(\state_{\datasetindex},\best{\action}_{\datasetindex}) + \margin(\state_{\datasetindex},\best{\action}_{\datasetindex}) - \transpose{\paramclassif}\featurestateactionfunc(\state_{\datasetindex},\action_{\datasetindex}).\\
\end{eqnarray}
Cette fonction n'est pas différentiable à cause de l'opérateur non linéaire $\max$ caché dans le terme $\best{\action}_{\datasetindex}$, mais la généralisation du gradient qu'est le sous-gradient permet de contourner cette difficulté.
# ?s subgrad '\nabla' 'Sous gradient d{\apos}une fonction'
Le sous gradient de la fonction de coût est :
\begin{equation}
\subgrad\structuredcost(\paramclassif) = \sum_{\datasetindex=1}^{N}\featurestateactionfunc(\state_{\datasetindex},\best{\action}_{\datasetindex}) - \featurestateactionfunc(\state_{\datasetindex},\action_{\datasetindex}),
\end{equation}
il est donc possible de résoudre le problème d'optimisation original en effectuant une simple descente de sous-gradient pour minimiser la fonction de coût $J(\paramclassif)$.
Nous avons présenté cette technique de classification plus en détail à des fins d'illustration de l'importance du choix des attributs, et également car nous la retrouverons plus loin dans le manuscrit lorsque nous nous intéresserons aux techniques d'\gls{irl}.

Il existe bien d'autres moyen de faire de la classification, comme par exemple les \gls{svm}.
*** \glsentrytext{svm}
# FIXME: Ecrire la sous partie sur les SVMs
# Goal: Faire une description rapide des MCSVM et de leurs avantages.
# Goal: Introduire la notion de noyau, de kenel-trick et faire le lien avec les attributs (poil au cul)
# Requires: attribut, classification
# Ensures noyau, kernel-trick, SVM
- Citer la généralisation de Yann.
- Parler du kernel-trick, faire le lien avec les attributs.
- Illustration \gls{mountaincar} : montrer les attributs choisis automatiquement par l'implémentation de la SVM choisie
Nous aurons également à faire avec des \gls{svm} dans la suite du manuscrit. Bien d'autres techniques de classification existent, une description exhaustive des techniques d'apprentissage supervisé propices à l'imitation dépasserait le cadre de cette thèse. \citet{hastie2005elements,vapnik1998statistical} sont des ouvrages de référence. Nous allons voir comment ces techniques ont été appliquées en pratique à l'apprentissage supervisé de la politique de l'expert.
*** Imitation par apprentissage supervisé de la politique
# <<hier:myopie>>
# Goal:Faire l'état de l'art des techniques d'imitation par apprentissage supervisé
# Goal:Ce serait bien qu'on ressente le besoin des notions du \gls{mdp}, notamment la récompense (bis)
# Requires:Classifieur, attributs, classif de taskar, (boosting?)
# Ensures: Boosting, ratliff2007imitation, 
Apprendre la politique de l'expert de manière supervisée à l'aide d'une base d'exemples peut s'avérer efficace, comme le démontrent plusieurs approches. Dans \citep{ratliff2007imitation}, les auteurs utilisent le classifieur à marge décrit plus haut
# checkref
# ?s featurestateactionhypothesisspace '\mathcal{H}_{\Phi}' 'Espace d{\apos}hypothèse où choisir une nouvelle composante pour l{\apos}attribut état action'
# ?cs 2 scalarprod '\left\langle\left.{#1}\right | {#2}\right\rangle' '\langle x|y\rangle' 'Produit scalaire de $x$ et $y$'
pour apprendre une politique experte sur un problème de locomotion quadrupède et sur un problème de manipulation d'objets. Le choix des attributs est automatisé grâce à une technique de \gls{boosting} similaire à \cite{friedman2001greedy,mason1999functional} : on choisit dans un espace d'hypothèses la fonction qui, ajoutée comme composante supplémentaire de la fonction d'attribut, aiderait le mieux à minimiser la fonction de coût. Mathématiquement, on choisit la fonction dont le produit scalaire avec le gradient fonctionnel de la fonction de coût est minimal : la nouvelle composante $\featurestateactionfunc_{\dimphi +1}$ est choisie dans l'espace d'hypothèse $\featurestateactionhypothesisspace$ selon 
\begin{equation}
\featurestateactionfunc_{\dimphi +1} = \arg\max_{\featurestateactionfunc\in \featurestateactionhypothesisspace} \scalarprod{\featurestateactionfunc}{-\subgrad \structuredcost}.
\end{equation}

Le boosting permet de déplacer de manière intelligente le problème du choix des attributs, sans le régler totalement. Il reste en effet à construire l'espace d'hypothèse $\featurestateactionhypothesisspace$ où choisir les nouveaux attributs. Un espace trop simple ne permettrait pas de minimiser efficacement la fonction de coût, tandis qu'un espace trop centré sur les données permettrait de la minimiser totalement, mais sans doute au prix d'un sur-apprentissage aux conséquences fâcheuses. C'est donc cet espace qui doit être calibré et construit afin de donner au classifieur ses capacités de généralisation. \citet{ratliff2007imitation} proposent d'utiliser un réseau de neurones.

Plus brutale, l'approche de \citet{lecun2006off} utilise un \gls{cnn} (réseau de convolution) à 6 couches pour apprendre une association directe entre une image (stéréo) d'entrée et un angle de braquage (la tâche à apprendre est la conduite d'un véhicule en terrain libre). Le problème de la généralisation est résolu en exigeant une base d'entraînement couvrant au maximum l'espace d'état. Les auteurs ne cachent pas la difficulté de constituer une telle base qui doit réunir des conditions de terrain et d'illumination variées tout en exigeant un comportement extrêmement cohérent et prédictible de la part de l'opérateur humain et ce sur un grand nombre de trajectoires (il faut réunir près d'une centaine de milliers d'échantillons). En contrepartie de ces efforts, la technique proposée est robuste et ne nécessite aucun travail d'ingénierie au niveau des attributs, puisque la politique apprise associe directement la sortie du capteur à la consigne de l'actuateur du robot. Bien que cela soit moins problématique aujourd'hui avec l'augmentation de puissance des équipements embarqués, elle semble également plus rapide (dans l'exploitation, non dans l'apprentissage) que l'état de l'art de l'époque. Elle améliore les résultats notamment par rapport à \gls{alvinn} \citep{pomerleau1993knowledge} en ceci que la résolution des caméras peut être augmentée sans trop grande explosion du réseau grâce à l'usage de la convolution et non d'un réseau complètement connecté, et que la tâche apprise est plus difficile, il s'agit de conduire en terrain libre et non de suivre une route.

Nous venons de voir deux techniques différentes (\gls{boosting} et réseau de convolution) permettant d'apprendre une politique à partir d'un base de données \gls{batch}, de manière supervisée, avec une intervention humaine minimale : soit l'on dispose de suffisamment de données pour que le risque empirique soit proche du risque réel, soit l'on construit des attributs (ou si l'on utilise du \gls{boosting} un espace d'hypothèse où les choisir) tels que l'apprentissage au mieux (en minimisant une fonction de coût exprimée sur les données) ne soit pas un apprentissage par cœur, mais un apprentissage généralisant sur tout l'espace d'état. Apprendre une politique de manière locale, c'est à dire en se concentrant trop sur une base de données lacunaire, n'est pas satisfaisant. Cela donne un résultat fragile, l'agent sera en effet pris au dépourvu s'il a à contrôler le système dans une configuration différente de celles sur lesquelles il a été entraîné : il ne dispose ni d'information relative au comportement de l'expert dans une telle situation, ni d'information sur la tâche à accomplir qui lui permettraient de déduire ce que pourrait être ce comportement.

Brisant la contrainte de la base de données inerte, l'idée de demander ces échantillons de manière interactive a été proposée afin de minimiser la quantité de données nécessaire à l'apprentissage de la politique experte. Un exemple d'une telle approche est décrit par \citet{chernova2007confidence}. Des \gls{gmm} sont appris à partir d'une base de données experte de départ, puis l'agent applique la politique apprise tout en demandant à l'expert, lorsque l'incertitude est trop grande, de lui fournir un échantillon supplémentaire. Cette approche permet de limiter la redondance de la base d'entraînement et de guider l'échantillonnage vers les zones intéressantes de l'espace d'état, ce qui est également une solution au problème de la généralisation : quand l'agent ne sait pas généraliser, il demande à l'expert. Cela est connu sous le nom d'apprentissage actif.

L'apprentissage direct de la politique experte est parfois intégré à dans un cadre plus large, où les notions de hiérarchie et de but apparaissent.

La classification par arbre de décision a été appliquée à l'apprentissage d'un plan de vol par \citet{sammut1992learning}. L'application est impressionnante ; piloter un avion, même en simulation, n'est pas une mince affaire puisqu'il faut en temps réel prendre en compte un grand nombre de facteurs pour décider d'une action parmi un éventail assez large. L'apprentissage automatique nécessite un grand nombre d'échantillons. Un comportement cohérent est exigé de l'expert humain (à un point tel que les démonstrations de deux experts ne peuvent être mélangées en une seule base d'entraînement). L'aspect automatique de l'approche est limité à l'apprentissage d'une politique par phase de vol. La détection de la phase de vol courante et donc le choix de la politique de contrôle à appliquer est effectué par des règles d'origine humaine.

De fait, cette approche a été le point de départ de nombreuses améliorations. Le travail présenté par \citet{stirling1995churps} (appelé \gls{churps}) consiste à déduire un contrôleur à partir d'une description du modèle d'évolution du système et du but à atteindre. Pour automatiser la création de ces descriptions, tâche réclamant un travail difficile car nécessitant de décrire des mécanismes précis à l'aide d'un langage contraignant, \citet{bain2000framework} proposent d'utiliser les données de l'expert. Les règles complexes ainsi apprises étant ajoutées à l'espace d'action, il est possible d'apprendre de manière automatique un classifieur plus concis que celui de \citet{sammut1992learning}, et nécessitant moins de données expertes. L'architecture proposée utilise la logique du premier ordre et donc le raisonnement symbolique. Cela permet d'introduire explicitement des connaissances expertes dans le système. Ces connaissances peuvent être acquises semi-automatiquement : les prédicats sont bâtis à la main et les paramètres sont appris grâce aux données de vol comme le proposent \citet{srinivasan1998inductive}. La sémantique des symboles (ici, virage, altitude, trajectoire de vol, etc.) est très liée au problème concerné. Retrouver la puissance des techniques d'apprentissage symbolique sur un autre problème nécessite d'effectuer de nouveau le difficile travail de définition des symboles et prédicats. Un autre élément potentiellement rédhibitoire est la difficulté d'exprimer la tâche à accomplir en utilisant un langage symbolique. Dans une approche hybride symbolique/automatique, \citet{shiraz1997combining} proposent à l'expert soit de décrire la tâche symboliquement, soit d'en démontrer l'exécution. Les phases les plus délicates (par exemple l'atterrissage) n'ont pu être décrites et ont été démontrées. La facilité d'exploitation des règles symboliques rentre en conflit avec la difficulté qu'il y a à les définir, à l'inverse la relative facilité de génération d'une base d'exemples se heurte à la difficulté qu'il y a à généraliser à partir de celle-ci.

Une autre approche utilise les notions de hiérarchie et de but, mais de manière quelque peu différente. Plutôt que d'utiliser la logique des prédicats, ce sont les principes de programmation impérative qui se voient assistés par l'apprentissage supervisé. Dans \cite{saunders2006teaching}, ce sont les \gls{knn} qui sont utilisés pour l'apprentissage supervisé d'une politique. Les attributs sont construits à la main à partir des valeurs de sortie des capteurs du robot, et portent une sémantique forte et explicite (distance, angle), donc pratique pour l'exploitation par un opérateur humain. Les problèmes de généralisation de l'apprentissage supervisé sont contournés par l'intégration dans un cadre beaucoup plus riche : l'opérateur humain peut élargir l'espace d'action à volonté, soit en définissant une séquence d'actions qui seront exécutées en série de manière déterministe, soit en proposant des exemples du comportement souhaité en précisant ou non un état-but correspondant à la situation dans laquelle on souhaite voir le robot une fois la politique exécutée. Ces exemples servent alors à l'apprentissage d'une politique de manière supervisée, cette politique est ajoutée en tant qu'action et son exécution pourra être déclenchée dans le cadre d'une autre politique, de niveau d'abstraction plus grand. Cette hiérarchisation des comportements permet de limiter l'effort humain, d'optimiser l'utilisation des exemples et de rapidement mettre en place des comportements complexes par la création de nouveaux niveaux d'abstraction. 

L'apprentissage supervisé est dans les approches que nous venons de citer utilisé comme sous routine d'un système beaucoup plus large, dans lequel l'expertise humaine explicite reste le moyen central permettant la généralisation des comportements.

Le principal problème de l'apprentissage direct de la politique de manière supervisé est, pour reprendre le terme de \citep{ratliff2009learning}, sa myopie. Pour compenser le fait que l'on travaille au niveau d'abstraction le plus bas, celui du choix immédiat d'une action en fonction des informations contenues dans un état transitoire, les approches que nous venons de détailler font apparaître en filigrane la notion de but : l'expert n'agit en effet pas à tâtons mais dirige le système en fonction de critères qu'il paraît difficile d'exprimer au niveau d'une simple politique. On se repose donc sur une formulation plus ou moins explicite (dans le choix des attributs, dans la définition de la base d'exemples, dans l'introduction de règles logiques ou dans la définition d'une hiérarchie) de ce but, mais toujours d'origine humaine. Nous allons voir qu'il est possible de formaliser cette notion de but tout en continuant de travailler avec une politique et des échantillons semblables à ceux auxquels nous nous sommes habitués. Nous verrons par la suite que le but de l'expert, formalisé de cette manière, peut alors être automatiquement déduit d'une base d'échantillons inerte. 
# Méthode de regroupement des actions : on apprend plus une politique en la copiant mais on essaie de comprendre comment fonctionne l'expert.
# ?? Moultes autres approches, labyrinthiques, exemples ultra rapide, se référer à blip et blop pour un survey
# ^(saunders2006teaching) citation [22] semble en proposer un survey. (saunders2006teaching) en propose lui-même un bon
# L'idée est bonne, mais (problèmes liés à l'approche). Ce qu'il faudrait c'est comprendre le but de l'expert, et essayer d'isoler ça.
# FIXME: La notion de but apparâit plusieurs fois
# Trucs que je sais pas où foutre :
# saunders2006learning, sec 3 : si on observe l'expert, on a pas accès à ses sensations ni à ses ordres directement, et ils correspondent pas à ceux de l'expert. Quoiqu'en changeant l'espace d'action (tour, fou etc.) , on devrait y arriver.  #correspondance problem
# 
# Trucs que j'ai pas lu, mais qu'il faudrait peut-être lire et mettre dans ce chapitre ou ailleurs, mais dont j'espère qu'ils sont de moindre importance et que donc c'est pas grave si je n'en parle pas
# (argall2009survey) T. Inamura, M. Inaba, H. Inoue, Acquisition of probabilistic behavior decision model based on the interactive teaching method, in: Proceedings of the Ninth International Conference on Advanced Robotics, ICAR’99, 1999.
# En fait toute la section 4.1 de argall2009survey mériterait d'être explorée ici, mais c'est long et chiant et il se fait tard.
# Faudrait aussi se faner schaal et son gros survey, mais c'est vraiment mal écrit, et je pense pas que je jeu en vaille la chandelle. Il faudrait penser à le citer, cependant.
# Ya bentivegna2004learning qui sert à rien mais qu'on peut rajouter si ya besoin de parler pour ne rien dire (problème dépendant)
# ya coates2008learning qui est impressionnant mais qui rentre dans aucune case
# ya  konidaris2011cst que je sais pas où foutre
# ya  leon2011teaching que je sais pas ou ranger non plus
# Quelque part il faudrait rajouter  montana2011towards
# Et natarajan2011imitation, c'est du supervisé, ou pas ?
# J'ai l'impression de m'embarquer dans un labyrinthe sans fin, avec toujours plus de papiers à résumer. Il est impossible d'être exhaustif en si peu de temps.
# FIXME Citer les deux surveys 


** Cadre des \glsentrytext{mdp} pour la prise de décision séquentielle
# <<hier:cadre>>
# Snippet: La classification ne se soucie pas de l'objectif de l'expert. Quid si une action mal choisie fait dérailler l'agent sur une partie totalement inconnue de l'espace d'état ?
# Goal: introduire les notions de l'AR qui sont nécessaires à la définition des notions d'ARI
# Requires:
# Ensures: trajectoire, probabilités de transition, trace sas, récompense, trace sars, trace sarsa, fonction de valeur, fonction de qualité, politique gloutonne, politique optimale, itération de la politique, itération de la valeur, dynamic programming, RL, LSPI
Pour comprendre ce but de l'expert qu'il nous importe de connaître, ce n'est pas au niveau du choix état-action que décrit la politique qu'il faut regarder, mais à un niveau d'abstraction plus grand : la dynamique que la politique de l'expert impose au système. La notion qui nous manque pour entamer le raisonnement est celle de l'effet d'une action. Nous ne nous sommes préoccupés que du choix de l'action en fonction de l'état courant sans nous soucier de ce que ce choix allait imposer comme contraintes sur le prochain état que l'agent va rencontrer. Afin de pouvoir considérer la politique de l'expert non plus comme un ensemble décousu d'associations état-action, mais comme un outil capable de produire des séquences d'actions porteuses de sens au point de vue d'un critère long terme, nous formalisons la notion de dynamique temporelle.

# ?s timeindex 't' 'Indice temporel'
# ?s timehorizon 'T' 'Horizon temporel'
# ?s naturals '\mathbb{N}' 'Entiers naturels'
L'agent (qu'il s'agisse de l'expert ou d'un agent artificiel que l'on entraîne) au manettes du système contrôle celui-ci non pas ponctuellement de temps à autres mais de manière cohérente sur un laps de temps durant lequel il devra opérer des actions de contrôle les unes après les autres. Il est donc naturel d'indexer ces actions et les états traversés par un indice temporel $\timeindex \in \llbracket 0;\infty\rrbracket$. Il est possible de prendre en charge une vision continue du temps avec quelques subtilités dont nous ne soucions pas ici, cette formalisation discrète étant suffisamment puissante pour les cas que nous souhaitons traiter. Elle n'impose par exemple pas de pas d'échantillonnage constant, il s'agit ici d'ordonner les états et actions par ordre de causalité, ce qui incidemment correspond[fn::\`A moins que /Doctor Who/ et /Retour vers le futur/ ne soient des documentaires.] à un indice temporel croissant, non pas de transcrire avec quelque fidélité les problèmes de l'échantillonnage temporel. Qui plus est cette formulation correspond à la réalité du contrôle numérique, intrinsèquement discret.

# ?s transprobfunc 'p' 'Probabilités de transition'
# ?cs 3 transprobfunceval 'p\left(#3|#1,#2\right)' 'p(s\prime|s,a)' 'Probabilité qu{\apos}un agent transite en $s\prime$ après avoir choisi l{\apos}action $a$ dans l{\apos}état $s$'
Pour prendre en compte les imperfections de la modélisation ou plus simplement parfois la nature réellement stochastique du problème, les effets d'une action sont décrits par une loi de probabilité, qui, informée d'un état $\state_{\timeindex}$ et d'une action $\action_{\timeindex}$, prédit vers quel état $\state_{\timeindex+1}$ le système va transiter. On note cela (encore une fois en identifiant loi de probabilité et densité de probabilité) :
\begin{equation}
\state_{\timeindex+1}\sim \transprobfunceval{\state_{\timeindex}}{\action_{\timeindex}}{\cdot}, \transprobfunc \in \twosetsfunctions{\statespace \times \actionspace \times \statespace}{[0;1]}.
\end{equation}
On constate que l'information sur l'état vers lequel on transite ne dépend que de l'état courant et de l'action courante, et non de la trajectoire. C'est la propriété de Markov qui donne son nom au \gls{mdp}.
# ?cs 1 transprobmat 'P^{#1}' 'P^{\pi}' 'Matrice des probabilités de transition induites par la politique $\policy$'
# ?cs 3 matrixbyterm '\left({#1}\right)^{#2}_{#3}' '\left(f(i,j)\right)^{i}_{j}' 'Matrice dont l{\apos}élément ligne $i$, colonne $j$ est $f(i,j)$'
# ?cs 1 stationarydistrib '\rho^{#1}' '\rho^\pi' 'Distribution stationnaire induite par la politique $\pi$'
La répétition du cycle consistant à choisir une action puis à transiter vers un nouvel état où l'agent choisit une action qui le fera transiter vers un nouvel état (etc.) forme une trajectoire. Les probabilités de transitions contraintes par une politique $\policy$ peuvent être dans le cas fini $\card{\statespace}<\infty$ représentées par une matrice de taille $\card{\statespace}\times\card{\statespace}$ :
\begin{equation}
\transprobmat{\policy} = \matrixbyterm{\transprobfunceval{\state}{\policy(\state)}{\state'}}{\state}{\state'},
\end{equation}
où une matrice est définie par son terme général : $\left(f(i,j)\right)^{i}_{j}$ est la matrice dont le terme à la ligne $i$ et à la colonne $j$ est $f(i,j)$.
Un agent de politique $\policy$ va visiter certains états plus que d'autres. Pour quantifier cela, il est possible d'utiliser la matrice des probabilités de transition que nous venons juste de définir. Le terme à la ligne $\state$ et à la colonne $\state'$ est la probabilité $\transprobfunceval{\state}{\policy(\state)}{\state'}$ que l'agent se trouve dans l'état $\state'$ au temps $\timeindex+1$ s'il était en $\state$ au temps $\timeindex$. Si l'on multiplie la matrice $\transprobmat{\policy}$ par elle même, le terme général (ligne $\state$, colonne $\state''$) du résultat est :
\begin{equation}
\sum_{\state'\in\statespace} \transprobfunceval{\state}{\policy(\state)}{\state'} \transprobfunceval{\state'}{\policy(\state')}{\state''},
\end{equation}
il s'agit de la probabilité pour l'agent de passer de $\state$ à $\state''$ en deux pas de temps. Si (et seulement si) l'agent peut espérer se trouver en chacun des états en temps fini, alors ce que l'on appelle la distribution stationnaire $\stationarydistrib{\policy}$ existe et est unique. Elle est définie telle que :
\begin{equation}
\transpose{\stationarydistrib{\policy}} \transprobmat{\policy} = \transpose{\stationarydistrib{\policy}}.
\end{equation}
On peut donc la voir comme la probabilité pour l'agent de se trouver en un certain état, après un temps infini, quel que soit l'état de départ. La condition d'ergodicité que nous plaçons généralement sur le \gls{mdp} contraint par la politique de l'expert implique l'existence de la distribution stationnaire de l'expert $\expertdistrib$. Un \gls{mdp} contraint par une politique correspond à une chaine de Markov \citep{norris1998markov}.

# ?s rewardfunc 'R' 'Fonction de récompense'
# ?cs 1 staterewardfunceval 'R\left(#1\right)' 'R(s)' 'Récompense en l{\apos}état $s$'
Dans les approches vues précédemment, le but était défini comme des valeurs spécifiques que doivent prendre certaines composantes de l'état (par exemple pour le pilotage, une certaine altitude). Il est au premier abord assez naturel de définir une consigne comme cela. Pour peu que l'espace d'état soit construit d'une manière qui permet l'analyse sémantique, l'opérateur humain n'a pas trop de mal à exprimer ce qu'il souhaite que la machine fasse en définissant quels sont les états désirables et ceux qu'il faut éviter. Charge à la machine de trouver comment se placer dans les états désirables en évitant les états problématiques. Nous formalisons cela sous la forme d'une fonction de récompense. Il s'agit d'un jugement local de l'intérêt qu'il y a à se trouver en un certain état :
\begin{equation}
\rewardfunc \in \twosetsfunctions{\statespace}{\reals}.
\end{equation}
Nous verrons par la suite qu'il est possible, sans que cela importe réellement, de définir la récompense sur les états seuls comme nous le faisons pour l'instant, ou sur des couple état-action, ou même sur des transitions état-action-état.

# ?cs 2 rlvalue 'V^{#1}_{#2}' 'V^{\pi}_R' 'Fonction de valeur pour la récompense $R$ lorsqu{\apos}on suit la politique $\pi$'
Il faut maintenant que ce critère local donne lieu à un comportement globalement intéressant. Comment, à l'échelle d'une politique choisissant une action pour un état, parvenir à un contrôle tenant compte de la dynamique complète du système ? Il faut qu'une politique $\policy$ soit jugée dans son ensemble sur la trajectoire qu'elle impose à l'agent. Mathématiquement nous souhaitons optimiser la valeur de la politique :
\begin{eqnarray}
\label{eq:Vdefsum}
\rlvalue{\policy}{\rewardfunc}(\state)&=&\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\staterewardfunceval{\state_{\timeindex}}}{\state_0 = s}\\
\textrm{avec }\forall \timeindex \in \llbracket 1;T\rrbracket, \state_{\timeindex} &\sim& \transprobfunceval{\state_{\timeindex-1}}{\policy(\state_{\timeindex-1})}{\cdot}.
\end{eqnarray}
# ?s discount '\gamma' 'Facteur d{\apos}amortissement'
Comme l'horizon temporel est infini, pour s'assurer de la convergence de la somme, le facteur d'amortissement $\discount \in [0;1[$ est introduit. Une conséquence est la diminution d'attrait des récompenses loin dans le futur au profit de récompenses accessibles plus rapidement. Cela permet de récompenser l'agent qui effectue rapidement sa tâche.

La fonction de valeur, et un peu plus loin la fonction de qualité (\autoref{eq:qualityref}) sont paramétrées par la fonction de récompense $\rewardfunc$ car par la suite nous serons amenés à étudier la valeur selon une certaine récompense d'une politique qui est optimale pour une autre récompense.

# ?s mdpbis '\mathcal{M}' 'Un \gls{mdp}'
L'ensemble de l'espace d'état $\statespace$, de l'espace d'action $\actionspace$, des probabilités de transitions $\transprobfunc$, de la fonction de récompense $\rewardfunc$ et du facteur d'amortissement $\discount$ forment un \gls{mdp} $\mdpbis$ \citep{puterman1994markov}
\begin{equation}
\mdpbis = \left\{\statespace, \actionspace, \transprobfunc, \rewardfunc, \discount\right\}
\end{equation}
dans lequel le problème de la prise de décision séquentielle pour le contrôle optimal peut être formulé.

# ?cs 1 optimalpolicy '\pi^*_{#1}''\pi^*_R' 'Une politique optimale pour la fonction de récompense $R$'
Nous recherchons une politique optimale $\optimalpolicy{\rewardfunc}$ telle qu'en tout état sa valeur soit supérieure ou égale à celle de tout autre politique $\pi$ :
\begin{equation}
\forall \state, \rlvalue{\optimalpolicy{\rewardfunc}}{\rewardfunc}(\state) \geq \rlvalue{\policy}{\rewardfunc}(\state).
\label{eq:optimalite}
\end{equation}
Pour résoudre ce problème, intéressons nous de plus près à l'expression de la valeur d'une politique, dont la définition qu'on en a donné $\autoref{eq:Vdefsum}$ peut être transformée en une expression récursive (grâce à la propriété de Markov) :
\begin{equation}
\label{eq:BellmanEval}
\rlvalue{\policy}{\rewardfunc}(\state) = \staterewardfunceval{\state} + \discount \sum_{\state'\in \statespace}\transprobfunceval{\state}{\policy(\state)}{\state'} \rlvalue{\policy}{\rewardfunc}(\state').
\end{equation}
# ?cs 3 bellmanevalopeval 'B^{#1}_{#2}{#3}' 'B^{\pi}_{R}' 'Opérateur d{\apos}évaluation de Bellman'
C'est l'équation d'évaluation de \citet{bellman2003dynamic} qui est à l'origine de l'opérateur d'évaluation de Bellman :
\begin{eqnarray}
\bellmanevalopeval{\policy}{\rewardfunc}{} &\in& \twosetsfunctions{\twosetsfunctions{\statespace}{\reals}}{\twosetsfunctions{\statespace}{\reals}}\\
\forall V \in \twosetsfunctions{\statespace}{\reals}, \bellmanevalopeval{\policy}{\rewardfunc}{V} &=& R + \discount \sum_{\cdot\in \statespace}\transprobfunceval{\state}{\policy(\state)}{\cdot} V. 
\label{eq:bellmanevop}
\end{eqnarray}

Cette opérateur est contractant, par conséquence il possède un point fixe, qui est unique. L'équation de définition de ce point fixe :
\begin{equation}
V = \bellmanevalopeval{\policy}{\rewardfunc}{V}
\end{equation}
est exactement la même que l'équation d'évaluation de Bellman \autoref{eq:BellmanEval}. L'unique point fixe de l'opérateur $\bellmanevalopeval{\policy}{\rewardfunc}{}$ est donc la fonction de valeur de la politique : $\rlvalue{\policy}{\rewardfunc}$. La famille d'algorithmes dits d'itération de la politique appliquent d'ailleurs de manière répétée l'opérateur d'évaluation de Bellman (ou une version approchée de cet opérateur) à une valeur de départ pour obtenir la valeur d'une politique.

# ?cs 2 quality 'Q^{#1}_{#2}' 'Q^{\pi}_{R}' 'Fonction de qualité de la politique $\pi$ pour la récompense $R$'
Dans l'équation de Bellman \autoref{eq:BellmanEval}, l'action qui fait passer de $\state$ à $\state'$ est explicitement donnée comme étant $\policy(\state)$. Les actions suivantes sont également choisies par la politique $\policy$ comme l'indique le terme $\rlvalue{{\color{red}\policy}}{\rewardfunc}$. Imaginons maintenant que connaissant la valeur d'une politique $\policy$, nous soyons chargé pour l'état $\state$ de choisir la meilleur action $\action$, qui peut être différente de $\policy(\state)$, mais qu'ensuite la politique $\policy$ reprenne le contrôle. C'est le degré de liberté décrit par la fonction de qualité $\quality{\policy}{\rewardfunc}$ :
\begin{eqnarray}
\label{eq:qualitydef}
\quality{\policy}{\rewardfunc} &\in& \twosetsfunctions{\statespace \times \actionspace}{\reals}\\
\quality{\policy}{\rewardfunc}(\state,\action) &=& \staterewardfunceval{\state} + \discount \sum_{\state'\in \statespace}\transprobfunceval{\state}{\action}{\state'} \rlvalue{\policy}{\rewardfunc}(\state').
\end{eqnarray}
Notre meilleur choix consisterait à maximiser la fonction de qualité, c'est à dire à rendre le contrôle à $\policy$ dans l'état $\state'$ dans lequel sa valeur est maximale. En effectuant ce choix sur chacun des états de $\statespace$, l'on définit une politique gloutonne :
# ?cs 1 greedy 'g\left(#1\right)' 'g(\policy)' 'Politique gloutonne vis-à-vis de la fonction de qualité de $\pi$'
\begin{equation}
\greedy{\policy}: \state\rightarrow \arg\max_{\action\in\actionspace}\quality{\policy}{\rewardfunc}(\state,\action). 
\end{equation}
# ?cs 2 bellmanoptopeval 'B^{*}_{#1}{#2}' 'B^{*}_{R}' 'Opérateur d{\apos}optimalité de Bellman'
Cette agglomération de choix localement optimisés permet un optimisation beaucoup plus générale. La politique gloutonne que nous venons de définir est le meilleur choix pour un problème d'optimisation plus large :
\begin{equation}
\greedy{\policy} = \max_{\policy'}\bellmanevalopeval{\policy'}{\rewardfunc}{\rlvalue{\policy}{\rewardfunc}}.
\end{equation}
L'opérateur associé :
\begin{equation}
\bellmanoptopeval{\rewardfunc}{V} = \max_{\policy}\bellmanevalopeval{\policy}{\rewardfunc}{V}
\end{equation}
est l'opérateur d'optimalité de Bellman. Contractant lui aussi, il admet donc un unique point fixe qui se trouve être la fonction de valeur optimale $\rlvalue{\optimalpolicy{\rewardfunc}}{\rewardfunc}$. Toute politique gloutonne vis-à-vis de la politique optimale associée est également une politique optimale. Ainsi :
\begin{eqnarray}
\optimalpolicy{\rewardfunc} &\in& \greedy{\optimalpolicy{\rewardfunc}}\\
\forall \state \in \statespace, \optimalpolicy{\rewardfunc}(\state) &\in& \arg\max_{\action\in \actionspace}\quality{\optimalpolicy{\rewardfunc}}{\rewardfunc}(\state,\action).
\end{eqnarray}
Les algorithmes d'itération de la valeur \citep{bertsekas2001dynamic,sutton1998reinforcement} appliquent de manière répétée l'opérateur d'optimalité de Bellman (ou une version approchée) à une politique de départ afin d'accéder à la politique optimale.

Il est intéressant de noter que, grâce à la fonction de valeur, l'optimisation "myope" répétée à l'échelle du choix d'une action pour un état mène à une optimisation à l'échelle de l'espace d'état complet, au niveau de la politique. Grâce à la prise en compte des probabilités de transitions, la fonction de valeur fait le lien entre le court et le long terme.

Lorsque les probabilités de transitions sont connues sur un espace d'état fini, l'on peut de manière exacte résoudre le problème du contrôle optimal grâce à la \gls{dp} \citep{puterman1994markov}. Les choses se corsent lorsque ces probabilités de transition sont inconnues ou que les algorithmes de \gls{dp} deviennent non tractables (espace d'état trop grand, principalement). Il faut alors avoir recours à l'\gls{rl} \citep{sutton1998reinforcement}. L'\gls{rl} permet, par interactions répétées avec le système, d'apprendre à contrôler celui-ci. Les probabilités de transition étant souvent difficiles à exprimer, nous allons porter notre attention sur l'\gls{rl} plus particulièrement que sur la \gls{dp}.

# ?s paramrl '\xi' 'Vecteur de paramètres pour l{\apos}AR'
# ?cs 1 appr '\hat{#1}' '\hat X' 'Approximation de $X$ à partir de données'
Un schéma classique est celui de l'approximation linéaire de la fonction de valeur[fn::On utilise parfois le terme de "fonction de valeur"" indistinctement pour la fonction de valeur et la fonction de qualité.]. On réduit alors le problème du contrôle optimal au choix du vecteur de paramètres $\paramrl$ donnant la meilleure approximation de la fonction de qualité optimale :
\begin{equation}
\appr{\quality{\optimalpolicy{\rewardfunc}}{\rewardfunc}}(\state,\action) = \transpose{\paramrl}\featurestateactionfunc(\state, \action).
\label{eq:paramrl}
\end{equation}
Comme pour la classification, le choix de l'espace $\featurestateactionspace$ n'est pas anodin car de son choix va en grande partie dépendre la qualité du contrôle.

Pour apprendre le vecteur de paramètres $\paramrl$, l'algorithme \gls{lspi} de \citet{lagoudakis2003least} n'a besoin que d'une base de données inertes. Il ne s'agit pas exactement d'une base $\satrace{}$ comme la classification en utilise, mais d'une base légèrement plus contraignante à réunir, qui contient l'état suivant l'action (afin d'obtenir de l'information sur la dynamique du système) et le signal de récompense (afin d'obtenir de l'information sur la qualité du contrôle) :
# ?cs 1 sasrtrace 'D_{sasr}^{#1}' 'D_{sasr}^{\rewardfunc}' 'Trace de type $s,a,s\prime,r$'
\begin{equation}
\sasrtrace{\rewardfunc} = \{(\state_{\datasetindex},\action_{\datasetindex}, \state'_{\datasetindex}, r_{\datasetindex} = \rewardfunc(\state_{\datasetindex})) | \datasetindex \in \llbracket 0;\nbsamples-1\rrbracket\}.
\end{equation}

Les échantillons de $\sasrtrace{\rewardfunc}$ n'ont pas besoin de se suivre, on peut avoir $\state_{\datasetindex+1} \neq \state'_{\datasetindex}$. Il n'y a pas non plus de contrainte précise sur $\action_{\datasetindex}$, mais cet algorithme est parfois de convergence capricieuse, et il importe que la base d'échantillons qui lui est fournie soit représentative de la dynamique du \gls{mdp} pour chaque action. Une technique pratique et de réunir plusieurs trajectoires d'une politique aléatoire, en la faisant démarrer de différents états.

# Probabilités de transitions, fonction de récompense, fonction de valeur, po- litique optimale. Programmation Dynamique (PD).
# Approximation de la fonction de valeur, AR. L’AR permettant d’apprendre le contrôle par interaction avec le système, il possède quelques avantages sur le PD, comme la possibilité de s’adapter à un milieu changeant pour certains algorithmes ou de manière plus générale l’absence de besoin de connaître les probabilités de transition.
# Pour appliquer l’AR au monde réel, il est nécessaire d’exploiter efficacement les échantillons. Les échantillons sont très faciles à obtenir si on dispose d’un simulateur, et sont les seules données accessible sur certains systèmes. Pouvoir les exploiter en batch et off-policy permet de contrôler beaucoup de types de systèmes différents (qui peut le plus peut le moins). Least Square Policy Iteration (LSPI) Lagoudakis and Parr [2003].
** Définition de l'\glsentrytext{irl} 
Nous voilà donc armés pour résoudre (de manière approchée) le problème du contrôle optimal, pour peu que l'on dispose d'une description de la tâche à accomplir sous la forme d'une fonction de récompense $\rewardfunc$ et d'échantillons représentatifs du problème.

*** Définition du problème 
# <<hier:problemdef>>

La question qui se pose maintenant est de savoir dans quelle mesure il est possible d'automatiquement déduire d'une trace du comportement de l'expert une description de son but sous la forme d'une fonction de récompense. Cette méthode d'imitation a été suggérée pour la première fois par \citet{russell1998learning}. L'état de l'art sera traité au chapitre suivant, nous nous attelons ici à trouver une formulation mathématique saine du problème et à introduire les notions nécessaires à l'analyse des méthodes existantes et à l'introduction des nouvelles approches que nous proposons.

Outre l'intérêt intrinsèque de la découverte des motivations de l'expert (\autoref{hier-intrinseque}), apprendre une fonction de récompense correspondant au comportement de l'expert permet de soigner les méthodes d'imitation de leur "myopie" (\autoref{hier-myopie}), en guidant l'agent vers une imitation non pas du /comment/, comme le fait l'imitation directe de la politique, mais du /quoi/.

# ?s expertreward 'R^E' 'Récompense inconnue optimisée par l{\apos}expert'
Dans le formalisme des \gls{mdp} introduit plus tôt, l'expression du problème de l'\gls{irl} est de prime abord simple : l'on suppose que la politique de l'expert $\expertpolicy$ est une politique optimale pour une certaine fonction de récompense $\expertreward$ qu'il s'agit de trouver.
Les choses se corsent malheureusement extrêmement rapidement. Tout d'abord, cette formulation n'est pas un problème mathématique bien posé au sens d'\citet{hadamard1902problemes} : il existe en effet de multiples solutions, dont l'une est dégénérée, la récompense uniformément nulle. Tous les comportements (donc celui de l'expert) ont la même valeur ($0$) pour cette récompense, donc tous sont optimaux. En inversant, la politique de l'expert est optimale pour la récompense nulle, qui est donc solution du problème.

Plus subtilement, utiliser le formalisme des \gls{mdp} présuppose que certaines conditions sont réunies, notamment en ce qui concerne les espaces d'état et d'action. Il faut être en mesure de définir un espace d'état markovien, afin qu'un agent puisse prendre sa décision quant à l'action en se basant uniquement sur les informations de l'état. Il faut également être en mesure d'obtenir une trace de l'expert (le problème se pose déjà pour l'imitation directe de la politique, nous le mentionnons brièvement en \autoref{hier-general}). La tâche à accomplir doit pouvoir se mettre sous la forme d'une fonction de l'espace d'état (ce n'est pas parce que l'on peut décrire une politique sur un espace d'état que l'on peut y décrire la tâche à accomplir). L'espace d'action doit rester discret et de faible cardinal si l'on veut que l'immense majorité des approches d'\gls{irl} y restent tractables. Enfin, il faut que l'expert soit effectivement optimal, l'introduction d'erreurs dans la démonstration pouvant poser problème.

# ?s expertdistrib '\rho^E' 'Distribution stationnaire de l{\apos}expert'
Ces conditions, dont la réunion n'est certes pas triviale, ne vont cependant pas nous préoccuper. Nous nous attelons à trouver de nouvelles solutions au problème formulé par \citet{russell1998learning}. Pour mesurer notre succès, nous envisageons deux mesures de la qualité de la récompense $\appr{\rewardfunc}$ trouvée par un algorithme d'\gls{irl}. L'une est objective :
\begin{equation}
\expectationknowing{\rlvalue{\expertpolicy}{\expertreward}(\state) - \rlvalue{\optimalpolicy{\appr{\rewardfunc}}}{\expertreward}(\state)}{\state\sim\spacedistrib},
\end{equation}
l'autre est estimable :
\begin{equation}
\expectationknowing{\rlvalue{\optimalpolicy{\appr{\rewardfunc}}}{\appr{\rewardfunc}}(\state) - \rlvalue{\expertpolicy}{\appr{\rewardfunc}}(\state)}{\state \sim \spacedistrib}.
\end{equation}

Ces deux critères sont positifs ou au mieux nuls, il s'agit de les minimiser. Les deux termes de gauche sont des fonctions de valeur optimales (puisque $\expertpolicy$ est optimale pour $\expertreward$ et que $\optimalpolicy{\appr{\rewardfunc}}$ est optimale pour $\appr{\rewardfunc}$). Ces deux critères sont donc liés à l'optimalité des valeurs de droite et tendront vers $0$ quand, pour le premier la politique apprise en optimisant la récompense $\appr{\rewardfunc}$ sera optimale pour la récompense inconnue $\expertreward$ et quand pour le second la récompense $\appr{\rewardfunc}$ est telle qu'elle admet la politique expert comme politique optimale.

La distribution en jeu $\spacedistrib$ est également importante. La distribution selon laquelle les données de l'expert sont tirées ($\expertdistrib$) est celle considérée par défaut, comme c'est le cas pour les méthodes supervisées (voir explication en \autoref{hier-classification}). L'\gls{irl} présente cependant sur les méthodes supervisées l'avantage de pouvoir bien mieux généraliser aux parties de l'espace d'état sur lesquels l'expert n'a pas fourni de démonstrations, il peut donc être intéressant d'évaluer la qualité du contrôle non pas sur la distribution correspondant aux démonstrations de l'expert, mais justement sur une distribution plus large (telle que $\spacedistrib\subset\expertdistrib$).

Le premier critère est celui qui nous intéresse vraiment. Il juge le comportement issu de l'optimisation par un algorithme d'\gls{rl} ou de \gls{dp} de la récompense $\appr{\rewardfunc}$ à l'aune de la description de la tâche confiée à l'expert, à savoir $\expertreward$. Cela permet dans les cas où plusieurs bonnes solutions existent de ne pas pénaliser un agent qui aurait fait un choix différent de celui de l'expert si cela importe peu. Il s'agit donc d'une mesure plus fine que l'erreur de classification $\classiferror{\expertdistrib}$, qui sanctionne toute divergence d'opinion avec l'expert. Lors du test d'algorithmes de \gls{irl} sur des problèmes jouets où la récompense $\expertreward$ nous est connue, nous étudierons ce critère ou un critère équivalent. Sur de vrais problèmes, en revanche, il est impossible de l'estimer : il faut résoudre le problème de l'\gls{irl} pour savoir si on a bien résolu le problème de l'\gls{irl}.

Le second critère a le défaut d'être optimisé par la récompense nulle. Nous verrons cependant, en analysant les différentes approches d'\gls{irl}, les mécanismes mis en place afin d'éviter les solutions dégénérées. Il a l'avantage de pouvoir être estimé puisqu'à l'inverse de $\expertreward$, la fonction de récompense $\appr{\rewardfunc}$ est connue. Un algorithme comme \gls{lstd} \citep{bradtke1996linear,boyan2002technical} peut fournir une approximation \gls{offpolicy} d'une fonction de valeur, c'est à dire évaluer une politique arbitraire, différente de celle qui contrôle le système.
*** Attribut moyen 
# <<hier:attribut>>

# ?s paramirl '\theta' 'Vecteur de paramètres pour l{\apos}ARI'
    Le fonction de récompense étant l'objet recherché celle-ci sera, pour ne pas bouder une méthode éprouvée, approximée par un schéma linéaire :
    \begin{equation}
    \appr{\rewardfunc}_{\paramirl}(\state) = \transpose{\paramirl}\featurestatefunc(\state)
    \label{eq:paramirl}
    \end{equation}
Là encore, le choix des attributs est important. Il faut qu'ils permettent à la fonction de récompense de décrire la tâche effectuée par l'expert.

# ?cs 1 rlmu '\mu^{#1}' '\mu^{\pi}' 'Attribut moyen de la politique $\pi$'
Dans le contexte de l'approximation linéaire de la fonction de récompense, l'expression de la fonction de valeur d'une politique $\policy$ fait apparaître un terme que nous appellerons $\rlmu{\policy}$ dont le prochain chapitre révèlera l'importance :
\begin{eqnarray}
\label{eq:mudef}
\textrm{avec }\forall \timeindex \in \llbracket 1;T\rrbracket, \state_{\timeindex} &\sim& \transprobfunceval{\state_{\timeindex-1}}{\policy(\state_{\timeindex-1})}{\cdot},\\
\rlvalue{\policy}{\appr{\rewardfunc}}(\state)&=&\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\appr{\rewardfunc}(\state_{\timeindex})}{\state_0=s}\\
&=&\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\transpose{\paramirl}\featurestatefunc(\state_{\timeindex})}{\state_0=s}\\
&=&\transpose{\paramirl}\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\featurestatefunc(\state_{\timeindex})}{\state_0=s}\\
&=&\transpose{\paramirl}\rlmu{\policy}(\state)
\label{eq:vthetamu}
\end{eqnarray}

Ce terme, l'attribut moyen d'une politique, est une fonction vectorielle qui porte la structure temporelle du \gls{mdp} contraint par la politique. C'est, que l'on me pardonne l'oxymore, une trace du futur qui est liée à la distribution qu'occupera l'agent à partir d'un certain état. La présence du facteur d'amortissement $\discount$ dans l'expression empêche l'identification complète à une distribution, ce qui est heureux car sans cela l'expression tendrait vers la distribution stationnaire quel que soit l'état de départ (à la projection dans $\featurestateactionspace$ près).

De la même manière que la fonction de qualité donne à la fonction de valeur un degré de liberté supplémentaire, il est possible de donner une action en argument à l'attribut moyen avec le même effet :
\begin{eqnarray}
\textrm{avec }\forall \timeindex \in \llbracket 1;T\rrbracket, \state_{\timeindex} &\sim& \transprobfunceval{\state_{\timeindex-1}}{\policy(\state_{\timeindex-1})}{\cdot},\\
\rlmu{\policy}(\state)&=&\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\featurestatefunc(\state_{\timeindex})}{\state_0 = \state}\\
\textrm{avec }\forall \timeindex \in \llbracket 2;T\rrbracket, \state_{\timeindex} &\sim& \transprobfunceval{\state_{\timeindex-1}}{\policy(\state_{\timeindex-1})}{\cdot},\\
\textrm{et }\state_{1} &\sim& \transprobfunceval{\state_{0}}{\action}{\cdot},\\
\rlmu{\policy}(\state,\action)&=&\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\featurestatefunc(\state_{\timeindex})}{\state_0 = \state}.
\label{eq:mudefsum}
\end{eqnarray}
Si la récompense est définie sur les couples état-action plutôt que sur les états seuls, alors l'expression de l'attribut moyen devient simplement :
\begin{equation}
\rlmu{\policy}(\state)=\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\featurestateactionfunc\left(\state_{\timeindex},\policy(\state_{\timeindex})\right)}{\state_0 = \state}.
\end{equation}

# ?s expertmu '\mu^E' 'Attribut moyen de l{\apos}expert'
L'attribut moyen donne une idée des états que traversera l'agent. Il contient donc des informations sur la dynamique induite par sa politique. Cela va avoir un impact important sur la qualité des diverses approximations linéaires dont nous avons parlé si l'attribut moyen est utilisé en tant qu'attribut tout court, comme il le sera au \autoref{hier-scirl}. La présence de la dynamique du \gls{mdp} dans l'attribut moyen est illustrée \autoref{fig:mountain_car_mu}.

# snippet illustration mountain car : la forme en escargot est caractéristique elle correspond à des va-et-vient dont l'apogée est de plus en plus haute.
#    Deux politiques ayant des attributs moyens similaires auront des valeurs similaires quelle que soit la récompense (exprimée dans le schéma d'approximation linéaire) considérée. En revanche, il est possible d'avoir deux attributs moyens complètement différents et d'avoir la même valeur vis à vis de la "vraie" fonction de récompense (illustration sur le \gls{gridworld}, passage en haut à gauche et passage en bas à droite).
# ?cs 2 norm '\left|\left|{#2}\right|\right|_{#1}' '||\cdot||_x' 'Norme $x$' 
Une propriété immédiate de l'attribut moyen est exploitée par une classe d'algorithme d'\gls{irl} (que nous décrirons plus en détail en \autoref{hier-repetee}). Cette propriété découle directement de la définition de l'attribut moyen :
\begin{proposition}
\label{thm:closemuclosev}
Soit $\policy_1$ et $\policy_2$ deux politiques sur un même \gls{mdp}, dont les attributs moyens respectifs $\rlmu{\policy_1}$ et $\rlmu{\policy_2}$ sont proches (pour une certaine norme $\norm{x}{\cdot}$) :
\begin{equation}
\norm{x}{\rlmu{\policy_1}-\rlmu{\policy_2}} \leq \epsilon \in \reals.
\end{equation}
Alors quelque soit la récompense $\rewardfunc_{\paramirl} = \transpose{\paramirl}\featurestatefunc$, les valeurs des deux politiques $\policy_1$ et $\policy_2$ sont également "proches" :
\begin{equation}
\norm{x}{\rlvalue{\policy_1}{\rewardfunc_{\paramirl}}-\rlvalue{\policy_2}{\rewardfunc_{\paramirl}}} \leq \norm{x}{\paramirl}\epsilon.
\end{equation}
\end{proposition}
\begin{proof}
D'après \autoref{eq:vthetamu} : 
\begin{eqnarray}
\norm{x}{\rlvalue{\policy_1}{\rewardfunc_{\paramirl}}-\rlvalue{\policy_2}{\rewardfunc_{\paramirl}}} &=& \norm{x}{\transpose{\paramirl}\rlmu{\policy_1}-\transpose{\paramirl}\rlmu{\policy_1}}\\
&=& \norm{x}{\transpose{\paramirl}(\rlmu{\policy_1}-\rlmu{\policy_1})}.\\
\end{eqnarray}
En utilisant l'inégalité de Cauchy-Schwartz, on conclut : 
\begin{eqnarray}
\norm{x}{\rlvalue{\policy_1}{\rewardfunc_{\paramirl}}-\rlvalue{\policy_2}{\rewardfunc_{\paramirl}}} &\leq& \norm{x}{\paramirl}\norm{x}{\rlmu{\policy_1}-\rlmu{\policy_1}}\\
&\leq& \norm{x}{\paramirl}\epsilon.
\end{eqnarray}
\end{proof}
Ce résultat est assez intuitif, si deux politiques induisent les mêmes trajectoires, elles auront même valeur. L'imitation telle que résolue par l'apprentissage supervisé donne ce type de résultats. Il est intéressant de noter que dans certains cas la réciproque est fausse FIXME:illustrer ça. Il est donc théoriquement possible de résoudre le problème de l'\gls{irl} en s'éloignant des solutions que l'apprentissage supervisé peut proposer. Il est en effet possible d'imaginer qu'un algorithme d'\gls{irl} renvoie une récompense qui, une fois optimisée, donnera une politique d'attribut moyen différent de celui de la politique de l'expert, mais de valeur semblable.

\begin{figure}
\begin{tikzpicture}[scale=1.75]
%\draw [help lines] (0,0) grid (6,-6);
\node at (0,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_6x0}};
\node at (0,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_5x0}};
\node at (0,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_4x0}};
\node at (0,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_3x0}};
\node at (0,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_2x0}};
\node at (0,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_1x0}};
\node at (0,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_0x0}};
					  
\node at (1,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_6x1}};
\node at (1,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_5x1}};
\node at (1,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_4x1}};
\node at (1,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_3x1}};
\node at (1,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_2x1}};
\node at (1,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_1x1}};
\node at (1,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_0x1}};
					  
\node at (2,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_6x2}};
\node at (2,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_5x2}};
\node at (2,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_4x2}};
\node at (2,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_3x2}};
\node at (2,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_2x2}};
\node at (2,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_1x2}};
\node at (2,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_0x2}};
					  
\node at (3,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_6x3}};
\node at (3,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_5x3}};
\node at (3,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_4x3}};
\node at (3,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_3x3}};
\node at (3,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_2x3}};
\node at (3,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_1x3}};
\node at (3,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_0x3}};
					  
\node at (4,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_6x4}};
\node at (4,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_5x4}};
\node at (4,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_4x4}};
\node at (4,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_3x4}};
\node at (4,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_2x4}};
\node at (4,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_1x4}};
\node at (4,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_0x4}};
					  
\node at (5,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_6x5}};
\node at (5,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_5x5}};
\node at (5,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_4x5}};
\node at (5,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_3x5}};
\node at (5,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_2x5}};
\node at (5,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_1x5}};
\node at (5,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_0x5}};
					  
\node at (6,-0) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_6x6}};
\node at (6,-1) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_5x6}};
\node at (6,-2) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_4x6}};
\node at (6,-3) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_3x6}};
\node at (6,-4) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_2x6}};
\node at (6,-5) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_1x6}};
\node at (6,-6) {\includegraphics[width=.13\textwidth]{Figures/Mountain_car_mu_0x6}};

\end{tikzpicture}
\caption{\`A comparer à \autoref{fig:mountain_car_psi}, l'attribut moyen (ici celui de l'expert sur le \gls{mountaincar}) est un reflet de la dynamique du système contraint par la politique. La forme en escargot, caractéristique de trajectoires en va-et-vient à l'apogée de plus en plus haute, a clairement laissé sa trace sur l'attribut moyen.}
\label{fig:mountain_car_mu}
\end{figure}

* État de l'art et problématique
  Avant d'analyser les approches existantes d'\gls{irl}, intéressons nous plus en détail à la définition précise de la fonction de récompense, dont nous avons mentionné plus haut qu'il était possible de la définir sur l'espace d'état, sur l'espace d'état-action ou même sur l'espace d'état-action-état, mais sans nous montrer plus spécifiques. Nous verrons ensuite quelles transformations il est possible d'y appliquer sans changer les politiques optimales, une technique connue sous le nom de \gls{rewardshaping}. 
** Fonction de récompense 
# <<hier:recompense>>
 
   Une fonction de récompense sur les états seuls quantifie de manière locale la désirabilité d'un état. Ce sont les fonctions de valeur et de qualité qui vont propager ce signal d'après la dynamique et vont faire intervenir les actions dans l'expression de la qualité du contrôle. Une récompense sur l'espace état-action pose dès le niveau local un jugement sur la pertinence d'une action dans un certain état. Dans notre exemple du \gls{mountaincar}, cela permettrait par exemple de décrire que les actions actives (droite et gauche, par opposition au neutre) consomment du carburant et donc ont une récompense associée négative pour tenir compte de cette consommation et la minimiser, tout en accomplissant la tâche. Nous allons voir que la différence entre ces deux manières de voir la récompense peut être comblée formellement au prix d'une augmentation de l'espace d'état. Dès lors nous ne ferons pas par la suite de différence entre les récompenses sur les états ou sur les couples état-action, et utiliserons la formulation la plus pratique pour l'exposition de l'algorithme d'\gls{irl} concerné. Pour poursuivre avec notre exemple du \gls{mountaincar}, la méthode proposée revient à introduire dans l'état la jauge de carburant et de conserver une récompense sur les état seuls. On obtient le même contrôle optimal qu'avec la récompense écologique sur l'espace d'état-action.

\begin{proposition}
Soit $\mdpbis = \left\{\statespace, \actionspace, \transprobfunc, \rewardfunc, \discount\right\}$ un \gls{mdp} dont la récompense est définie sur l'espace état-action-état :
\begin{eqnarray}
R &\in& \twosetsfunctions{\statespace \times \actionspace \times \statespace}{\reals}\\
 R( \state_t,\action_t,\state_{t+1}) &=& R_{\statespace}(\state_t) + R_{\statespace \times \actionspace \times \statespace}( \state_t,\action_t,\state_{t+1})
\end{eqnarray}
Soit $\mdpbis' = \left\{\statespace', \actionspace, \transprobfunc', \rewardfunc', \discount\right\}$ le \gls{mdp} augmenté dont l'espace d'état est le même que pour $\mdpbis$, à une composante $\component{\state}{\dimstate+1}$ près. Les probabilités de transitions $\transprobfunc'$ sont définies par :
\begin{multline}
\forall \state'_{t} = \begin{pmatrix}s_t\in \statespace \\s_{t}^{d_{\statespace}+1}\end{pmatrix},\action_t,\state'_{t+1} = \begin{pmatrix}s_{t+1}\in \statespace \\s_{t+1}^{\dimstate+1}\end{pmatrix} \in \statespace'\times \actionspace\times \statespace'\\ \transprobfunc'\left(\state'_{t+1}|\state'_{t},\action_{t}\right) = \begin{cases}
\transprobfunceval{\state_t}{\action_t}{\state_{t+1}}& \textrm{si }\component{\state}{\dimstate+1} = {1\over \discount}R_{\statespace \times \actionspace \times \statespace}( \state_t,\action_t,\state_{t+1})\\
0 &\textrm{sinon.}
\end{cases}
\end{multline}
La fonction de récompense $\rewardfunc'$ est définie sur les états seuls. Elle vaut :
\begin{equation}
\rewardfunc'\left(\state'_{\timeindex} = \begin{pmatrix}\state_{\timeindex}\\\component{\state}{\dimstate+1}_{\timeindex} \end{pmatrix}\right) = \rewardfunc_{\statespace}(\state_{\timeindex}) +  \component{\state}{\dimstate+1}_{\timeindex}
\end{equation}
Considérant naturellement qu'une politique définie sur $\mdpbis$ l'est aussi sur $\mdpbis'$ en n'utilisant que les $\dimstate$ premières composantes de l'espace d'état (donc les composantes communes à $\statespace$ et $\statespace'$), on a alors le résultat suivant :
\begin{equation}
\forall \policy \in \twosetsfunctions{\statespace}{\actionspace} \subset \twosetsfunctions{\statespace'}{\actionspace}, \rlvalue{\policy}{\rewardfunc} = \rlvalue{\policy}{\rewardfunc'}.
\end{equation}
Toute politique définie dans $\mdpbis$ ayant la même valeur dans $\mdpbis$ que dans $\mdpbis'$, un corollaire immédiat est que chacune des politiques optimales pour $\mdpbis$ est optimale dans $\mdpbis'$. On peut ainsi travailler indifféremment avec des récompenses sur les états seuls, sur les couples état-action ou sur des triplets état-action-état sans perte de généralité, pour peu que l'espace d'état soit correctement défini.
\end{proposition} 
\begin{proof}
Pour peu que l'on convienne que la composante supplémentaire dans $\statespace'$ d'un état de départ $s_0$ est nulle, et que l'on augmente l'horizon d'un pas de temps dans $\mdpbis'$ avec une récompense sur les états seuls nulle :
\begin{eqnarray}
\component{\state}{\dimstate+1}_{0} &=& 0,\\
\rewardfunc_{\statespace}(\state_{\infty+1}) &=& 0,
\end{eqnarray}
la démonstration de ce résultat se déroule assez simplement (on ne note pas l'espérance, qui devrait apparaître à chaque ligne sauf la dernière):
\begin{eqnarray}
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0}\right) &=& \sum\limits_{\timeindex = 0}^{\infty+1} \discount^{\timeindex}\rewardfunc'(\state'_{\timeindex})\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=& \discount^{0}\rewardfunc'(\state'_{0}) + \sum\limits_{\timeindex = 1}^{\infty} \discount^{\timeindex}\rewardfunc'(\state'_{\timeindex}) + \discount^{\infty+1}\rewardfunc'(\state'_{\infty+1})\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=& \rewardfunc_{\statespace}(\state_{0}) + \component{\state}{\dimstate+1}_{0} + \sum\limits_{\timeindex = 1}^{\infty} \discount^{\timeindex}\left(\rewardfunc_{\statespace}(\state_{\timeindex}) + \component{\state}{\dimstate+1}_{\timeindex} \right) +  \discount^{\infty+1}(\rewardfunc_{\statespace}(\state_{\infty+1}) + \component{\state}{\dimstate+1}_{\infty+1})\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=& \rewardfunc_{\statespace}(\state_{0}) + \sum\limits_{\timeindex = 1}^{\infty}\discount^{\timeindex}\rewardfunc_{\statespace}(\state_{\timeindex}) + \sum\limits_{\timeindex = 1}^{\infty}\discount^{\timeindex}\component{\state}{\dimstate+1}_{\timeindex} +  \discount^{\infty+1} \component{\state}{\dimstate+1}_{\infty+1}\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=& \sum\limits_{\timeindex = 0}^{\infty}\discount^{\timeindex}\rewardfunc_{\statespace}(\state_{\timeindex}) + \sum\limits_{\timeindex = 1}^{\infty+1}\discount^{\timeindex}\component{\state}{\dimstate+1}_{\timeindex}\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=&  \sum\limits_{\timeindex = 0}^{\infty}\discount^{\timeindex}\rewardfunc_{\statespace}(\state_{\timeindex}) + \sum\limits_{\timeindex = 1}^{\infty+1}\discount^{\timeindex}{1\over \discount}\rewardfunc_{\statespace \times \actionspace \times \statespace}(\state_{t-1},\action_{t-1},\state_{t})\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=&  \sum\limits_{\timeindex = 0}^{\infty}\discount^{\timeindex}\rewardfunc_{\statespace}(\state_{\timeindex}) + \sum\limits_{\timeindex = 1}^{\infty+1}\discount^{\timeindex-1}\rewardfunc_{\statespace \times \actionspace \times \statespace}(\state_{t-1},\action_{t-1},\state_{t})\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=&  \sum\limits_{\timeindex = 0}^{\infty}\discount^{\timeindex}\rewardfunc_{\statespace}(\state_{\timeindex}) + \sum\limits_{\timeindex = 0}^{\infty}\discount^{\timeindex}\rewardfunc_{\statespace \times \actionspace \times \statespace}(\state_{t},\action_{t},\state_{t+1})\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=&  \sum\limits_{\timeindex = 0}^{\infty}\discount^{\timeindex}\left(\rewardfunc_{\statespace}(\state_{\timeindex}) + \rewardfunc_{\statespace \times \actionspace \times \statespace}(\state_{t},\action_{t},\state_{t+1})\right)\\
\rlvalue{\policy}{\rewardfunc'}\left(\state'_{0} \right) &=& \rlvalue{\policy}{\rewardfunc}(\state_0)
\end{eqnarray}
\end{proof}

Nous savons maintenant formellement que l'imprécision que nous avons laissé flotter sur la définition de la fonction de récompense n'a pas de conséquence sur l'objet que l'on cherche lorsqu'on résout le problème de l'\gls{irl}. Armé d'un bon espace d'état il n'importe pas que l'algorithme concerné recherche la solution dans $\twosetsfunctions{\statespace}{\reals}$,  $\twosetsfunctions{\statespace\times \actionspace}{\reals}$ ou  $\twosetsfunctions{\statespace\times \actionspace \times \statespace}{\reals}$.

L'étude de la relation entre la condition d'optimalité d'une politique (comme celle de l'expert) et la fonction de récompense associée (que l'on cherche à connaître) nous amène à découvrir d'autres propriétés importantes pour l'analyse du problème de l'\gls{irl}. Nous nous intéressons particulièrement aux transformations qu'il est possible d'appliquer à la récompense sans perturber la relation d'ordre existant entre les valeurs des différentes politiques (relation d'ordre sur laquelle est basée la notion d'optimalité \autoref{eq:optimalite}). Nous ne souhaitons pas voir ces transformations en mesure de minimiser la fonction objectif d'un algorithme d'\gls{irl}. En effet, cela signifierait que l'on modifie la récompense sans modifier le sens qu'elle porte et donc qu'en réalité l'on ne résout pas le problème de l'\gls{irl} mais une formulation dégénérée de celui-ci.

Un résultat immédiat, qui découle du fait que la condition d'optimalité d'une politique est basée sur une relation d'ordre sur les valeurs, qui sont par définition linéaires vis-à-vis de la récompense, est qu'il est possible d'appliquer n'importe quelle transformation affine de coefficient strictement positif à une récompense sans en changer la signification. Ainsi notamment nous pourrons normaliser une récompense sans perte de généralité.

L'étude des transformations n'ayant pas d'impact sur les politiques optimales a été menée dans le cadre de l'\gls{rl}. L'objectif est de faciliter la recherche de politiques optimales sans modifier celles-ci. \citet{ng1999policy} trouvent une élégante condition nécessaire et suffisante à la préservation de politiques optimales par changement de récompense : à une récompense $\rewardfunc$ définie sur le triple espace état-action-état $\statespace \times \actionspace \times \statespace$, on peut ajouter toute fonction $\rewardfunc'$ définie sous la forme : 
\begin{equation}
\label{eq:rewardshaping}
\rewardfunc'(\state,\action,\state_{+1}) = \discount\rewardfunc_{\statespace}(s_{+1}) -  \rewardfunc_{\statespace}(s),
\end{equation}
où $\rewardfunc_{\statespace}$ est définie sur l'espace d'état. Cette condition est nécessaire dans le sens que toute fonction de récompense définie de la sorte peut être ajoutée à une fonction de récompense de $\twosetsfunctions{\statespace \times \actionspace \times \statespace}{\reals}$ sans en changer les politiques optimales. Elle est suffisante en ceci qu'une fonction de récompense $\rewardfunc'$ pourra être ajoutée à une autre si il existe une fonction $\rewardfunc_{\statespace}$ qui permette de l'écrire sous la forme donnée en \autoref{eq:rewardshaping}. Si une telle fonction n'existe pas, il est possible que les probabilités de transitions soient telles que la modification de la fonction de récompense entraînera une modification des politiques optimales.


Maintenant que nous disposons d'une meilleure compréhension de l'objet que nous cherchons, voyons comment les approches existantes mènent cette recherche.
** Premières formulations du problème 
   Dans la dernière partie de sa contribution, \citet{russell1998learning} pose le problème de l'\gls{irl} en termes généraux. Il ne propose pas de solution, mais explique l'intérêt du problème et isole des questions-clefs dont certaines restent encore ouvertes aujourd'hui.

   C'est dans \citep{ng2000algorithms} que les premières solutions apparaissent. Afin de trouver la récompense, les auteurs inversent l'équation de Bellman et formulent ainsi une condition nécessaire et suffisante sur la récompense pour qu'elle soit solution du problème de l'\gls{irl}. Cette condition n'écarte pas les solutions dégénérées comme la récompense nulle. En conséquence, pour les \gls{mdp} finis où les probabilités de transitions sont connues, les auteurs ajoutent à cette condition nécessaire et suffisante une contrainte de pénalisation du désaccord avec l'expert ainsi qu'une contrainte de régularisation en norme $1$ et obtiennent un programme linéaire qu'il est possible de résoudre. Pour les \gls{mdp} dont l'espace d'état est grand ou infini, une paramétrisation linéaire de la fonction de récompense est naturellement choisie (rares sont les approches qui choisissent un autre schéma). De fait, et bien qu'il ne porte pas encore ce nom, l'attribut moyen de l'expert apparaît dans le développement. Une version échantillonnée du programme linéaire découlant du schéma d'approximation linéaire de la récompense est proposé. La contrainte d'optimalité repose non plus sur la connaissance des probabilités de transition, mais sur la recherche d'une récompense donnant à la politique de l'expert une plus grande valeur qu'aux politiques optimales trouvées en optimisant la récompense telle que connues pendant les itérations précédentes de l'algorithme (il faut donc résoudre un \gls{mdp} à chaque itération).

   On retrouve dans ce travail plusieurs éléments communs aux approches ultérieures. Il y a le constat primordial que contrairement à ce qui se passe pour l'\gls{rl} où l'on cherche le point fixe d'un opérateur contractant (quand on estime une fonction de valeur) et qui donc existe et est unique, l'on a ici affaire à un problème mal posé, et il convient d'introduire des contraintes supplémentaires afin d'éviter les solutions dégénérées. Le choix et la justification de ces contraintes est un bon moyen de différencier les approches existantes. Il y a le schéma presque ubiquitaire de l'approximation linéaire de la fonction de récompense, qui entraîne l'apparition de l'attribut moyen. Enfin on retrouve la nécessité pour l'algorithme d'\gls{irl} de résoudre le problème de l'\gls{rl} de manière répétée.

** Méthodes nécessitant la résolution répétée d'un \glsentrytext{mdp}
# <<hier:repetee>>
   Cette nécessité est lourde de conséquence quant à l'usage pratique d'un algorithme d'\gls{irl}. La résolution du problème de l'\gls{rl} nécessite en effet soit beaucoup de données pour sa résolution \gls{offpolicy}, soit l'accès à un simulateur ou au système réel pour sa résolution \gls{onpolicy}.

   \citet{abbeel2004apprenticeship} proposent l'algorithme \gls{pirl} dans un travail séminal dont la structure va être reprise (plus ou moins consciemment) par plusieurs autres approches. La récompense est paramétrée linéairement, de fait la notion d'attribut moyen apparaît assez logiquement (\autoref{eq:mudef}). Elle obtient même une place centrale dans cette approche itérative de l'\gls{irl}. L'idée est, à chaque itération, d'apprendre la politique optimale pour la fonction de récompense courante (via un algorithme d'\gls{rl}). L'attribut moyen de cette politique est alors évalué. La fonction de coût que minimise l'approche joue sur une définition de la proximité entre l'attribut moyen de cette politique et l'attribut moyen de la politique de l'expert. A chaque itération cette proximité est évaluée, et le vecteur de paramètre $\paramirl$ de la récompense change dans une direction qui va minimiser cette distance. La justification est que deux politiques d'attribut moyen semblable auront une valeur semblable pour toute récompense, y compris donc la récompense inconnue optimisée par l'expert (\autoref{thm:closemuclosev}). Le seul bémol de cette logique est que la réciproque peut être fausse, deux attributs extrêmement différents peuvent mener à la même valeur FIXME:schéma Gridworld. Plusieurs autres approches sont basées sur le même schéma itératif. Certaines le sont explicitement, d'autre se défendent d'utiliser la même logique, mais la revue proposée par \citet{neu2009training} démontre que mathématiquement, ces approches sont bien semblables au travail de \citet{abbeel2004apprenticeship}. Cette structure commune est décrite \autoref{alg:repetee}.

# ?s mudistance 'd' 'Distance entre deux attributs moyens'
# ?s muupdate 'u' 'Règle de mise à jour pour les algorithme d{\apos}ARI itératifs'

\begin{algorithm}
\Entree{
Une démonstration de la politique experte $\expertpolicy$\;
Un critère d'arrêt $\epsilon$\;}
\Donnees{
Une notion de distance $\mudistance$ permettant d'évaluer la similarité de deux attributs moyens\;
Une règle de mise à jour $\muupdate$ permettant de réduire la distance\;
Une méthode d'évaluation de l'attribut moyen d'une politique arbitraire\;
Une méthode de résolution d'un \gls{mdp}\;
}
\Sortie{Un vecteur de paramètres $\paramirl$ pour la récompense $\appr{\rewardfunc}_{\paramirl}$}
Calculer l'attribut moyen de l'expert $\expertmu$ à partir de la démonstration\;
Initialiser $\theta$ arbitrairement\;
\Tq{$\mudistance(\expertmu,\appr{\rlmu{\policy}}) > \epsilon$}{
\nllabel{alg:l:mdpsolver}$\policy \leftarrow $ Une politique (quasi) optimale pour la récompense $\appr{\rewardfunc}_{\paramirl}$\;
\nllabel{alg:l:musolver}$\appr{\rlmu{\policy}} \leftarrow$ Une approximation de l'attribut moyen de la politique $\policy$\;
$\theta \leftarrow \muupdate(\expertmu,\appr{\rlmu{\policy}})$\;
}
\Retour{$\theta$}
\caption{Structure commune à la plupart des algorithmes d'\gls{irl}}
\label{alg:repetee}
\end{algorithm}

\gls{pirl} est intrinsèquement un algorithme d'imitation plus que d'\gls{irl}, en ce sens que la sortie est une politique stochastique (plus précisément un $\beta$-\gls{mixing} de politiques déterministes) et non une fonction de récompense. Il est cependant facile de le "convertir" en extrayant la meilleure fonction de récompense, celle liée à la plus petite distance entre attribut moyen de la politique optimale et attribut moyen de l'expert. L'on perd en peut en pureté théorique ce que l'on gagne en simplicité d'utilisation.

   La technique proposée dans \cite{neu2007apprenticeship} (\gls{pm}) repose sur une réduction des désaccords entre la politique de l'expert et la politique à l'itération courante, via une recherche dans l'espace des paramètres $\paramirl$ de la récompense. Le lien unissant récompense et politique, qui est lié à la résolution d'un \gls{mdp}, est ici caractérisé par des outils d'analyse fonctionnelle afin d'extraire une solution analytique du problème qui soit également calculable. Cette contribution présente une plus grand robustesse aux changements d'échelle des attributs. L'on a vu en \autoref{hier-recompense} que l'ensemble des politiques optimales est stable par dilatation de la récompense, et donc par extension par dilatation des attributs. Cette plus grande robustesse est donc signe d'une formulation plus saine de l'\gls{irl}. Le bruit dans les attributs est également mieux géré. Le raisonnement porte toujours malheureusement sur une sortie sous forme de politique et non de récompense.

   Basé sur la théorie des jeux dans un contexte où l'agent définit une politique qui doit être meilleure que celle de l'expert pour une récompense choisie par l'adversaire, l'algorithme \gls{mwal} de \cite{syed2008game} tombe sur un os[fn::Os à \gls{mwal}.]. L'inclusion de savoir \emph{a priori} quant à la contribution positive ou néfaste de certains attributs à la réalisation de la tâche donne un algorithme d'\gls{irl} dont le résultat peut s'avérer meilleur que l'expert ou, encore plus paradoxal, qui peut fonctionner même en l'absence d'expert (sic). En contrepartie de la nécessité de disposer de ces connaissances \emph{a priori}, il est plus rapide à l'exécution que \gls{pirl} et capable de gérer la non optimalité de l'expert. Il exige malheureusement de connaître la dynamique du \gls{mdp}. La définition de l'\gls{irl} comme un problème distinct de l'imitation y gagne en clarté car cette contribution mentionne explicitement les problèmes liés aux raisonnements sur des politiques mixées et non des récompenses, sans pour autant les résoudre. Les auteurs rendent aussi explicites les difficultés soulevées en \autoref{hier-problemdef} quant à la définition des espaces d'état et d'action, notamment le respect de la propriété de Markov.

   Ce travail est étendu dans \cite{syed2008apprenticeship} qui, en formulant la résolution du MDP comme un programme linéaire, assainissent la formulation de \gls{pirl} et \gls{mwal} en leur permettant de raisonner sur une politique stationnaire et sans \gls{mixing}. En formulant l'\gls{irl} comme un programme linéaire également, les auteurs proposent \gls{lpal}, qui retourne un politique (non une récompense). La formulation de ce programme linéaire met en jeu des grandeurs qui correspondent à l'attribut moyen de politiques optimales pour des récompenses arbitraires, la structure itérative est donc toujours présente.

   L'approche de classification à marge structurée décrite en \autoref{hier:attributs} et utilisée en \citep{ratliff2007imitation} pour l'imitation supervisée apparaît également dans \citep{ratliff2006maximum}. Plutôt que d'associer une action à un état comme dans l'approche supervisée, l'algorithme \gls{mmp} proposé associe une fonction de récompense à un \gls{mdp}. Bien que les auteurs s'en défendent, cette formulation est compatible avec la structure familière des autres algorithmes d'\gls{irl} de cette période, comme le démontrent \citet{neu2009training}. La philosophie à la base de l'approche reste cependant différente, et bien que les applications envisagées sont variées et non triviales (voir \citep{ratliff2009learning}, où elles sont toutes réunies et la méthode de boosting proposée en \cite{ratliff2007boosting} expliquée plus en profondeur), d'autres problèmes apparaissent comme la nécessité de résoudre de multiples \gls{mdp} de manière tractable, et de formuler le problème du contrôle non pas comme un \gls{mdp}, mais comme de multiples \gls{mdp} "compatibles" entre eux.

   La formulation probabiliste de \gls{maxent} de \cite{ziebart2008maximum} est intéressante car elle formule un critère (l'entropie) pour choisir entre deux politiques qui jusqu'à présent étaient équivalentes (même valeur ou même attribut moyen). Structurellement, cependant, les différences restent minces avec toujours présentes la résolution répétée d'un MDP et l'estimation de l'attribut moyen de politiques arbitraire.

   Curieusement semblables à une approche non officiellement \gls{irl} et plus ancienne, \cite{chajewska2001learning}, les travaux de  \cite{ramachandran2007bayesian} posent l'\gls{irl} comme un problème d'inférence bayésienne où l'on cherche la récompense la plus vraisemblable au vu de la démonstration de l'expert. Il en résulte une formulation plutôt saine de l'\gls{irl} permettant par exemple d'incorporer de la connaissance \emph{a priori} sans que cela soit pour autant nécessaire, ou de qualifier le niveau de confiance dans la démonstration de l'expert, qui peut alors être sous-optimal. Des arguments similaires à ceux développés pour \gls{maxent} permettent de lever les ambiguïtés induites par le fait que l'\gls{irl} est un problème mal posé. Ces travaux sont étendus par \cite{lopes2009active} qui en extrayant une information d'incertitude proposent un schéma d'échantillonnage interactif selon la même logique, et avec les mêmes avantages que ceux développés par \citet{chernova2007confidence} pour l'approche supervisée (voir \autoref{hier-myopie}). Bien que cette approche diffère des autres approches dans l'exposition du raisonnement, elle reste très similaire dans l'implémentation, puisque'il faut toujours calculer des politiques optimales et obtenir des échantillons de cette politique (pour calculer des postérieurs bayésiens et non plus des attributs moyens). 

   Signalons finalement les travaux de \cite{jin2010gaussian}, qui par l'usage des \gls{gp} plutôt que d'un schéma linéaire pour l'approximation de la récompense, ainsi que par l'usage d'algorithmes d'\gls{rl} mettant les \gls{gp} en jeu \citep{rasmussen2004gaussian,deisenroth2009gaussian} permet de ne plus avoir à concevoir des attributs pour appliquer le schéma itératif classique de l'\gls{irl}, se basant sur les échantillons comme points supports.

   La plupart de ces approches sont résumées dans \cite{neu2009training}. Ces différentes contributions ont le mérite d'observer le problème sous plusieurs angles, de se placer aux limites du problème (expert non optimal, attributs bruités, etc.) et d'aborder des questions fondamentales (notion de distance entre politiques ou récompenses). Le manque d'harmonisation du domaine de l'\gls{irl} (encore jeune) se fait sentir. Chacun redéfinit le problème à sa manière. Toutes ces approches sont malgré cela structurellement très similaires  : schéma itératif avec résolution répétée de MDP et approximation de l'attribut moyen de la politique courante pour comparaison à celui de l'expert. Les approches les plus tardives font apparaître les difficultés soulevées par la recherche d'un politique mixée, d'une politique au lieu d'une récompense, de l'absence d'un critère commun, de la résolution répétée du MDP et de l'approximation de l'attribut moyen. Si de bonnes solutions aux deux premiers problèmes sont proposées, les autres soucis ne trouvent en revanche pas encore de réponse.
   
   Outre les applications variées développées au fur et à mesure par \citet{ratliff2009learning}, il est intéressant de constater l'efficacité des algorithmes de cette veine que \citet{abbeel2010autonomous} ont démontrée par le contrôle d'un hélicoptère radiocommandé en vol acrobatique. L'\gls{irl} ne joue cependant qu'un petit rôle dans cette expérience, où la contribution principale est l'apprentissage du modèle dynamique de l'hélicoptère. Ce modèle est ensuite utilisé pour la résolution du problème de l'\gls{rl}, et donc également de manière répétée lors de la résolution de l'\gls{irl}.


** Méthodes ne nécessitant pas la résolution répétée d'un \glsentrytext{mdp} 
   Les problèmes intrinsèquement liés à la structure itérative des premiers algorithmes d'\gls{irl} étant identifiés, un regain de créativité a permis l'émergence d'approches beaucoup plus hétérogènes ces dernières années.

   Un schéma itératif (mais sans lien évident avec le schéma itératif classique de la section précédente) permet à \cite{levine2010feature} d'effectuer de la sélection d'attribut tout en apprenant la récompense, à l'aide d'une méthode semblable dans sa justification au \gls{boosting} opéré dans \citep{ratliff2007boosting}. La méthode de \citet{levine2010feature} semble cependant plus générique, et l'idée qu'un récompense "simple" est préférable y est enracinée. La tractabilité malheureusement n'est pas au rendez-vous.
   
   Présenté philosophiquement comme une approche supervisée de l'imitation, la méthode de \cite{melo2010learning} repose sur une notion particulière de distance dans un \gls{mdp}. Cette distance n'est pas uniquement basée sur la définition de l'espace d'état (comme l'est par exemple la distance euclidienne standard) mais prend en compte la dynamique du \gls{mdp} afin que deux états semblables pour le problème qui nous préoccupe (permettant d'accéder aux mêmes états en effectuant la même action) aient une distance nulle. Le calcul de cette distance cache la résolution du \gls{mdp} par des méthodes de \gls{dp}, l'on trouve le point fixe d'un opérateur par son application répétée. Bien que la récompense n'apparaisse pas explicitement dans l'exposition de la méthode, le raisonnement qui définit l'échantillonnage interactif et la politique de sortie de l'algorithme est similaire a ce qui a été proposé dans \cite{lopes2009active}.

   Raisonnant selon la même logique bayésienne, mais présentant la particularité de ne pas faire appel à une approximation linéaire de la fonction de récompense, le travail de \cite{levine2011nonlinear} utilise des \gls{gp} dans une approche itérative (appelée \gls{gpirl}) qui apprend en même temps la fonction de récompense et le noyau permettant d'en représenter la structure. Elle souffre d'un problème de tractabilité que les auteurs résolvent en partie en proposant une version échantillonnée sur un partie de l'espace d'état, ce qui induit d'autres problèmes, notamment au niveau de la qualité de la généralisation. Cette approche pourrait facilement être augmenté d'un schéma d'échantillonnage actif en raison de la présence de l'information d'incertitude dans les \gls{gp}.

   Les \gls{gp} sont aussi présents chez \cite{qiao2011inverse}, dans une approche également bayésienne mais beaucoup plus analytique. Il ne s'agit pas simplement de calculer un maximum \emph{a posteriori} sur une classe de fonction de récompense : le modèle bayésien est sémantiquement plus riche (et donc malheureusement plus lourd), chaque choix de l'expert rajoute une relation dans un modèle bayésien sur les $Q$ valeurs. Cette approche semble ne pas souffrir des problèmes de généralisation qui pénalisent \gls{gpirl}, mais paraît encore moins tractable et est réservée aux \gls{mdp} discrets.

   L'algorithme proposé par \cite{dvijotham2010inverse} remplace la résolution du problème de l'\gls{rl} par l'inversion d'un \gls{mdp} soluble linéairement. Les résultats obtenus sont aussi bon ou meilleurs que ceux obtenus avec les algorithmes itératifs classique, et le sont plus rapidement. L'astuce réside dans le fait que pour convertir un \gls{mdp} arbitraire en \gls{mdp} soluble linéairement, il faut connaître les probabilités de transition.

   Finalement, \gls{relent} de \citet{boularias2011relative} reste dans la lignée des algorithmes itératifs classiques en ceci que son argumentaire ressemble à celui qui amène \gls{maxent} et qu'il tient compte des progrès fait dans la définition du problème de l'\gls{irl} : il s'agit de trouver une récompense (non un politique), sans accès complet à la dynamique du système (mais en ayant accès à des trajectoires) et en observant la politique de l'expert (sans la connaître totalement). La méthode cependant est plus efficace, la version échantillonnée de l'algorithme est en effet capable de fonctionner en utilisant uniquement des échantillons de l'expert et des échantillons aléatoires. La résolution répétée du problème de l'\gls{rl} n'est plus nécessaire.

   Ces nouvelles approches abandonnent la structure itérative classique, évitant ainsi les problèmes qu'elle implique. Elles profitent des avancées dans la définition du problème pour se concentrer sur de meilleures méthodes de résolution. L'avancée la plus prometteuse est de ne plus avoir à résoudre le problème de l'\gls{rl} de manière répétée, ce qui d'un point de vue technique rendait l'algorithme lent et gourmand en données, et d'un point de vue théorique faisait apparaître des termes d'erreur mal maîtrisés.

* Calcul de l'attribut moyen : \glsentrytext{lstdmu} 
# <<hier:lstdmu>>
  De par la sémantique dont il est porteur, explorée en et illustrée en \autoref{hier-attribut}, et de par le rôle qu'il tient dans les algorithmes itératifs énumérés en \autoref{hier-repetee}, l'attribut moyen est une notion centrale en \gls{irl}.
 
  Malgré cela, la littérature est étrangement silencieuse quant à la manière de l'estimer. Nous verrons en \autoref{hier-lstdmuavantages} qu'une des raisons de ce silence réside sans doute dans le fait que l'estimation de l'attribut moyen n'est pas le problème principal des algorithmes d'\gls{irl} itératifs. Cela est illustré en étudiant l'impact de la première (chronologiquement) contribution de cette thèse, qui consiste précisément en une méthode d'estimation de l'attribut moyen. Cette méthode est exposée (avec ses concurrentes) en \autoref{hier-lstdmuprincipe}. La pertinence de cette méthode dans le cadre ouvert par les approches récentes de l'\gls{irl} est finalement étudiée en \autoref{hier-lstdmuonpolicy}, qui présage des défis relevés par les contributions des \autoref{hier-scirl}, \autoref{hier-cascading}.

** Principe 
# <<hier:lstdmuprincipe>>

   Les approches discutées en \autoref{hier-repetee} font toutes appel, du fait de leur structure commune, à une procédure d'estimation de l'attribut moyen de l'expert ainsi que, de manière répétée, à une procédure d'estimation de l'attribut moyen d'une politique arbitraire $\policy$ :
\begin{eqnarray}
\rlmu{\policy} &\in& \twosetsfunctions{\statespace \times \actionspace}{(\reals^{\dimpsi})}\\
\rlmu{\policy}(\state,\action)&=&\expectationknowing{\sum\limits_{\timeindex = 0}^{\infty} \discount^{\timeindex}\featurestatefunc(\state_{\timeindex})}{\state_0 = \state}\\
\textrm{avec }\forall \timeindex \in \llbracket 2;T\rrbracket, \state_{\timeindex} &\sim& \transprobfunceval{\state_{\timeindex-1}}{\policy(\state_{\timeindex-1})}{\cdot},\\
\textrm{et }\state_{1} &\sim& \transprobfunceval{\state_{0}}{\action}{\cdot}.\\
\end{eqnarray}

# ?s muhorizon 'L' 'Longueur d{\apos}une trajectoire'
# ?s munbtraj 'M' 'Nombre de trajectoires'
   Quand elle est précisée, la méthode préconisée pour le calcul de l'attribut moyen est l'utilisation d'un simulateur pour obtenir les échantillons nécessaires à une estimation de Monte-Carlo comme cela est formalisé dans l'\autoref{alg:mcmu}. Cet estimateur non biaisé possède de surcroit l'avantage d'être simple à implémenter… pour peu que l'on dispose d'un simulateur.

   L'horizon $\muhorizon$ après lequel on tronque la trajectoire peut-être choisi en fonction du facteur d'amortissement $\discount$ : au bout d'un moment le terme $\discount^{\timeindex}$ en facteur de l'attribut devient négligeable, poursuivre le calcul de la somme n'a donc plus d'intérêt. Le nombre de trajectoire $\munbtraj$ est le fruit d'un compromis entre variance et temps de calcul.

   Il est à noter que la méthode est sans biais, pour peu que le simulateur soit fidèle à la réalité. Dans le cas contraire, l'on obtient un estimateur non biaisé d'une grandeur ne correspondant pas à ce qu'on cherche. Il reste toujours la possibilité d'utiliser des données en provenance d'un système réel, mais cela implique de pouvoir placer le système dans un état arbitraire et de laisser une politique potentiellement très mauvaise aux manettes sans provoquer de casse, le tout pour un coût en temps ou ressources raisonnable.

\begin{algorithm}
\Entree{Le couple $(\state,\action)$ en lequel l'on souhaite évaluer $\rlmu{\policy}$\;}
\Donnees{La politique $\policy$ et un simulateur du système à contrôler\;}
\Res{\rlmu{\policy}(\state,\action)\;}
$\satrace{\policy} \leftarrow \varnothing$\;
\SetKwFor{nfois}{répéter}{fois}{fin répéter}
\nfois{$\munbtraj$}{
Simuler une trajectoire $D$ de longueur $\muhorizon$, commençant par $\state$ et $\action$, puis suivant $\policy$ par la suite\;
$\satrace{\policy} \leftarrow \satrace{\policy}\cup D$\;
}
\Retour{\begin{equation*}
{1\over \munbtraj}\sum_{\datasetindex = 1}^{\munbtraj}\sum\limits_{\timeindex = 0}^{\muhorizon-1} \discount^{\timeindex}\featurestatefunc(\state_{\timeindex})
\end{equation*}}
\caption{Estimation de $\rlmu{\policy}$ par une méthode de Monte-Carlo}
\label{alg:mcmu}
\end{algorithm}


Si l'on compare la définition de l'attribut moyen (\autoref{eq:mudefsum}) que nous venons de rappeler à la définition de la fonction de valeur (\autoref{eq:Vdefsum}), l'on constate que les deux expressions sont quasiment identiques. La fonction de valeur est définie par l'espérance d'un cumul $\discount$-pondéré de récompenses, tandis que l'attribut moyen est l'espérance d'un cumul $\discount$-pondéré d'attributs. La sémantique des ces deux objets n'est pas la même puisque l'attribut ne porte généralement pas d'information immédiatement exploitable quant à la qualité du contrôle mais, mathématiquement, les différences sont minimes. La seule nuance entre l'attribut moyen et la fonction de valeur est la dimension. La fonction de valeur travaille sur une récompense réelle tandis ce qui est sommé dans l'attribut moyen est une grandeur vectorielle (de dimension $\dimpsi$).

Si l'on considère l'attribut moyen composante par composante, alors cette dissimilitude s'estompe. La composante $\dimindex$ de l'attribut moyen est exactement le même objet que la fonction de valeur pour la récompense donnée par la $\dimindex$-ième composante de l'attribut $\featurestatefunc$ :
\begin{equation}
\component{(\rlmu{\policy})}{\dimindex} = \rlvalue{\policy}{\component{\featurestatefunc}{\dimindex}}.
\end{equation}
Sachant cela, il paraît naturel, pour calculer l'attribut moyen d'une politique, d'adapter les techniques existantes permettant de calculer la valeur de cette politique. Nous n'aurons comme nous allons le voir même pas à utiliser la décomposition en composante comme nous venons de le faire, le fait que l'expression soit vectorielle plutôt que scalaire ne posant pas de problèmes insurmontables.

   
Dans le cas idyllique où l'espace d'état est discret de faible cardinal et la matrice des probabilités de transition $\transprobmat{\policy}$ induites par la politique $\policy$ est connue, un algorithme d'évaluation de la politique tiré de la \gls{dp} convient tout à fait à l'estimation de l'attribut moyen. Par application répétée de l'opérateur d'évaluation de Bellman $\bellmanevalopeval{\policy}{\component{\featurestatefunc}{\dimindex}}{}$ (\autoref{eq:bellmanevop}), l'on peut arbitrairement se rapprocher de la valeur exacte de l'attribut moyen, comme l'explicite l'\autoref{alg:dpmu}.

# FIXME: epsilon n'est pas dans le glossaire
# FIXME: Delta non plus

\begin{algorithm}
\Entree{La matrice $\transprobmat{\policy}$ des probabilités de transition $\transprobfunceval{\state}{\policy(\state)}{\state'}$\;
La fonction d'attribut $\featurestatefunc$\;
Un critère d'arrêt $\epsilon$\;} 
\Res{Une approximation à $\epsilon$ près de $\rlmu{\policy}$\;}
Choisir $\mu$ arbitrairement dans $\twosetsfunctions{\statespace}{(\reals^{\dimpsi})}$\;
\Tq{$\Delta > \epsilon$}{
$\Delta \leftarrow 0$\;
\PourCh{$\state \in \statespace$}{
$\mu' \leftarrow \mu(\state)$\;
$\mu(\state)\leftarrow \featurestatefunc(s) + \discount\sum\limits_{\state'\in \statespace}\transprobfunceval{s}{\policy(s)}{\state'} \mu(s')$\;
$\Delta \leftarrow \max(\norm{2}{\mu'-\mu(\state)},\Delta)$\;
}
}
\caption{Estimation de $\rlmu{\policy}$ par une méthode de \gls{dp}}
\label{alg:dpmu}
\end{algorithm}

Maintenant que le principe d'estimer l'attribut moyen par une méthode de calcul de la fonction de valeur est établi, voyons comment il est possible d'adapter non plus un algorithme de \gls{dp} mais un algorithme d'\gls{rl}. La \gls{dp} suppose connues les probabilités de transition. Cela est en général difficile à réaliser sur les systèmes réels, notamment les systèmes robotiques. Le passage à un algorithme d'\gls{rl} permettrait d'apprendre l'attribut moyen au travers d'interactions avec le système réel, en minimisant le travail d'ingénierie.

Un algorithme d'estimation de la fonction de valeur particulièrement approprié est \gls{lstd} de \citet{bradtke1996linear}. Dans sa variante proposée par \citet{lagoudakis2003least}, \gls{lstd}-$Q$, il permet d'estimer la fonction de qualité d'une politique de manière \gls{batch} et \gls{offpolicy}. Cela signifie que le fonctionnement de l'algorithme est complètement découplé de la commande du système. \gls{lstd}-$Q$ peut en effet fonctionner à partir de données recueillies une fois pour toutes (c'est l'aspect \gls{batch}, ou /par lots/) par une politique qui n'est pas celle que l'on évalue (c'est l'aspect \gls{offpolicy}).

Cet algorithme, comme beaucoup d'autres en \gls{rl} repose sur une approximation linéaire de la fonction de qualité (\autoref{eq:paramrl}). Il nous va donc falloir utiliser un schéma d'approximation linéaire pour estimer l'attribut moyen qui lui même est issu de la définition d'un schéma d'approximation linéaire pour la récompense[fn::Yo dawg, I heard you like linear approximation schemes...] (\autoref{eq:mudef}). Pour éviter les conflits de notations, on suppose ici que la récompense est approximée selon le schéma de l'\autoref{eq:paramirl} :
\begin{equation}
\appr{\rewardfunc}_{\paramirl}(\state) = \transpose{\paramirl}\featurestatefunc(\state).
\end{equation}
# ?s paramlstdmu '\zeta' 'Matrice de paramètres pour LSTD-$\mu$'
L'attribut moyen est donc l'espérance du cumul $\discount$-pondéré de l'attribut $\featurestatefunc$. Nous allons l'approximer dans un espace d'hypothèse généré par un autre attribut : $\featurestateactionfunc$ :
\begin{equation}
%\appr{\rlmu{\policy}}_{\paramlstdmu}(\state,\action) = \transpose{\paramlstdmu}\featurestateactionfunc(\state,\action).
\hat{\mu}^\pi_{\zeta}(\state,\action) = \transpose{\paramlstdmu}\featurestateactionfunc(\state,\action).
\label{eq:paramlstdmu}
\end{equation}

Ceci posé, l'adaptation de l'algorithme est relativement simple, le calcul de la matrice de paramètres $\paramlstdmu$ est décrit \autoref{alg:lstdmu}. La principale différence avec \gls{lstd}-$Q$ réside dans la dimension du vecteur de paramètres. Pour \gls{lstd}-$Q$, il s'agit d'un vecteur de dimension $\dimphi\times 1$, tandis que pour \gls{lstdmu}, il s'agit d'une matrice de taille $\dimphi \times \dimpsi$. Pour l'évaluation de $\hat\mu^{\pi}_\zeta$ en un couple $(\state,\action)$, il suffit d'utiliser l'\autoref{eq:paramlstdmu}.

# ?s reg '\lambda' 'Coefficient de régularisation lors d{\apos}une inversion de matrice'

\begin{algorithm}
\Entree{Un base de données $\sasrtrace{\featurestatefunc}$\;
Une fonction d'attribut $\featurestateactionfunc$\;
La politique $\policy$\;
Un paramètre de régularisation $\reg$\;}
\Res{La matrice $\paramlstdmu$ de paramètres pour l'approximation linéaire de l'attribut moyen\;}
$A\leftarrow 0$\tcc*{Matrice de taille $\dimphi\times\dimphi$}
$b \leftarrow 0$\tcc*{Matrice de taille $\dimphi\times\dimpsi$}
\PourCh{$\state,\action,\state',\featurestatefunc(\state)\in\sasrtrace{\featurestatefunc}$}{
\nllabel{alg:lstdmu:A}$A\leftarrow A+\featurestateactionfunc(\state,\action)\transpose{\left(
\featurestateactionfunc(\state,\action) - \discount \featurestateactionfunc(\state',\policy(\state'))
\right)}$\;
\nllabel{alg:lstdmu:b}$b\leftarrow b + \featurestateactionfunc(\state,\action)\transpose{\featurestatefunc(\state)}$\;
}
$\paramlstdmu \leftarrow (A+\reg Id)^{-1}b$\;
\Retour{$\paramlstdmu$}
\caption{Estimation de $\rlmu{\policy}$ par \gls{lstdmu}}
\label{alg:lstdmu}
\end{algorithm}



** Avantages dans le cadre des approches existantes
# <<hier:lstdmuavantages>>
   
   Dans le schéma itératif standard des premiers algorithmes d'\gls{irl} (\autoref{alg:repetee}), les problèmes techniques se cachent dans les étapes des \autoref{alg:l:mdpsolver} et \autoref{alg:l:musolver}, à savoir la résolution du \gls{mdp} et l'estimation de l'attribut moyen. Au delà de l'aspect computationnel, qui pose de moins en moins de problème à mesure que le temps passe et que la loi de \citet{moore1965cramming} nous offre des machines dont la puissance de calcul croit exponentiellement, le goulot d'étranglement réside dans la collecte des données nécessaire à la résolution de ces deux étapes.

   Les auteurs des approches mentionnées en \autoref{hier-repetee} présupposent la disponibilité d'un simulateur. Dans bien des cas, la conception de ce simulateur représente un défi. Par exemple dans les travaux déjà mentionnés de \citet{abbeel2010autonomous}, l'apprentissage du modèle est si peu évident qu'il est l'objet principal de la contribution.

# ?cs 1 sastrace 'D_{sas}^{#1}' 'D_{sas}^{\policy}' 'Trace de type $s,a,s\dash$ obtenue en suivant la politique $\pi$'
   Dans les cas où il est impossible ou incommode de construire un tel simulateur, recueillir des données reste cependant souvent faisable. Nous nous plaçons dans le cas où l'expert peut contrôler le système, amassant ainsi une démonstration experte $\sastrace{\expertpolicy}$ tout en pouvant aller explorer d'autres zones de l'espace d'état que celles dans lesquelles il souhaite se trouver, soit que l'on perturbe sa démonstration de manière aléatoire, soit qu'il agisse volontairement de manière sous optimale dans un but d'exploration. Ces données non expertes sont recueillies dans une trace $\sastrace{\sim}$. Il s'agit là d'une structure extrêmement contrainte, mais qui correspond par exemple aux problèmes de locomotion robotique ou d'Interaction Homme-Machine où les modèles sont imparfaits et la collecte de données coûteuse.

   Dans le cadre de l'\gls{rl}, l'approche de \citet{lagoudakis2003least} peut fonctionner dans avec de telles contraintes. Elle découple, comme nous l'avons dit dans la \autoref{hier-lstdmuprincipe}, la collecte de données et la résolution du \gls{mdp}. L'aspect de traitement par lot, couplé à l'aspect \gls{offpolicy}, permet de réutiliser tous les échantillons pour évaluer une politique différente de celle qui était au commandes du système lors de la collecte. L'algorithme \gls{lspi}, qui utilise $\gls{lstd}$-$Q$ dans un schéma d'itération de la politique permet donc de résoudre un \gls{mdp} à partir d'une trace $\sasrtrace{\rewardfunc}$. Comme, pour créer \gls{lstdmu}, nous nous sommes inspirés de \gls{lstd}-$Q$, les mêmes avantages sont transférés à l'estimation de l'attribut moyen. Nous pouvons donc facilement imaginer porter le schéma itératif classique de l'\gls{irl} décrit dans l'\autoref{alg:repetee} à un cadre où la collecte de données est coûteuse. C'est ce que présente l'\autoref{alg:repeteelstd}.

\begin{algorithm}
\Entree{
Une démonstration $\sastrace{\expertpolicy}$ de la politique expert $\expertpolicy$\;
Un critère d'arrêt $\epsilon$\;}
\Donnees{
Une notion de distance $\mudistance$ permettant d'évaluer la similarité de deux attributs moyens\;
Une règle de mise à jour $\muupdate$ permettant de réduire la distance\;
{\color{blue}Des données non-expertes $\sastrace{\sim}$}\;
}
\Sortie{Un vecteur de paramètres $\paramirl$ pour la récompense $\appr{\rewardfunc}_{\paramirl}$}
Calculer l'attribut moyen de l'expert $\expertmu$ à partir de la démonstration\;
Initialiser $\theta$ arbitrairement\;
\Tq{$\mudistance(\expertmu,\appr{\rlmu{\policy}}) > \epsilon$}{
{\color{blue}
Construire la trace $\sasrtrace{\hat\rewardfunc_{\paramirl}}$ à partir de $\sastrace{\sim}$ et $\hat\rewardfunc_{\paramirl}$\;
Construire la trace $\sasrtrace{\featurestatefunc}$ à partir de $\sastrace{\sim}$ et $\featurestatefunc$\;
$\policy \leftarrow LSPI(\sasrtrace{\hat\rewardfunc_{\paramirl}})$\;
$\appr{\rlmu{\policy}} \leftarrow LSTD\mu(\sasrtrace{\featurestatefunc})$\;}
$\theta \leftarrow \muupdate(\expertmu,\appr{\rlmu{\policy}})$\;
}
\Retour{$\theta$}

\caption{Instanciation du schéma itératif de l'\gls{irl} avec des méthodes \gls{lstd} (en bleu les changements par rapport à l'\autoref{alg:repetee})}
\label{alg:repeteelstd}
\end{algorithm}

L'estimation de l'attribut moyen par \gls{lstdmu} est bien sûr imparfaite. Une erreur trop grande empêcherait l'algorithme d'\gls{irl} de converger. Dans le schéma de l'\autoref{alg:repeteelstd}, les sources d'erreurs principales sont l'estimation de l'attribut moyen par \gls{lstdmu} et la résolution du \gls{mdp} par \gls{lspi}. Les erreurs introduites par ces deux routines dépendent de la qualité et de la quantité des données contenues dans la trace $\sastrace{\sim}$ de données non expertes. Si le comportement et les limites de \gls{lspi} sont plutôt bien connues, il n'en est pas de même pour \gls{lstdmu}. Il existe des analyses en échantillons finis de \gls{lstd} et \gls{lspi} : \citep{lazaric2010finiteLSTD,lazaric2010finiteLSPI}, mais elles ne se transfèrent pas de manière évidente à \gls{lstdmu}.

Intuitivement cependant, on peut argumenter que le schéma d'itération de la politique qu'est \gls{lspi}, qui implique donc l'évaluation répétée de politiques arbitraires, sera plus problématique qu'une simple évaluation de la politique (même sur plusieurs composantes) comme \gls{lstdmu}. Une expérience permet de confirmer empiriquement cette intuition (elle fut pour la première fois présentée dans notre article \citep{klein2011batch}). Le schéma proposé dans l'\autoref{alg:repeteelstd} (avec $\mudistance$ et $\muupdate$ choisis conformément aux travaux de \citet{abbeel2004apprenticeship}) est comparé à un autre schéma qui, toutes choses étant égales par ailleurs, fait appel à un oracle pour l'estimation de $\appr{\rlmu{\policy}}$. Le problème jouet (le pendule inverse FIXME:expliquer ou refaire exp sur mountain car) sur lequel ces deux approches sont testées permettant de définir un simulateur, l'oracle est un estimateur de Monte-Carlo tel que décrit par l'\autoref{alg:mcmu} dont les paramètres $\muhorizon$ et $\munbtraj$ sont réglés de manière à obtenir une variance négligeable.

La comparaison des deux approches va porter sur leur comportement en fonction de la taille de la trace non experte $\sastrace{\sim}$. Le résultat de l'expérience est donné \autoref{fig:lstdmuexp}. On constate que la taille de $\sastrace{\sim}$ est bien le facteur déterminant de la qualité de l'imitation. On constate surtout que les deux approches manifestent le même comportement. Cela signifie que quelle qu'ait pu être l'erreur introduite par l'utilisation de \gls{lstdmu}, elle est gommée par les besoins de \gls{lspi} en données. Quand ce dernier dispose de suffisamment de données pour résoudre le \gls{mdp} suffisamment bien pour que l'algorithme d'\gls{irl} converge vers une bonne solution, \gls{lstdmu} possède suffisamment d'information pour fournir une estimation de l'attribut moyen qui pour l'usage qui en est fait est aussi bonne que celle de l'oracle.

\begin{figure}
\centering
\subfigure[Moyenne et écart-type sur 100 expériences]{
\centering
\includegraphics[width=.8\linewidth]{Figures/LSTDMU_stat.pdf}
}
\subfigure[Une seule expérience]{
  \centering
  \includegraphics[width=.8\linewidth]{Figures/LSTDMU_once.pdf}
}
\caption{Comparaison de deux approches itératives de l'\gls{irl} sur le problème du pendule inversé. L'une est décrite \autoref{alg:repeteelstd} (LSTD sur les courbes), l'autres est identique sauf en ce qui concerne l'estimation de l'attribut moyen, qui est faite par un oracle (MC sur les courbes). On constate que les deux approches ont le même comportement. Dès lors que la quantité de données aléatoire est suffisante pour que \gls{lspi} résolve le \gls{mdp} correctement, l'algorithme d'\gls{irl} converge vers une bonne solution. Cela démontre empiriquement que les erreurs introduites par la résolution répétée du \gls{mdp} sont plus importantes que celles liées à l'estimation de l'attribut moyen par notre algorithme \gls{lstdmu}.}
\label{fig:lstdmuexp}
\end{figure}


Le goulot d'étranglement des approches itératives est donc bien la résolution répétée du \gls{mdp}, il importe de faire sauter cette contrainte si l'on souhaite pouvoir utiliser l'\gls{irl} sur des problèmes complexes sans avoir à déployer des trésors d'ingénierie.

Un algorithme de ce tonneau là est l'approche de \citet{boularias2011relative}, \gls{relent}, qui peut fonctionner dans le même cadre contraint que celui que nous venons de décrire, à savoir dans le cadre où les seules données dont on dispose sont une trace de l'expert $\sastrace{\expertpolicy}$ et une trace non experte $\sastrace{\sim}$. Il serait cependant souhaitable d'aller encore plus loin et de pouvoir résoudre le problème de l'\gls{irl} armé uniquement de données expertes. Ce sont en effet les données les plus faciles à rassembler.

** \glstext{onpolicy} : \glstext{lstdmu} ou Monte-Carlo ?
# <<hier:lstdmuonpolicy>>

   Comme nous le verrons aux \autoref{hier-scirl} et \autoref{hier-cascading}, il est possible de résoudre le problème de l'\gls{irl} en utilisant uniquement des données expertes. Plus particulièrement, l'approche présentée au \autoref{hier-scirl} a besoin pour fonctionner de connaitre l'attribut moyen de l'expert en certains points de l'espace d'état-action. L'on ne se placera pas dans le cas de la \autoref{hier-lstdmuavantages}, où le but est d'estimer l'attribut moyen d'une politique arbitraire à l'aide de données échantillonnées par une autre politique. Il s'agira de faire une estimation \gls{onpolicy}, c'est à dire où la politique ayant servi à l'échantillonnage est la même que celle dont on cherche l'attribut moyen. Il n'y aura pas non plus de résolution de \gls{mdp}, l'erreur d'estimation de l'attribut moyen devient donc à nouveau un potentiel point problématique.

   Pour minimiser cette erreur d'estimation de l'attribut moyen, l'on peut se poser la question du choix de la méthode. Si l'on dispose des probabilités de transitions, l'\autoref{alg:dpmu} est tout indiqué. S'il est possible d'utiliser un simulateur, c'est la méthode décrite \autoref{alg:mcmu} qui sera employée. Si l'on fonctionne dans le cadre contraint à une base données experte et une base non experte, alors \gls{lstdmu} est à notre connaissance la seule méthode capable de fonctionner en exploitant uniquement les deux bases. Le but des approches présentées plus loin dans ce manuscrit étant de fonctionner dans un cadre où la collecte de données est très difficile, nous allons étudier le cas où seules les données de l'expert sont disponibles.

   Comme nous le préciserons au prochain chapitre, ce que nous souhaitons estimer, étant donnée une base d'entraînement $\sastrace{\expertpolicy}$, est l'attribut moyen évalué en chacun des états présents dans la base, pour toute action de l'espace d'action $\actionspace$, à savoir l'ensemble 
\begin{equation}
\{\expertmu(\state, \action)|\exists\datasetindex,\state =\state_{\datasetindex}\textrm{ et }\state_{\datasetindex},a_{\datasetindex},\state'_{\datasetindex}\in\sastrace{\expertpolicy},\forall a\in \actionspace\}.
\end{equation}

# ?s spaceindex 'm' 'Entier indexant un ensemble'
Dans certains \gls{mdp}, il peut arriver que \gls{lstdmu} soit l'approche idéale pour cette approximation. Il faut que les attributs utilisés pour l'approximation de l'attribut moyen (dénotés $\featurestateactionfunc$ dans l'\autoref{alg:lstdmu}) aient une cohérence spatiale, c'est à dire que $\featurestateactionfunc(\state_1,\action)$ soit proche de $\featurestateactionfunc(\state_2,\action)$ si $\state_1$ et $\state_2$ sont proches, et que pour tout couple $(\state,\action)$ où une estimation de l'attribut moyen est nécessaire, l'expert ait également fourni des échantillons $(\state_{\spaceindex},\action_{\spaceindex},\state'_{\spaceindex})$ pour toutes les actions $\action_{\spaceindex} \neq \action$, avec $\state_{\spaceindex}$ proche de $\state$. Dans ce cas, les échantillons $(\state_{\spaceindex},\action_{\spaceindex},\state'_{\spaceindex})$ permettent à \gls{lstdmu} d'inférer une valeur acceptable pour $\appr{\rlmu{\expertpolicy}}(\state, \action_{\spaceindex}\neq \action)$. L'on peut s'en convaincre en remarquant que les mises à jour des \autoref{alg:lstdmu:A} et \autoref{alg:lstdmu:b} de l'\autoref{alg:lstdmu} sont proches pour $(\state,\action_{\spaceindex},\state')$ (qui n'est pas dans les données) et pour $(\state_{\spaceindex},\action_{\spaceindex},\state'_{\spaceindex})$ qui lui s'y trouve, pour peu que la fonction d'attribut $\featurestateactionfunc$ ait une cohérence spatiale et que la dynamique du système ne soit pas trop irrégulière. Le pendule inversé est un exemple de ce type de \gls{mdp}, l'expert reste dans une zone restreinte près du point d'équilibre, les attributs standards (un maillage de gaussiennes) possède la propriété de cohérence spatiale nécessaire et l'expert emploie dans cette petite zone les trois actions qui composent l'espace d'état.

Dans les autres cas, il faudra avoir recours à une méthode /ad-hoc/ que nous expliciterons et justifierons au prochain chapitre.

* \glsentrytext{scirl}
# <<hier:scirl>>  

# snippet: tiré de ratliff2009learning, l'extrait suivant est exactement ce que SCIRL fait : Unfortunately, such a system is inherently myopic. This form of imitation learning assumes that all information necessary to make a decision at the current state can be easily encoded into a single vector of features that enables prediction with a relatively simple, learned function. However, this is often particularly difficult because the features must encode information regarding the future long-term consequences of taking an action. Under this framework, much of the work that we would like learning to handle is pushed onto the engineer in the form of feature extraction.
--> On a vu que les approches existantes restent limitées parla résolution du MDP, mais que dès que ce problème est réglé elles peuvent tourner. Si on a pas assez de données pour résoudre le MDP, alors si on a besoin d'estimer mu, on a des stratégies. Etudions maintenant cet algo qui n'a pas besoin de résoudre le MDP.

# snippet : copier du papier nips le truc qui explique qu'une récompense nule implique une erreur de classif de batard.
** Liens entre classification et \glsentrytext{rl}
  La classification peut-être utilisée pour faire de l'imitation (fait mentionné en sous-section \autoref{hier-nonari}). Cela à l'avantage de ne nécessiter que des données de l'expert. Mais cela ne tient pas compte de la structure du MDP. La plupart des classifieurs apprennent une fonction de score [fn::Les arbres sont une exception.]. De fait la règle de décision du classifieur et la règle de décision d'un agent optimal dans un \gls{mdp} (équation présentée en \autoref{hier-cadre}) sont similaires. On peut donc dresser un parallèle entre la fonction de score du classifieur et la fonction de qualité de l'expert.

  \gls{scirl} et \gls{cascading} (décrit en \autoref{hier-cascading}) utilisent cette similarité pour introduire la structure du MDP dans (ou à la suite de) une méthode de classification. On espère ainsi pouvoir faire de l'\gls{irl} (trouver une récompense, pas apprendre une politique par copie) tout en profitant des avantages offerts par la méthode supervisée (efficacité en termes de données, implémentations /off-the-shelf/, etc.).

  Si l'on utilise un classifieur où cette fonction de score/qualité est approximée par un schéma linéaire, alors on retombe sur l'attribut moyen. Il faut encore approximer celui-ci, mais cela est courant dans la littérature, et surtout c'est précisément le problème résolu par \gls{lstdmu} (en [[hier:lstdmu]]). 

** Description 
** Validation théorique 
** Validation pratique rapide (problèmes jouets) 
*** TODO Donner la structure : qu'est-ce que chaque application démontre ?
** Mise en relief de l'influence des routines 
** Conclusion 
   SCIRL règle quelques problèmes des premiers algorithmes d'IRL et a de meilleures performances que \gls{maxent}. Il est théoriquement simple à expliquer et résout bien le problème de l'\gls{irl} tel qu'on l'a formulé. Il peut fonctionner avec uniquement des données de l'expert, ce qui ouvre la porte à des applications réelles.

* SCIRLBoost 
** Problème de la définition des fonctions de base  
** Description de l'algorithme 
** Validation sur les problèmes jouets 
* \glsentrytext{cascading} 
# <<hier:cascading>>
** Description 
*** Description 
*** Différences de concept avec \glsentrytext{scirl} 
** Validation théorique 
** Validation pratique rapide (problèmes jouets) 
** Mise en relief de l'influence des routines 
** Comparaison pratique rapide de \glsentrytext{scirl} et \glsentrytext{cascading} 
** Conclusion 
   Cascading a des performances similaires à SCIRL et est encore plus flexible, puisque des méthodes supervisées non paramétriques (ou à détermination automatique de paramètres) peuvent être employées, ce qui en plus de résoudre les problèmes structurels de PIRL, résout les problèmes plus fondamentaux de l'approximation linéaire de la fonction de valeur ou du choix des attributs.
* (Validation expérimentale) 
* Rappel des contributions 
* Perspectives de recherche 
* Bibliographie 
\bibliographystyle{plainnat}
\bibliography{../../Biblio/Biblio}
* Glossaire 
\printglossaries
