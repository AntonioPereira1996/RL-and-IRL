#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \usepackage{natbib}

* Notations
** Conventions
   Bold symbols represent the set or the space of the elements represented by a symbol in normal font. e.g., $\mathbf T$ is the space of all transitions, and a transition is noted $T$. The dimensionality of such a space (or any other space) is usually noted $n_X$, e.g. the dimension of the state space $S$ is noted $n_S$.


   A integer index often indicates a temporal order (or just an order) for example $s_i \in \{s_0, s_1, \dots\}$ for consecutive states in a trajectory.


   Linear approximations are denoted thanks to the character $\hat\dot$. e.g., the linear approximation of $V$ will be noted $\hat V$.

   
   Notions relative to a policy $\pi$ are noted with $\pi$ in exponent : e.g., $V^\pi$ for the value function of the policy $\pi$. When the policy is not available but some samples $D$ are, the value for $V$ computed from these samples is noted $V^D$. Notions relative to the optimal policy with respect to a given reward $R$ can be noted with $R$ in exponent (e.g. $\mu^R$) or, if $\hat R$ is an approximation of vector of parameters $\theta$, with $\theta$ in exponent e.g., $\mu^\theta$.


   The $i$-th component of a vectorial value $X$ is noted $X^i$.


   $X'$ denotes an element that is timewise one step after $X$.
** List of used symbols
   | $S$                                 | State space                                  | \ref{S.def}      |
   | $n_S$                               | Dimension of $S$                             | \ref{S.def}      |
   | $A$                                 | Action space                                 | \ref{A.def}      |
   | $n_A$                               | Dimension of $A$                             | \ref{A.def}      |
   | $R$                                 | A reward function                            | \ref{R.def}      |
   | $\pi$                               | A deterministic policy                       | \ref{pi.def}     |
   | $T_i$                               | A single transition                          | \ref{Ti.def}     |
   | $\mathbf{T}$                        | The transition space                         | \ref{Ti.def}     |
   | $T_i^R$, $T_i^\theta$               | An augmented transition                      | \ref{TiR.def}    |
   | $\mathbf{T}^R$, $\mathbf{T}^\theta$ | The augmented transition space               | \ref{TiR.def}    |
   | $\xi_j$                             | A trajectory                                 | \ref{xi.def}     |
   | $\Xi$                               | The trajectory space                         | \ref{xi.def}     |
   | $\xi_j^R$, $\xi_j^\theta$           | An augmented trajectory                      | \ref{xi.def}     |
   | $\Xi^R$, $\Xi^\theta$               | The augmented trajectory space               | \ref{xi.def}     |
   | $D$                                 | A dataset                                    | \ref{D.def}      |
   | $\mathbf{D}$                        | Space of all the datasets                    | \ref{D.def}      |
   | $D^R$                               | An augmented dataset                         | \ref{D.def}      |
   | $\mathbf{D}^R$                      | Space of all the augmented datasets          | \ref{D.def}      |
   | $P$                                 | System dynamics                              | \ref{P.def}      |
   | $V^\pi$                             | State value function for policy $\pi$        | \ref{V.def}      |
   | $\gamma$                            | discounting factor                           | \ref{V.def}      |
   | $Q^\pi$                             | State-action value function for policy $\pi$ | \ref{Q.def}      |
   | $\hat V$, $\hat Q$, $\hat R$        | Linear approximations                        | \ref{approx.def} |
   | $\phi$, $\psi$                      | State(-action) space features                | \ref{approx.def} |
   | $n_\phi$, $n_\psi$                  | Dimension of State(-action) feature space    | \ref{approx.def} |
   | $\omega$, $\theta$                  | Weight vectors                               | \ref{approx.def} |
   | $\mu^\pi$                           | Feature expectation for policy $\pi$         | \ref{mu.def}     |
   | $M$                                 | A markov decision process                    | \ref{MDP.def}    |
** States, Actions, Reward
  #+begin_definition
  \label{S.def}
  $S$ is the state space, of cardinality $|S|$, of dimension $n_S$.
  #+end_definition
  
  #+begin_definition
  \label{A.def}
  $A$ is the action space, of cardinality $|A|$, of dimension $n_A$.
  #+end_definition

  The last two definition are pretty straightforward as my work does not deal (yet ?) with partial observability, belief states and so on.

  #+begin_definition
  \label{R.def}
  $R : S\rightarrow \mathbb{R}$ is a reward.
  #+end_definition

  The way to define rewards is not normalized. Basically the choices are :
  - Use a mapping $S\times S \rightarrow \mathbb{R}$. This is what is assumed in \citep{ng1999policy}.
  - Use a mapping $S \rightarrow \mathbb{R}$, this is what I argue for.
  - Hide everything else as a mapping $S\times A \times S\rightarrow \mathbb{R}$. This is the most mathematically general solution, as it encapsulates every other. The problem is it also is the most difficult to work with.

    
  I believe one should not take the action into account when computing the reward. First because the action is irrelevant, what the reward judges is the consequences of an action. No matter the means, the result is what is important. If an action is risky and has yeld a high reward by chance, this will likely not happen again, and the reward need not be lowered by the taking of a risky action as an upcoming transition will likely give the information that taking this action may not be a good idea. Second, in the case of IRL, recording the action can be difficult. It is possible to record the state an agent is in from an external point of view, but guessing the action from the outside is an supplementary engineering problem we shall get rid of if we can.\\

  
  As transitions follow each others, I believe there is no use in taking both states of the transition into account. The following state will be present in the next transition, so the associated reward can be computed then. Plus, needing both states to compute the reward is a sign that the states may not respect the markovian criterion.\\

  For this reasons, I argue for the reward to be a mapping from the states only to the reals.

** Policy
   #+begin_definition
   \label{pi.def}
   A deterministic policy $\pi$ is a mapping $S\rightarrow A$.
   #+end_definition

   For the time being, we only consider deterministic policies. If I recall correctly, there always is an optimal deterministic policy, so I don't want to bother with stochastic policies and the awful lot of engineering and mathematical problems they bring.
** Transition, trajectory, dataset
  #+begin_definition
  \label{Ti.def}
  $T_i = \{s_i,a_i,s_{i+1}\}\in \mathbf{T} = S\times A\times S$ is a /transition/.
  #+end_definition

  The reward is not included in the transition, as I believe there is a strong intrinsic difference between the tuple $T_i = \{s_i,a_i,s_{i+1}\}$ and the associated reward : the tuple is data stemming from the environment, which the operator do not control whereas the reward is usually defined by the operator.

   The augmented transition can be defined as :

  #+begin_definition
  \label{TiR.def}
  $T^R_i = \{s_i,a_i,s_{i+1},R(s_i)\}\in \mathbf{T}^R = S\times A\times S \times \mathbb{R}$ is an /augmented transition/.
  #+end_definition  
 
  #+begin_definition
  \label{xi.def}
  $\xi_j = \{T_i\}_i \in \Xi$ is a /trajectory/. $\xi_j^R = \{T_i^R\}_i \in \Xi^R$ is an augmented trajectory.
  #+end_definition

  #+begin_definition
  \label{D.def}
  $D = \{\xi_j\}_j$ is a dataset. $D^R= \{\xi_j^R\}_j$ is the augmented dataset. \\ 
  $\mathbf{D}$ is the space of datasets.
  #+end_definition

   By abusing the notation, when using an approximation framework $\hat R$ of vector of parameters $\theta$, the augmented item (for example a transition $T$) will be noted $T^\theta = T^{\hat R}$.

** System Dynamics
   #+begin_definition
   \label{P.def}
   The function $P : S\times A \times S \rightarrow [0;1],~ s,a,s' \mapsto P(s'|s,a)$ represents the system dynamics : is gives the probability of transitionning to state $s'$ when taking action $a$ in state $s$.
   #+end_definition
   
   #+begin_definition
   \label{Pmat.def}
   The system dynamics and the $P$ function can also be represented as a set of $|A|$ matrices (in the discrete, finite case) where element $i\times j$ of the matrix $\mathcal{P}_a$ is (assuming a numbering of the states) $P(s_j|s_i,a)$. A policy $\pi$ can be defined by a $\mathcal{P}_\pi$ matrix the $i$-th line of which is the $i$-th line of the $\mathcal{P}_{\pi(s_i)}$ matrix.
   #+end_definition
** Value function
   The definition of the value function I use is the widespread one using the sum of the discounted rewards :
   
   #+begin_definition
   \label{V.def}
   $V^{\pi} : S\rightarrow \mathbb{R}$, $s \mapsto E\left(\left.\sum\limits_{t=0}^{\infty} \gamma^tR(s_t)\right|s_0=s,\pi\right)$
   #+end_definition
   #+begin_definition
   \label{Q.def}
   $Q^{\pi} : S\times A\rightarrow \mathbb{R}$, $s,a \mapsto E\left(\left.R(s) + \gamma V^\pi(s')\right|s,a,\pi\right)$
   #+end_definition
  
** Features
   Tabular representation often fall short (notably for $Q$ and $V$), so approximation frameworks have to be used. A widespread approximation framework is the following (explicited for $V$ but usable on any function) :
   #+begin_definition
   \label{approx.def}
   A linear approximator for $V$ is $\hat V(s) = \omega^T \phi(s)$, with :\\
   - $\phi : S \rightarrow \mathbb{R}^{n_\mathbf{\phi}}$ a vectorial feature function
   - $\omega\in \mathbb{R}^{n_\mathbf{\phi}}$ a weight vector
   #+end_definition
** Feature expectation
   When one uses an approach based on an approximation of the reward :
   \begin{equation}
   \hat R(s) = \theta^T \phi(s)
   \end{equation}
   The next logical step is to introduce the feature expectation $\mu$.
   #+begin_definition
   \label{mu.def}
   $\mu^{\pi} : S\rightarrow \mathbb{R}^{n_\mathbf{\phi}}$, $s \mapsto E\left(\left.\sum\limits_{t=0}^{\infty} \gamma^t\phi(s_t)\right|s_0=s,\pi\right)$
   #+end_definition

** Markov Decision Process
#+begin_definition
\label{MDP.def}
  $M = \{S,A,P,\gamma,R\}$ is a Markov Decision Process (MDP).
#+end_definition

* RL
  \citep{doshi2008reinforcement} traite du risque dans les POMDP et présente une méthode sûre se basant sur un expert auquel l'agent peut faire appel en cas d'incertitude.
** /Model free/ RL
*** LSTD/LSPI

   LSPI est un algo de RL basé sur l'approximation de la fonction de valeur par LSTD. Tout cela est expliqué dans \citep{lagoudakis2003least}. Une analyse en échantillons finis de LSPI est fournie par \citep{lazaric2010finiteLSPI}, celle en échantillons finis de LSTD peut-être trouvée dans \citep{lazaric2010finiteLSTD}.
*** LAPI
    En changeant le cout quadratique de LSPI par un coût en valeur absolue, \citet{sugiyama2010least} dérive l'algorithme LAPI qui est plus robustes aux samples isolés, qui représentent des évènemnts de faible probabilité mais de grande récompense (robot qui casse, par exemple).
** Applications
   Le papier \citep{paek2006reinforcement} dresse la liste des avantages et inconvénients du RL appliqué aux systèmes de gestion de dialogues.
** LSTD($\lambda$)
   LSTD($\lambda$) est une méthode dérivée de LSTD, proposée dans \citep{boyan2002technical}, dont l'analyse a été fournie par \citep{nedic2003least}.
** $\lambda$ LSPE
   Un algo d'itération de la politique, présenté et analysé dans l'article \citep{nedic2003least}.
* IRL
** Overview
  The problem has been suggested in \citep{russell1998learning} and properly defined in \citep{ng2000algorithms}. An eminent seminal work is \citep{abbeel2004apprenticeship}, which introduce the feature expectation $\mu$ under its canonical name.



  The work of \citet{abbeel2004apprenticeship} inspired or shares similarities with at least these follow-up :
  - The MWAL algorithm of \citet{syed2008game}, followed by LPAL&Co \citep{syed2008game}
  - The PM algorithm of \citet{neu2007apprenticeship}
  - The MMP algorithm of \citet{ratliff2006maximum}
  - The ME method of \citet{ziebart2008maximum}
        
  Most of these approaches (namely \citep{abbeel2004apprenticeship}, MWAL, PM, MMP and ME) have been summed up in \citep{neu2009training}.
  
  
  Other non $\mu$ based approaches exist. Namely \citep{ramachandran2007bayesian}.


  Finally, parallel, non IRL work may be of interest, for example \citep{chajewska2001learning}.

** $\mu$ based approaches
   
   The feature expectation $\mu$ took its name in \citet{abbeel2004apprenticeship}'s paper, but a similar notion under a different name is presented in section 5 of \citet{ng2000algorithms}'s work.


   There are more than one way to compute $\mu$, the original approach of \citet{abbeel2004apprenticeship} uses a monte carlo estimation :
   \begin{equation}
   \hat \mu^\pi = \sum_{\xi_j\in D_\pi} \sum_{s_i\in T_i\in \xi_j} \gamma^i\phi(s_i)
   \end{equation}
   
   A wonderful, amazing idea was to use a LSTD style estimation, as proposed in \citep{klein2011batch} :
   \begin{eqnarray}
   \hat \mu^\pi(s) = E\left[\left.\omega^T \psi(s_0)\right|s_0 \in \delta_0\right]\\
   \omega = LSTD_\psi( D^\phi )
   \end{eqnarray}
   where $\delta_0$ is the distribution of the initial state and $D^\phi$ a dataset where the reward is in fact the vectorial feature $\phi$, used in the linear approximation of the value function.


   \citep{neu2009training} sums up most of the $\mu$ based approaches.
   
   They introduce an imitation metric, noted $J : \mathbb{R}^n\times \mathbf{D} \rightarrow \mathbb{R}, (\theta, D_E) \mapsto J(\theta, D_E)$, which grades the similarity between the trajectories of the expert (as demonstrated in the dataset $D_E$) and the trajectories of the agent that optimizes the reward defined by $\theta$.

   The goal of all these algorithms is to find a weight vector $\theta^*$ so that for a given $D_E$ :
   \begin{equation}
   \theta^* = \arg\min_\theta J(\theta,D_E)
   \end{equation}

   They do so using an iterative procedure on $\theta$ whose update step is of the form :
   \begin{equation}
   \theta_{k+1} = g(g^{-1}(\theta_k) + \alpha_k\Delta_k)
   \end{equation}



   Although the update procedure may not be the most important thing in a given paper, they are given here when possible. The discussion will take place over the choice of the dissimilarity function $J$.

*** Ng & Russel's approach
    The paper \citep{ng2000algorithms} defines two loss functions. The first works in the discrete case. A compact definition of this loss function can be obtained by working with two policies. The first is $\pi_E$, the policy of the expert. The second is $\pi$, the \emph{next-to-best} policy the definition of which is $\pi(s) = \arg\min\limits_{a\in A\setminus\pi_E(s)}Q(s,\pi_E(s))-Q(s,a)$. The loss function can now be expressed as :
    \begin{equation}
    J(\theta,\pi_E) = -||(\mathcal{P}_{\pi_E} - \mathcal{P}_\pi)(I-\gamma \mathcal{P}_{\pi_E})^{-1}\theta||_1 + \lambda ||\theta||_1
    \end{equation}

    

    Une thèse de master, \citep{shiraev2003inverse}, applique cet algorithme à la découverte d'algorithme de routage dans les réseaux.
*** Abbeel & Ng's algorithm
    
    The dissimilarity function is :
    \begin{equation}
    J(\theta, D_E) = ||\hat \mu^{D_E} - \hat \mu^\theta||_2
    \end{equation}

    This means that the algorithm seeks a policy that has the same history as the expert's.\\

    The update step uses $g : x\mapsto x$ and $\forall k, \alpha_k=1$, and the following update vector : 
    \begin{eqnarray}
    \Delta_k = \beta_k(\hat\mu^E(s_0) - \hat\mu^{\theta_k}) - \beta_k\theta_k\\
    \textrm{with: } \beta_k = {(\hat\mu^{\theta_k}-\bar\mu_{k-1})^T(\hat\mu^E-\bar\mu_{k-1})\over(\hat\mu^{\theta_k}-\bar\mu_{k-1})^T(\hat\mu^{\theta_k}-\bar\mu_{k-1})}\\
    \textrm{and: } \bar\mu_k = \bar\mu_{k-1} + \beta_k(\hat\mu^{\theta_k}-\bar\mu_{k-1})
    \end{eqnarray}

    With the initialization : $\bar\mu^0 = \mu^0$ and $\theta_1 = \hat\mu^{D_E} - \mu^0$

*** MWAL
    
    The dissimilarity function is : 
    \begin{equation}
    J(\theta, D_E ) = \theta^T(\hat \mu^\theta - \hat \mu^{D_E})
    \end{equation}

    The update step uses $g : x\mapsto e^x$ and the update vector :
    \begin{equation}
    \Delta_k = \hat \mu^{D_E} - \hat \mu^\theta
    \end{equation}

    In the original paper the update step is said to be, for every component $i \in \{1,\dots,n_\phi\}$ of $\theta_k$: 
    \begin{eqnarray}
    \theta_{k+1}^i &=&  \theta_k^i exp\left(ln\left(\left(1+\sqrt{2ln(n_\phi)\over k}\right)^{-1}\right){((1-\gamma)(\hat \mu^\theta -  \hat \mu^{D_E})+2)\over 4}\right)\\
    \theta_{k+1}^i &=&  \theta_k^i exp\left(-ln\left(1+\sqrt{2ln(n_\phi)\over k}\right){((1-\gamma)(\hat \mu^\theta -  \hat \mu^{D_E})+2)\over 4}\right)\\
    \theta_{k+1}^i &=&  \theta_k^i exp\left(ln\left(1+\sqrt{2ln(n_\phi)\over k}\right){((1-\gamma)(\hat \mu^{D_E} - \hat \mu^\theta)+2)\over 4}\right)\\
    \theta_{k+1}^i &=&  \theta_k^i exp\left(ln\left(1+\sqrt{2ln(n_\phi)\over k}\right){1-\gamma\over 4}(\hat \mu^{D_E} - \hat \mu^\theta)+{2\over 4}\right)\\
    \end{eqnarray}
   
    Small detail : the update step described in \citep{syed2008game} includes a computation  of the form :
    \begin{equation}
    \theta_k^i = {W^i\over \sum\limits_i^{n_\phi}W^i}
    \end{equation}
    as every term involved is positive, this is the same as making the $1$-norm of $\theta$ be one by definition. This may be necessary for the numerical stability of the algorithm and does not change the set of optimal policies. We will drop it for now as it is not mathematically useful.
  
    Whereas in the other paper, it is said to be :
    \begin{eqnarray}
    \theta_{k+1} &=& g(g^{-1}(\theta_k) + \alpha_k\Delta_k)\\
    \theta_{k+1} &=& exp(ln(\theta_k) + \alpha_k(\hat \mu^{D_E} - \hat \mu^\theta))
    \end{eqnarray}

    The step-size parameter $\alpha_k$ is therefore : 
    \begin{equation}
    \alpha_k = ln\left(1+\sqrt{2ln(n_\phi)\over k}\right){1-\gamma\over 4} + {1\over 2(\hat \mu^{D_E} - \hat \mu^\theta)}
    \end{equation}
    
    This strange expression, as outlined in \citep{neu2009training}, is useful to derive theoretical bounds, but is empirically not the wisest choice. Other proposed values include :
    \begin{eqnarray}
    \alpha_k &=& {1\over k}\\
    \alpha_k &=& \alpha \in \mathbb{R}\\
    \alpha_k &=& {1\over \sqrt{k}}\\
    \end{eqnarray}
    or following the iRprop rule \citep{igel2000improving} I didn't read about (yet).


    The follow-up paper \citep{syed2008apprenticeship} present a computationally more efficient method for the same optimization problem. Among the subtelties I hide here lies the problem of the suboptimal expert I am not interested in at the moment.

*** MMP
    The max-margin planning algorithm has been introduced in \citep{ratliff2006maximum}. A few things should be noted about this algorithm : 
    - The authors work with state-action visitation frequencies, which can be seen as state-action feature expectation with $\gamma = 1$.
    - Because of this, estimating the feature expectation can not be done off-policy using the LSTD-$Q$ trick.
    - This method maintains two policies, the first (noted $\pi$) is the same as in the other algorithms (i.e., the optimal policy with respect to the reward defined by $\theta$). The second (noted $\pi'$) is the optimal policy with respect to the reward defined by $\theta - c\cdot \mu^{D_E}$, with $c$ being a constant parameter of the algorithm.
    - Feature expectations for both maintained policies $\pi$ and $\pi'$ ought to be computed, noted respectively $\hat\mu^\theta$ and $\hat\mu^{\theta l}$

     
    The dissimilarity function is : 
    \begin{equation}
    J(\theta, D_E) = (\theta - c\hat \mu^{D_E})^T \mu^{\theta l} - \theta^T\mu^{D_E} + {\lambda \over 2} ||\theta||_2^2
    \end{equation}

    
    The update step uses $g : x\mapsto x$ and the update vector :
    \begin{equation}
    \Delta_k = \hat \mu^{D_E} - \hat \mu^{\theta l } - \lambda \theta_k
    \end{equation}


    I did not cross check this with the original paper as I did with the previous algorithms because the original authors' writing is somewhat confusing.


    The follow-up paper, \citep{ratliff2007boosting} does not change the $J$ function. The improvment made by this paper (boosting in new features) can however be ported to other settings.

    An applicative paper, \citep{ratliff2007imitation} presents the same algorithm (cast as a multi class label problem) with some impressive real life robotics applications.

    Un papier de \citet{boularias2011bootstrapping} donne une méthode de bootstraping de MWAL et LPAL, à utiliser quand l'espace d'état est grand et que la politique de l'expert n'en couvre qu'une partie.
*** PM
    The Policy Matching algorithm was described by the same authors as the survey. Therefore I assumed there were faithful and I did not cross check their transcription either.

    This work deals with stochastic policies. I modified it for the deterministic case because I don't (yet) see the point of stochastic policies in my framework. This has some implications : in the original work $\pi(.|s)$ can be defined arbitrarily when unknown ; the dissimilarity function no longer contains a squared term that was probably useful for the proof.


    They work with a discrete state space. What they denote by $\mu$ is the frequency of visitation of a state. Assuming discrete feature $\phi$, their definition of $\mu(s)$ maps to (using our notations and setting $\gamma$ to $1$) :
    \begin{equation}
    E(\phi(s)^T\mu(s_0)|s_0\sim \delta_0)
    \end{equation}
    where $\delta_0$ is the distribution of the starting state. We will use this correspondance to generalize to continuous state spaces.

    The dissimilarity function is (where $s\in D$ formally means $s\in\{s\in T | T\in \xi | \xi \in D\}$ ) : 
    \begin{equation}
    J(\theta, D_E) = {1\over 2}\sum_{s \in D} E(\phi(s)^T\hat\mu^{D_E}(s_0)|s_0\sim \delta_0) \mathbb{I}( \pi(s), \pi_E(s) )
    \end{equation}
    
    
    The update needs an agent that accept stochastic policies to be implemented.
*** Maximum Entropy IRL
    I did not fully understood the math behind this method, so I just transcript the useful information here.

    
    The $J$ function is : 
    \begin{equation}
    J(\theta, D_E) = - \theta^T\hat\mu^{D_E} + log(\sum_{\xi\in\Xi} e^{(\theta^T \hat\mu^\xi)})
    \end{equation}
    with $\Xi$ being the set of all possible trajectories. Computation involving this set may be tricky, so the survey discusses some tricks about doing this in real life. $\hat\mu^\xi$ is defined as $\sum\limits_{s\in \xi} \phi(s)$
    
    The update is presumably additive ($g : x\mapsto x$) and use the following step :
    \begin{equation}
    \Delta_k = \hat\mu^E - \sum_{\xi \in \Xi}\left(e^{(\theta^T \hat\mu^\xi)} \over\sum\limits_{\xi'\in\Xi} e^{(\theta^T \hat\mu^{\xi'})}\right)\hat\mu^\xi
    \end{equation}
    
*** GPIRL
    \citet{jin2010gaussian} propose an approach based on gaussian processes. This solves the IRL algorithm in continuous settings. An interesting feature of this algorithm is the fact that it keeps track of the uncertainty.

    Philisophically, the authors claim to use a max margin between the expert and the best policy optimization, although practically their algorithm is a port of \citep{abbeel2004apprenticeship}. The $J$ function is the same.
** Other approaches

   \citep{ramachandran2007bayesian} is a bayesian approach. The dissimilarity they try to minimize is :
   \begin{equation}
   J(\theta,D_E) = E\left(\left.||R-R_\theta||_x\right|D_E\right)
   \end{equation}
   with $x=1$ or $x=2$ (they give solution for both settings).

   

   \citet{chajewska2001learning}'s work acknowledge \citet{ng2000algorithms} work and uses it for comparison purpose. However, the approach they suggest leads (if I read correctly) to the same $J$ function and the same algorithm as in \citep{ramachandran2007bayesian}, only 6 years earlier. I don't know why this paper has not been acknowledged by the IRL community. Maybe the framing of the problem as a decision tree (and not an MDP) and the use of unusual keywords were enough for it to stay unnoticed.
   

   
   Un autre algo Bayesien, G-MH a été publié dans \citep{rothkopf2008modular}. Cette approche généralise le travail de \citet{ramachandran2007bayesian}.
* Learning from Observation/Demonstration
  Une tentative d'unification est présentée en \cite{montana2011towards}. Autant le survey est utile, autant je reste circonspect tant aux maths du papier, qui laissent un peu l'IRL à l'écart en ne tenant pas compte du fait que les états respectent le critère de Markov. A revérifier.

  Un papier applicatif assez sympa, \citep{leon2011teaching}, fait un survey du domaine et combine ces méthodes avec celles consistant à intégrer du feedback humain dans du RL pour réussir une expérience de déplacement d'objet par un bras robotisé en utilisant du matériel grand public (un kinect).


  Plus impressionnant, le vol acrobatique d'un hélicoptère télécommandé dans \citep{coates2008learning} fait appel à quelques bidouilles d'ingénierie propres au problème bien que le principe soit au fond rigoureux et transposable à d'autres applications.


  Une amélioration de l'algorithme Donut de \citep{grollman2011donut} est comparé à l'algorithme POWER de \cite{kober2008policy} dans \cite{grollman2011imitation}. Le but est d'apprendre non pas à partir de démonstrations réussies, mais de démonstrations où l'expert s'est trompé.
* Feature Selection
  \citep{saxe2010random} montre que lorsque l'on utilise des /convolutional pooling architectures/, le poids importe moins que l'architecture, et propose en conséquence de déterminer la meilleure architecture en se basant sur ses performances à poids aléatoires.

  L'architecture à base de dictionnaire et d'encodeur a été analysée par \citet{coates2011importance}.
* Feature Learning
  SOM of \citet{kohonen1995self} are a good start. They have been refined giving birth to the THSOM algorithm \citep{koutnik2007inductive,koutnik2008temporal} and T$^2$SOM algorithm \citep{ferro2010reading}.

  La méthode TNT \citep{graziano2011unsupervised} présente encore une amélioration des SOM, qui permet de s'interfacer entre l'espace d'état-action et un algo de RL en proposant une modélisation d'un POMDP.
* Transfer Learning
** TL and RL
   \citet{lazaric2008knowledge} a travaillé sur le /transfer learning/ appliqué au RL. \citep{fachantidis2011transfer} reprend ces travaux afin qu'ils utilisent plusieurs /mappings/ et non un seul, ce qui donne de meilleurs résultats, l'algorithme résultant est appelé COMBREL, il fonctionne aussi bien en présence qu'en l'absence de modèle.
* Things to do :
** TODO Reread \citep{chajewska2001learning} to see the difference with \citep{ramachandran2007bayesian}
* Open questions
  - Can the LPAL, MWAL-PI, MWAL-VI and MWAL-Dual algorithms of \cite{syed2008apprenticeship} be expressed in the framework of \cite{neu2009training} ?
  - How does \cite{ratliff2007boosting} relates to \cite{neu2009training} ?
  - Is it possibl to plug Thm1 (lambda alpha) in \cite{ramachandran2007bayesian}
  - Can \cite{ramachandran2007bayesian} be expressend in the framework of \cite{syed2008apprenticeship} ?
  - What is the exact relationship between \cite{chajewska2001learning} and \cite{ramachandran2007bayesian} ?
   \bibliographystyle{plainnat}
   \bibliography{../Biblio/Biblio.bib}
