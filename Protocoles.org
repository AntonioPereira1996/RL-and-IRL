#+TITLE:Protocoles expérimentaux pour l'IRL
#+LATEX_HEADER:\usepackage[plain,cm]{fullpage} 
#+LATEX_HEADER:\usepackage{algorithm}
#+LATEX_HEADER:\usepackage{algpseudocode}
* Algos à tester
  L'[[file:abbeel2004apprenticeship.org][algo d'Abbeel]] sert de baseline, puisqu'il est déjà implémenté et bien connu dans la littérature. Il renvoie une politique, c'est cette politique que l'on teste.

  [[file:NouveauxAlgos.org][SCIRL]] est un des algos à tester. On vend le fait qu'il fonctionne avec uniquement les données de l'expert.

  A tester également dans un futur proche (quand ils seront implémentés) :
  - Relative Entropy de Boularias
  - Classif + regression de nous
** Paramètres des algorithmes
*** ANIRL
    Sans se soucier du choix des routines, les paramètres de cet algorithme concernent le critère d'arrêt. La valeur $t = ||\mu_E - \bar\mu||_2$ doit pouvoir passer en dessous d'une certaine valeur $\epsilon_A$. Dans le cas où cela est impossible (à cause d'erreurs dans ce que retournent les routines, par exemple), l'algorithme s'arrête après $T$ itérations.
*** SCIRL
    En laissant également de côté le choix des routines, on trouve comme paramètres pour cet algorithme la fonction $l$, le pas d'apprentissage $\alpha$, le nombre maximum d'itérations $T$, un threshold sur la norme du gradient $\epsilon_S$ et le vecteur récompense initial $\theta_0$.
* Routines disponibles
** Calcul d'une politique
   Nous avons implémenté un algorithme de [[file:TaskTransfer.org::*Dynamic%20programming][programmation dynamique]] pour les cas où les probabilités de transition sont connues. Il est également possible d'utiliser [[file:LSPI.org][LSPI]] pour les autres cas.
** Calcul de $\mu$
   Si les probabilités de transitions et la politique sont connues, alors $E[\mu(s_0)]$ et $\mu(s,a)$ peuvent être calculés grâce à une [[file:DP_mu.org][adaptation]] des algorithmes de programmation dynamique.\\

   Si les probabilités de transition sont inconnues, mais qu'un simulateur est disponible, on peut obtenir une très bonne approximation de $E[\mu(s_0)]$ et $\mu(s,a)$ grâce à un Monte-Carlo simple.

   Enfin, dans le cas où seules les données de l'expert sont disponibles, on utilisera LSTD$\mu$ en mode /off-/ ou /on-policy/. Il existe deux versions, en effet ANIRL a besoin de $E[\mu(s_0)]$ alors que SCIRL a besoin de $\mu(s,a),\forall s \in D_E, \forall a \in A$.
* Expériences
** Là où tout marche
   Pour montrer qu'en théorie notre algorithme n'est pas fondamentalement hors de propos, on va le comparer à celui d'Abbeel dans le cas merveilleux où l'on a autant d'information que nécessaire. Le but est de montrer que les arguments développés de manière théorique portent du sens car en pratique les résultats sont similaires à (espérons, meilleurs que) ceux d'un algo central de la littérature.\\

   Sur le GridWorld, le setting est décrit algo \ref{GridParfait.algo}, l'application d'ANIRL est donnée algo \ref{AnirlGridParfait.algo} et celle de SCIRL algo \ref{ScirlGridParfait.algo}.\\
    
\begin{algorithm}
\caption{Réglages du GridWorld dans le cas parfait}
\label{GridParfait.algo}
\begin{algorithmic}
\State Définir la vraie récompense $R$ comme $R(s)=0$ sauf en haut à droite où $R(s)=1$.
\State Entraîner l'expert sur $R$ grâce à l'algorithme de programmation dynamique.
\State Créer $D_E$ la base de données contenant chaque état et l'action de l'expert associée
\State Tracer la vraie récompense
\State Tracer la fonction de valeur associée
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Utilisation d'Abbeel sur le GridWorld dans le cas parfait}
\label{AnirlGridParfait.algo}
\begin{algorithmic}
\State Instancier l'algorithme d'Abbeel avec :
\Procedure{$\pi$}{$\theta$}
\State Utiliser l'algorithme de programmation dynamique exact, vis à vis de la récompense $R(s) = \theta^T\psi(s)$.
\EndProcedure
\Procedure{$\mu$}{$\pi$}
\State Utiliser l'algorithme de programmation dynamique exact.
\EndProcedure
\Procedure{$\mu_E$}{}
\State Utiliser l'algorithme de programmation dynamique exact.
\EndProcedure
\State Fixer $\epsilon_A = 0.01$
\State Fixer $T$ suffisament grand pour que l'algorithme s'arrête avant $T$ itérations
\State Faire tourner ANIRL
\State Comparer la politique obtenue avec celle de l'expert
\State Tracer la récompense ayant donné cette politique
\State Tracer la fonction de valeur associée
\end{algorithmic}
\end{algorithm}

   
\begin{algorithm}
\caption{Utilisation de SCIRL sur le GridWorld dans le cas parfait}
\label{ScirlGridParfait.algo}
\begin{algorithmic}
\State Instancier SCIRL avec :
\Procedure{$\mu_E$}{s,a}
\State Utiliser l'algorithme de programmation dynamique exact.
\EndProcedure
\State Fixer $\epsilon_S=0.01$
\State Fixer $T$ suffisament grand pour que l'algorithme s'arrête avant $T$ itérations
\State Trouver un $\alpha(t)$ qui fonctionne avec une gridsearch.
\State Initialiser $\theta_0 = {\mathbf 0}$
\State Faire tourner SCIRL sur $D_E$
\State Tracer la récompense 
\State Entraîner un agent sur la récompense obtenue
\State Comparer la politique de l'agent avec celle de l'expert
\State Tracer la fonction de valeur associée
\end{algorithmic}
\end{algorithm}

Pour le pendule inversé, les probabilités de transition ne sont plus disponibles. Le setting est donc un peu différent (algo \ref{InvertedParfait.algo}), l'application d'ANIRL (algo \ref{AnirlInvertedParfait.algo}) et de SCIRL (algo \ref{ScirlInvertedParfait.algo}) également. Notamment, si dans le cas précédent il n'y avait pas de place pour l'aléatoire, une variance existe dans ce cas ci. Répéter l'expérience plusieurs fois montrera que cette variance est minime.\\


\begin{algorithm}
\caption{Settings sur le pendule inversé dans le cas parfait}
\label{InvertedParfait.algo}
\begin{algorithmic}
\State Créer $D$ une base de données de $M_D$ trajectoires débutant dans un état choisi aléatoirement, en suivant une politique aléatoire.
\State Entraîner l'expert grâce à LSPI comme décrit dans le papier de Lagoudakis
\State Créer $N$ matrices $D_E$ contenant chacune $M_E$ trajectoires de l'expert de longueur maximum $L_{ip}$
\State Tracer la vraie récompense
\State Tracer la fonction de valeur associée
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Utilisation d'Abbeel sur le pendule inversé dans le cas parfait}
\label{AnirlInvertedParfait.algo}
\begin{algorithmic}
\State Instancier l'algorithme d'Abbeel avec :
\Procedure{$\pi$}{$\theta$}
\State Utiliser LSPI, vis à vis de la récompense $R(s) = \theta^T\psi(s)$.
\EndProcedure
\Procedure{$\mu$}{$\pi$}
\State Utiliser un Monte-Carlo de $M_{mcip}$ trajectoires de longueur maximum $L_{mcip}$
\EndProcedure
\Procedure{$\mu_E$}{}
\State Utiliser un Monte-Carlo sur $D_E$
\EndProcedure
\State Fixer $\epsilon_A = 0.01$
\State Fixer $T$ suffisament grand pour que l'algorithme s'arrête avant $T$ itérations
\ForAll{$D_E$}
\State Faire tourner ANIRL
\EndFor
\State Comparer les performances moyenne des politiques obtenues avec celles de l'expert
\State Tracer la récompense ayant donné une des politiques
\State Tracer la fonction de valeur associée
\end{algorithmic}
\end{algorithm}

   
\begin{algorithm}
\caption{Utilisation de SCIRL sur le pendule inversé dans le cas parfait}
\label{ScirlInvertedParfait.algo}
\begin{algorithmic}
\State Instancier SCIRL avec :
\Procedure{$\mu_E$}{s,a}
\State Utiliser un Monte-Carlo de $M_{mcip}$ trajectoires de longueur maximum $L_{mcip}$
\EndProcedure
\State Fixer $\epsilon_S=0.01$
\State Fixer $T$ suffisament grand pour que l'algorithme s'arrête avant $T$ itérations
\State Trouver un $\alpha(t)$ qui fonctionne avec une gridsearch.
\State Initialiser $\theta_0 = {\mathbf 0}$
\ForAll{$D_E$}
\State faire tourner SCIRL sur $D_E$
\State Entraîner un agent sur la récompense obtenue
\EndFor
\State Tracer une des récompense 
\State Tracer la fonction de valeur associée
\State Comparer les performances moyenne des agents avec celles de l'expert
\end{algorithmic}
\end{algorithm}

Le problème du Highway n'est pas exploitable à l'heure actuelle, mais avec un peu de travail il est possible de le changer en un problème discret un peu plus difficile que le GridWorld. Cela a pour avantage de nous fournir les probabilités de transitions. Le protocole est quasiment le même que pour le GridWorld, avec le setting décrit dans l'algo \ref{HighwayParfait.algo}, l'application d'ANIRL décrite algo \ref{AnirlHighwayParfait.algo} et celle de SCIRL décrite algo \ref{ScirlHighwayParfait.algo}.\\

    
\begin{algorithm}
\caption{Réglages du Highway dans le cas parfait}
\label{HighwayParfait.algo}
\begin{algorithmic}
\State Définir la vraie récompense $R$ comme récompensant la vitesse et pénalisant les collisions et sortie de route (aka /fast driving/) : $R = [0.5~0.25~0.25]$.
\State Entraîner l'expert sur $R$ grâce à l'algorithme de programmation dynamique.
\State Créer $D_E$ la base de données contenant chaque état et l'action de l'expert associée
\State Evaluer les performances de l'expert
\State Enregistrer une courte démo de l'expert qui conduit
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Utilisation d'Abbeel sur le Highway dans le cas parfait}
\label{AnirlHighwayParfait.algo}
\begin{algorithmic}
\State Instancier l'algorithme d'Abbeel avec :
\Procedure{$\pi$}{$\theta$}
\State Utiliser l'algorithme de programmation dynamique exact, vis à vis de la récompense $R(s) = \theta^T\psi(s)$.
\EndProcedure
\Procedure{$\mu$}{$\pi$}
\State Utiliser l'algorithme de programmation dynamique exact.
\EndProcedure
\Procedure{$\mu_E$}{}
\State Utiliser l'algorithme de programmation dynamique exact.
\EndProcedure
\State Fixer $\epsilon_A = 0.01$
\State Fixer $T$ suffisament grand pour que l'algorithme s'arrête avant $T$ itérations
\State Faire tourner ANIRL
\State Comparer la politique obtenue avec celle de l'expert
\State Enregistrer une courte démo de l'agent qui conduit
\end{algorithmic}
\end{algorithm}

   
\begin{algorithm}
\caption{Utilisation de SCIRL sur le Highway dans le cas parfait}
\label{ScirlHighwayParfait.algo}
\begin{algorithmic}
\State Instancier SCIRL avec :
\Procedure{$\mu_E$}{s,a}
\State Utiliser l'algorithme de programmation dynamique exact.
\EndProcedure
\State Fixer $\epsilon_S=0.01$
\State Fixer $T$ suffisament grand pour que l'algorithme s'arrête avant $T$ itérations
\State Trouver un $\alpha(t)$ qui fonctionne avec une gridsearch.
\State Initialiser $\theta_0 = {\mathbf 0}$
\State Faire tourner SCIRL sur $D_E$
\State Entraîner un agent sur la récompense obtenue
\State Comparer la politique de l'agent avec celle de l'expert
\State Enregistrer une courte démo de l'agent qui conduit
\end{algorithmic}
\end{algorithm}

Ces quelques expériences devraient montrer que les algorithmes mis en jeu sont en mesure de résoudre le problème qui leur est posé /s'ils disposent de toute l'information nécessaire/. La difficulté réside dans l'exploitation de la (faible) quantité d'information disponible dans le cas réel.



** Cas extrême où seules les données de l'expert sont disponibles
   Après avoir prouvé que les algos considérés fonctionnent correctement lorsque les conditions sont idéales, on va se pencher sur le cas le plus proche de la réalité, à savoir le cas où seules les données de l'expert sont disponibles.\\
    
   Pour les trois benchmark (GridWorld, pendule et Highway), le setting est le même à ceci près qu'on testera également différentes formes de politiques aléatoires afin d'avoir une vision un peu objective de la qualité de la sortie d'un algo. La méthodologie pour obtenir ces baselines aléatoire est expliqué algo \ref{RandomGrid.algo} pour le GridWorld, algo \ref{RandomInverted.algo} pour le pendule inversé et algo \ref{RandomHighway.algo} pour le Highway.\\

   Dans les trois cas, les données de l'expert correspondent /à une unique trajectoire de l'expert/. Plusieurs de ces trajectoires seront utilisées les unes après les autres afin de faire des statistiques sur les résultats.\\

   Dans le cas du Gridworld, ANIRL (algo \ref{AnirlGridXtreme.algo}) et SCIRL (algo \ref{ScirlGridXtreme.algo}) devraient s'en sortir avec des performances similaires.\\

   Dans le cas du pendule, on a déjà vu en testant LSTD$\mu$ et l'expérience devrait le montrer à nouveau que ANIRL (algo \ref{AnirlInvertedXtreme.algo}) ne parvient pas à résoudre le problème inverse car LSPI n'est pas en mesure de résoudre le problème direct. Il faut à ce moment là introduire Boularias pour comparer avec SCIRL (algo \ref{ScirlInvertedXtreme}) qui, lui, devrait réussir.\\


   Enfin dans le cas du Highway remasterisé (i.e. pas la version de Syed), on devrait arriver à quelque chose avec LSTD$\mu$ et SCIRL (algo \ref{ScirlHighwayXtreme.algo}), sans avoir besoin d'implémenter d'autres méthodes d'approximation de $\mu$. Il se peut qu'il faille bidouiller les fonctions de base. Je pars sur un réseau de RBF, je ne vois pas /a priori/ pourquoi ça ne marcherait pas. J'ai de gros doutes sur la capacité de LSPI à résoudre le problème à partir des seules données de l'expert, aussi je pense qu'on arrivera au même type de résultats que pour le pendule, c'est à dire qu'ANIRL (algo \ref{AnirlHighwayXtreme.algo}) ne devrait pas arriver à grand chose.\\
\begin{algorithm}
\caption{Baseline aléatoire sur le GridWorld}
\label{RandomGrid.algo}
\begin{algorithmic}
\For{$N_{rg}$ fois}
\State Définir une récompense $R_r$ telle que pour chaque état $s$, $R_r(s)$ est une variable aléatoire tirée selon une loi uniforme dans $[-1,1]$.
\State Entraîner un agent sur $R_r$ grâce à l'algorithme de programmation dynamique.
\State Faire agir l'agent
\State Donner le contrôle à la politique choisissant aléatoirement une action à chaque pas
\EndFor
\State Evaluer les performances moyennes de l'agent et de la politique aléatoire vis à vis de la vraie récompense $R$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Baseline aléatoire sur le pendule inversé}
\label{RandomInverted.algo}
\begin{algorithmic}
\State Créer $D$ une base de données de $M_D$ trajectoires débutant dans un état choisi aléatoirement, en suivant une politique aléatoire.
\For{$N_{rp}$ fois}
\State Définir une récompense $R_r = \theta_r^T\phi(s)$ telle que pour chaque composante $i$ de $\theta_r$, $\theta_r^i$ est une variable aléatoire tirée selon une loi uniforme dans $[-1,1]$.
\State Entraîner un agent sur $R_r$ grâce à LSPI ayant accès à $D$
\State Faire agir l'agent
\State Donner le contrôle à la politique choisissant aléatoirement une action à chaque pas
\EndFor
\State Evaluer les performances moyennes de l'agent et de la politique aléatoire vis à vis de la vraie récompense $R$.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Baseline aléatoire sur le Highway}
\label{RandomHighway.algo}
\begin{algorithmic}
\For{$N_{rh}$ fois}
\State Définir une récompense $R_r$ telle que pour chaque état $s$, $R_r(s)$ est une variable aléatoire tirée selon une loi uniforme dans $[-1,1]$.
\State Entraîner un agent sur $R_r$ grâce à l'algorithme de programmation dynamique.
\State Faire agir l'agent
\State Donner le contrôle à la politique choisissant aléatoirement une action à chaque pas
\EndFor
\State Evaluer les performances moyennes de l'agent et de la politique aléatoire vis à vis de la vraie récompense $R$.
\State Enregistrer une courte démo d'un agent et d'un run de la politique aléatoire
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Utilisation d'Abbeel sur le GridWorld dans le cas extrême}
\label{AnirlGridXtreme.algo}
\begin{algorithmic}
\State Instancier l'algorithme d'Abbeel avec :
\Procedure{$\pi$}{$\theta$}
\State Utiliser LSPI nourri uniquement avec les transitions de l'expert.
\EndProcedure
\Procedure{$\mu$}{$\pi$}
\State Utiliser LSTD$\mu$ /off-policy/ nourri uniquement avec les données de l'expert.
\EndProcedure
\Procedure{$\mu_E$}{}
\State Utiliser LSTD$\mu$ /on-policy/.
\EndProcedure
\State Fixer $\epsilon_A = 0.01$
\State Fixer $T$ à la valeur utilisée dans le cas parfait.
\For{$N_{xg}$ fois}
\State Faire tourner ANIRL
\State Comparer la politique obtenue avec celle de l'expert
\EndFor
\State Tracer la récompense ayant donné une des politiques
\State Tracer la fonction de valeur associée
\State Comparer les performances moyennes des politiques obtenues avec celles de l'expert et les baselines aléatoires.
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Utilisation de SCIRL sur le GridWorld dans le cas extrême}
\label{ScirlGridXtreme.algo}
\begin{algorithmic}
\State Instancier SCIRL avec :
\Procedure{$\mu_E$}{s,a}
\State Utiliser LSTD$\mu(s,a)$ /on-policy/
\EndProcedure
\State Fixer $\epsilon_S=0.01$
\State Fixer $T$ de la même manière que cans le cas parfait
\State Fixer $\alpha(t)$ de la même manière que cans le cas parfait
\State Initialiser $\theta_0 = {\mathbf 0}$
\For{$N_{xg}$ fois}
\State Faire tourner SCIRL sur $D_E$
\State Entraîner un agent sur la récompense obtenue
\State Comparer la politique de l'agent avec celle de l'expert
\EndFor
\State Tracer une des  récompenses
\State Tracer la fonction de valeur associée
\State Comparer les performances moyennes des politiques obtenues avec celles de l'expert et les baselines aléatoires.
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Utilisation d'Abbeel sur le pendule inversé dans le cas extrême}
\label{AnirlInvertedXtreme.algo}
\begin{algorithmic}
\State Instancier l'algorithme d'Abbeel avec :
\Procedure{$\pi$}{$\theta$}
\State Utiliser LSPI, vis à vis de la récompense $R(s) = \theta^T\psi(s)$, nourri avec les données de l'expert.
\EndProcedure
\Procedure{$\mu$}{$\pi$}
\State Utiliser LSTD$\mu$ /off-policy/ nourri uniquement avec les données de l'expert.
\EndProcedure
\Procedure{$\mu_E$}{}
\State Utiliser LSTD$\mu$ /on-policy/.
\EndProcedure
\State Fixer $\epsilon_A = 0.01$
\State Fixer $T$ à la valeur utilisée dans le cas parfait.
\For{$N_{xp}$ fois}
\State Faire tourner ANIRL
\EndFor
\State Comparer les performances moyennes des politiques obtenues avec celles de l'expert et les baselines aléatoires.
\State Tracer la récompense ayant donné une des politiques
\State Tracer la fonction de valeur associée
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Utilisation de SCIRL sur le pendule inversé dans le cas extrême}
\label{ScirlInvertedXtreme.algo}
\begin{algorithmic}
\State Instancier SCIRL avec :
\Procedure{$\mu_E$}{s,a}
\State Utiliser LSTD$\mu(s,a)$ /on-policy/
\EndProcedure
\State Fixer $\epsilon_S=0.01$
\State Fixer $T$ de la même manière que cans le cas parfait
\State Fixer $\alpha(t)$ de la même manière que cans le cas parfait
\State Initialiser $\theta_0 = {\mathbf 0}$
\For{$N_{xp}$ fois}
\State faire tourner SCIRL sur les données de l'expert
\State Entraîner un agent sur la récompense obtenue
\EndFor
\State Tracer une des récompense 
\State Tracer la fonction de valeur associée
\State Comparer les performances moyenne des agents avec celles de l'expert et des baselines aléatoires.
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Utilisation de SCIRL sur le Highway dans le cas extrême}
\label{ScirlHighwayXtreme.algo}
\begin{algorithmic}
\State Instancier SCIRL avec :
\Procedure{$\mu_E$}{s,a}
\State Utiliser LSTD$\mu(s,a)$ /on-policy/
\EndProcedure
\State Fixer $\epsilon_S=0.01$
\State Fixer $T$ de la même manière que cans le cas parfait
\State Trouver un $\alpha(t)$ qui fonctionne avec une gridsearch.
\State Initialiser $\theta_0 = {\mathbf 0}$
\For{$N_{xh}$ fois}
\State Faire tourner SCIRL sur $D_E$
\State Entraîner un agent sur la récompense obtenue
\EndFor
\State Comparer les performances moyennes des politiques obtenues avec celles de l'expert et les baselines aléatoires.
\State Enregistrer une courte démo d'un agent qui conduit
\end{algorithmic}
\end{algorithm}


\begin{algorithm}
\caption{Utilisation d'Abbeel sur le Highway dans le cas extrême}
\label{AnirlHighwayXtreme.algo}
\begin{algorithmic}
\State Instancier l'algorithme d'Abbeel avec :
\Procedure{$\pi$}{$\theta$}
\State Utiliser LSPI, vis à vis de la récompense $R(s) = \theta^T\psi(s)$, nourri avec les données de l'expert.
\EndProcedure
\Procedure{$\mu$}{$\pi$}
\State Utiliser LSTD$\mu$ /off-policy/ nourri uniquement avec les données de l'expert.
\EndProcedure
\Procedure{$\mu_E$}{}
\State Utiliser LSTD$\mu$ /on-policy/.
\EndProcedure
\State Fixer $\epsilon_A = 0.01$
\State Fixer $T$ à la valeur utilisée dans le cas parfait.
\For{$N_{xh}$ fois}
\State Faire tourner ANIRL
\EndFor
\State Comparer les performances moyennes des politiques obtenues avec celles de l'expert et les baselines aléatoires.
\State Tracer la récompense ayant donné une des politiques
\State Tracer la fonction de valeur associée
\end{algorithmic}
\end{algorithm}



* Conclusion
  
  L'étude dans le cas extrême devrait fournir une preuve empirique de la validité de SCIRL. Une fois qu'on aura augmenté cette étude en y injectant le Relative Entropy on aura une image plus claire, car ANIRL n'est pas assez puissant pour jouer sur le même terrain que nous.\\
  
  L'étude dans le cas parfait montre le bien fondé de l'IRL sur les problèmes que l'on attaque.\\

  L'étude dans le cas parfait sert aussi à obtenir un jeu de paramètres propres aux algos (par opposition au paramètres des routines) (genre $\alpha(t)$) qui fonctionnent.\\

  Si rien n'est précisé, les paramètres des routines sont obtenus par une recherche manuelle vers ce qui fonctionne le mieux. De tête le seul cas où cela aura lieu est le Highway, puisque les autres expériences sont très similaires à celles déjà effectuées, on a donc déjà une idée des paramètres qui fonctionnent.

