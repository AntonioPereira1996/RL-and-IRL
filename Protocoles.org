#+TITLE:Protocoles expérimentaux pour l'IRL
#+LATEX_HEADER:\usepackage[plain,cm]{fullpage} 
#+LATEX_HEADER:\usepackage{algorithm}
* Les Benchmarks
  On dispose de trois tests pour nos algorithmes, par ordre de difficulté on a le GridWorld, le pendule inversé et le highway. Le GridWorld est très simple à analyser, toutes les fonctions sont représentées sous forme tabulaire, les probabilités de transitions sont connues. On peut visualiser de la récompense et la fonction de valeur et interpréter cette visualisation.\\

  Le pendule est un peu plus complexe. Les probabilités de transitions sont inconnues, mais grâce à \cite{lagoudakis2003least}, on dispose d'outils d'analyse (LSPI et les features associées sur ce problème) qui alliés au simulateur que l'on a codé permettent des calculs à une précision arbitrairement grande. Les actions sont discrètes mais les états sont continus. La visualisation de la récompense et de la fonction de valeur, bien qu'elle soit plus dure à interpréter que sur le GridWorld apporte de la valeur à la discussion.\\

  Le highway, une fois qu'il sera modifié pour compenser les défauts de celui de Syed, est un problème ambivalant. Bien qu'il soit formellement discret et puisse être vu sous cet aspect comme le GridWorld, on le traitera dans certaines expériences comme continu (il faudra alors définir un set de features qui fonctionne, problème qui n'est pas sans soulever quelques difficultés). Il a ceci de plus que le GridWorld qu'il permet de chouettes démos, puisqu'il est assez visuel dans sa représentation : l'ordi conduit la voiture, évite les collisions, etc.\\

* Les algorithmes
  Dans un premier temps, on ne va tester que deux algorithmes, ANIRL d'Abbeel \cite{abbeel2004apprenticeship} et SCIRL de nous. Cela parce qu'ils sont déjà implémentés. ANIRL est un algo reconnu dans la littérature et beaucoup de papier l'utilisent comme comparaison, il est donc légitime que nous nous comparions également à lui. SCIRL est notre petit nouveau dont nous allons essayer d'exhiber une preuve empirique du fait qu'il fonctionne.\\

  On va réutiliser les implémentations d'ANIRL utilisées lors du travail sur LSTD$\mu$. Il faudra adapter un brin pour permettre une résolution exacte dans les cas qui l'exigent, mais de manière générale l'algo reste le même, à savoir la variante décrite comme la méthode par projection, dont les seuls paramètres propres (c'est à dire qui ne sont pas liés à une sous routine particulière) sont liés au critère d'arrêt. La valeur $t = ||\mu_E - \bar\mu||_2$ doit pouvoir passer en dessous d'une certaine valeur $\epsilon_A$. Dans le cas où cela est impossible (à cause d'erreurs dans ce que retournent les routines, par exemple), l'algorithme s'arrête après $T_A$ itérations.\\

  L'implémentation de SCIRL est la dernière en date, à savoir celle où la descente de sous-gradient est normalisée (l'update est $\theta_{t+1} = \theta_t -\alpha_t {\Delta\theta\over N ||\Delta\theta||_2}$) et où l'on retourne le meilleur résultat obtenu (celui avec le plus petit gradient) et non le dernier. Les paramètres propres de cet algorithmes sont la fonction $l$, le pas d'apprentissage $\alpha$, le nombre maximum d'itérations $T_S$, un seuil sur la norme du gradient $\epsilon_S$ et le vecteur récompense initial $\theta_0$. On prévoit à priori d'utiliser partout la fonction $l$ classique qui consiste à renvoyer $1$ en cas de désaccord avec l'expert et à renvoyer $0$ en cas d'accord. De la même manière on prévoit à priori d'initialiser le vecteur récompense $\theta$ à 0.

* Considérations philosophiques sur le plan d'expériences proposé
  
  Dans un premier temps, les deux algorithmes considérés vont être testés sur les trois benchmarks dans le cas "parfait" où ils disposent de toute l'information nécessaire. Cela signifie pour le GridWorld et le highway l'exploitation des probabilités de transition par des algorithmes de programmation dynamique exacts et pour le pendule l'utilisation de données du simulateur en nombre suffisant pour permettre une grande précision.\\

  Un des buts de cet démarche est de faire une démonstration de la pertinence de SCIRL en le comparant ses résultats à ceux d'un algo reconnu de la litérature à savoir ANIRL. Cela confirmerait les résultats théoriques obtenus par Bilal et Matthieu.\\

  L'autre but, beaucoup plus pragmatique, est de pouvoir trouver les paramètres propres des algorithmes en s'affranchissant des doutes sur les sous routines pusique celles-ci sont des sous routines exactes ou approchées mais très bonnes et ayant fait leurs preuves par ailleurs dans le cas du pendule.\\

  Dans un second temps, on passera en conditions complètement dégradées, où seules sont accessibles les données de l'expert. Sur les deux benchmarks les plus durs, ANIRL ne devrait pas être en mesure de fournir de résultats. Il faudra alors comparer les résultats fournis par SCIRL à une ou plusieurs baselines aléatoires afin de montrer qu'ils sont meilleurs. Ce round d'expériences a pour but de montrer que notre algo réussit là où les autres échouent, c'est à dire dans les cas qui se rapprochent des cas d'application rééls.\\

Pour augmenter la pertinence de la démarche dans le cas dégradé, il faudra assez rapidement introduire d'autres algorithmes, notamment \cite{boularias2011relative} qui ont des prétentions relativement similaires aux nôtres. On pourra alors comparer les résultats et se positionner. Si même Relative Entropy ne permet pas d'obtenir des résultats dans ces conditions, alors il faudra introduire de l'information petit à petit (on pourra faire de même pour ANIRL) jusqu'à ce que l'algo fonctionne au même niveau que SCIRL dans le cas entièrement dégradé. On pourra alors dire "tel et tel algo ont besoin d'exactement tant de données en plus que nous pour arriver au même résultat". Ca a déjà en partie été fait sur ANIRL lors du travail sur LSTD$\mu$.

* Choix des paramètres
  Certains paramètres propres des algos devront être réglés, les valeurs qui fonctionnent n'étant pas encore connues. Il faut également mener une réflexion sur la valeur des paramètres des sous routines utilisées.\\

  En ce qui concerne le set de features permettant le travail sur le highway dans le cas dégradé, un bon indicateur pourrait être la bonne résolution du problème par LSPI. Aussi une des expériences qui devra être tentée est la résolution du highway par LSPI, en comparant la qualité des politiques trouvées avec celles trouvées par les algos de programmation dynamique.\\

  Le choix du paramètre de SCIRL $\alpha(t)$ se fera en fixant $T_S$ à une valeur réellement énorme (correspondant à plusieurs heures de calcul) et en regardant l'évolution du gradient en fonction de la valeur choisie pour le paramètre $\alpha(t)$ parmis un set de départ. Un tatonnement manuel guidé par des observations telles que "Quelle est la valeur minimale du gradient ?", "Au bout de combien d'itération est-elle atteinte ?" sur le set prédéfini devrait au final amener à trouver un jeu de paramètres pour chacun des benchmarks.\\
* Expériences
  On a deux algos à tester, sur 3 benchmarks, dans deux types de conditions. En tout et pour tout cela donne 12 expériences à faire tourner. Il y a des points communs entre les différentes expériences\\

** Définition des experts et données dans le cas dégradé
*** GridWorld
    Récompense 1 en haut à droite, 0 partout. L'expert est entrâiné par programmaton dynamique. On fournit une trajectoire de l'expert, sachant qu'il part du bas à gauche.
*** Pendule inversé
    Récompense définie dans \cite{lagoudakis2003least}. On fournit une trajectoire tronquée au bout de 3000 pas de temps. L'expert est entraîné par LSPI.
*** Highway
    Récompense correspondant au "Fast driving" des expériences précédentes, à savoir $\theta = [0.5~0.25~0.25]$, avec les composantes correspondant respectivement à la feature de vitesse, de non collision et de non sortie de route définie par Syed. L'expert est entraîné par programmation dynamique. On fournit une trajectoire d'une longueur correspondant au passage de trois voitures rouges.
** Baselines aléatoires et barres d'erreur
   Pour tous les problèmes, deux types de baselines aléatoires sont étudiées, celle consistant à entraîner un expert sur une récompense choisie aléatoirement et celle consistant à choisir aléatoirement à chaque pas une action.

   Dans les cas dégradés, on présentera la moyenne et la variance des résultats sur 100 runs. Un run s'entend comme une reprise à zéro de l'expérience, génération des données par l'expert comprise.
** Généralités sur le réglage de paramètres
  De manière générale, les algorithmes de programmation dynamique ne nécessitent pas de réglage de paramètres, dans les cas parfait sur le GridWorld et le Highway on aura donc uniquement à régler les paramètres propres de SCIRL et ANIRL.\\

  Dans tous les cas parfait, on fixera les seuil d'arrêt des algorithmes à $1\%$ de la valeur de $||\mu_E(s_0)||_2$ et on règlera le nombre maximum d'itérations à 10 fois la valeur nécessaire au passage sous le seuil. On diminuera le seuil d'un facteur 10 autant de fois que nécessaire pour obtenir des performances de l'agent similaires à celles de l'expert. Dans le cas de SCIRL, on utilisera un $\alpha(t)$ tel que $\alpha(t) = A/t$. On testera pour quelques valeurs de $A$ de 0.1 à 10000 et en observant le comportement de la norme du gradient on en déduira à la main un $\alpha(t)$ de la forme $\alpha(t) = {A\over Bt}$ qui permet l'arrêt de l'algorithme en moins d'un millier d'itérations.\\

  Dans tous les cas pour SCIRL, on utilisera $\theta_0 = \mathbf{0}$ et $l(s,a)$ telle que $l(s,a)=1$ si $a\neq \pi_E(s)$ et $0$ sinon.\\

  Sur le GridWorld et le Highway, on nourrit SCIRL avec une base de données comprenant chaque état et l'action de l'expert associée (dans le cas parfait). Sur le Pendule on lui donne la même chose qu'à ANIRL (qui sur les deux autres problème a accès à la politique en tant que fonction).\\
** LSPI dans les cas dégradés
  Dans tous les cas dégradés, ANIRL sera instancié avec LSPI comme solveur de MDP et LSTD$\mu$ dans ses versions /on/ et /off-policy/ pour $\mu_E$ et $\mu$. Pour chacun des becnhmarks, une sous expérience sera menée afin de déterminer les paramètres de LSPI permettant de résoudre le MDP /à condition de disposer d'une quantité de données suffisante/.\\

  Ces paramètres sont $\phi$ la fonction de feature et $\lambda$ le coefficient de régularisation. Le seul exemple pouvant poser problème est le highway. On essaiera avec un réseau de RBF et $\lambda$ à $0.1$. En cas d'échec on denséifira le réseau et testera plusieurs valeurs de $\lambda$ entre $0.01$ et $0.5$.

  Une fois ces paramètres obtenus, on observera le comportement de LSPI lorsqu'il ne dispose plus, pour résoudre le MDP, que des données fournies par l'expert. On sait déjà que ça ne fonctionne pas dans le cas du pendule et que cela fonctionne dans le cas du GridWorld.\\

  Un échec à ce stade étant rédhibitoire pour ANIRL, on n'essaiera pas de le faire tourner si LSPI n'est pas en mesure de mener sa tâche à bien.\\
** Estimation de $\mu_E(s,a)$ dans les cas dégradés
   Par défaut, SCIRL sera instancié avec LSTD$\mu$ pour estimer $\mu_E(s,a)$. On cherchera le paramètre $\lambda$ de LSTD$\mu$ comme pour LSPI, en comparant la valeur obtenue par LSTD$\mu$ à celle obtenue par programmation dynamique ou par Monte-Carlo selon le problème.\\

En cas d'échec (à craindre dans le cas du highway) en présence des seules données de l'expert, il faudra envisager d'employer les autres méthodes trouvées par Matthieu.
** Expérience : ANIRL, GridWorld, Parfait
   On instancie ANIRL avec les algos de programmation dynamique. On lui donne $\pi_E$, la politique de l'expert, en argument.
** Expérience : ANIRL, Pendule, Parfait
   On instancie ANIRL avec LSPI en tant que solveur de MDP et Monte-Carlo pour calculer les différentes /feature expectations/. Les paramètres de LSPI sont directement repris de \cite{lagoudakis2003least}. La longueur du Monte-Carlo est telle que la norme 2 de l'écart moyen à la moyenne sur 100 runs est de moins de $1$\% de la norme 2 de la moyenne. càd :
\begin{equation}
{1\over 100}\left|\left|\sum_{i=1}^{100}\hat \mu^i_E(s_0)-\bar \mu_E(s_0)\right|\right|_2 < 0.01||\bar \mu_E(s_0)||_2 
\end{equation}
** Expérience : ANIRL, Highway, Parfait
   On instancie ANIRL avec les algos de programmation dynamique. On lui donne $\pi_E$, la politique de l'expert, en argument.
** Expérience : SCIRL, GridWorld, Parfait
   SCIRL est instancié avec un algo de programmation dynamique pour calculer $\mu_E(s,a)$.
** Expérience : SCIRL, Pendule, Parfait
   SCIRL est instancié avec un Monte-Carlo aussi long que celui utilisé pour ANIRL afin de calculer $\mu_E(s,a)$.
** Expérience : SCIRL, Highway, Parfait
   SCIRL est instancié avec un algo de programmation dynamique pour calculer $\mu_E(s,a)$.
** Expérience : ANIRL, GridWorld, Dégradé
   On reprend les paramètres propres trouvés dans le cas parfait.
** Expérience : ANIRL, Pendule, Dégradé
   Ne sera pas tentée, LSPI ne peut résoudre le problème.
** Expérience : ANIRL, Highway, Dégradé
   Subordonné à la réussite de LSPI sur les seules données de l'expert. Si on tente, on utilisera les paramètres du cas parfait.
** Expérience : SCIRL, GridWorld, Dégradé
   On reprend les paramètres propres trouvés dans le cas parfait.
** Expérience : SCIRL, Pendule, Dégradé
   On reprend les paramètres propres trouvés dans le cas parfait
** Expérience : SCIRL, Highway, Dégradé
   Subordonné à la réussite de LSPI sur les seules données de l'expert. Si on tente, on utilisera les paramètres propres du cas parfait.

\bibliographystyle{plain}
\bibliography{../Biblio/Biblio}
