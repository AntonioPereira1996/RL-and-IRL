#+TITLE: Reducing the dimentionality of the reward space in the Inverse Reinforcement Learning problem
#+AUTHOR: \IEEEauthorblockN{Edouard Klein}\IEEEauthorblockA{Supélec-metz Campus,\\IMS Research group, France\\Equipe ABC,\\LORIA-CNRS, France} \and \IEEEauthorblockN{Matthieu Geist}\IEEEauthorblockA{Supélec-metz Campus,\\IMS Research group, France} \and \IEEEauthorblockN{Olivier Pietquin}\IEEEauthorblockA{Supélec-metz Campus,\\IMS Research group, France\\UMI 2958 CNRS, \\GeorgiaTech, France}

#+begin_src emacs-lisp :results silent :exports none
(unless (find "IEEp" org-export-latex-classes :key 'car
          :test 'equal)
  (add-to-list 'org-export-latex-classes
	       '("IEEE"
		 "\\documentclass{IEEEtran}
                  [NO-DEFAULT-PACKAGES]"
		 ("\\section{%s}" . "\\section*{%s}")
		 ("\\subsection{%s}" . "\\subsection*{%s}")
		 ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
		 ("\\paragraph{%s}" . "\\paragraph*{%s}")
		 ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
  )
 #+end_src
#+EXPORT_EXCLUDE_TAGS: code
#+LaTeX_CLASS: IEEE
#+LaTeX_CLASS_OPTIONS: [conference]
#+LATEX_HEADER: \usepackage{blkarray}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{tabularx}
#+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \newtheorem{lemma}{Lemma}



#+begin_abstract
Blah
#+end_abstract

#+begin_LaTeX
\IEEEpeerreviewmaketitle
#+end_LaTeX

* Introduction
  In the /Reinforcement Learning/ (RL) framework, an agent is left to find the behavior that, in the long run, maximizes a cumulative reward. By correctly defining the reward, one can use the RL framewrk to make an agent fulfil a certain task.\\

  Some tasks make defining the corresponding reward can be a daunting process. Surely, one has at least an /idea/ about what states are preferable to others. This is however not enough to precisely define the numerical reward that will yeld the desired behavior. An example of such task can be the task of driving a car. It is preferable not to be too close to the car in front of ours. Similarily it is preferable not to brake too hard in order to avoid being rear-ended. But exactly how much is it preferable not to break too hard over not being too close to the car in front of ours ? Variations between the relative weights of these two criteria in the reward will certainly lead to very different driving behaviors.\\

  A proposed workaround is the /Inverse Reinforcement Learning/ (IRL) method, where one tries to learn the reward optimized by an expert\footnote{The expert may be non human, in which one hope to use a slow or costly expert system to train a less costly agent.}. Then, the /idea/ about what states are preferable to others can be empirically used by a human expert driving the car. The reward function can be infered from the data of the demonstration.\\
* Background
  Often, the RL problem is solved in a /Markov Decision Process/ setting : the agent is said to be in a state $s_t\in S$ which respect the Markovian criterion (it contains all the information the agent needs) where it has to choose an action $a\in A$ which will make it transit to another state $s_{t+1}$, receiving a reward $R(s_t)$.\\

  The reward is seen here as a mapping from the states only to the reals. This is not normalized in the literature, some authors using a mapping $S \times S \rightarrow \mathbb{R}$ (e.g. in \cite{ng1999policy}) or a mapping $S \times A \times S \rightarrow \mathbb{R}$. We argue that one should not take the action into account when computing the reward. First because the action is irrelevant, what the reward judges is the consequences of an action. No matter the means, the result is what is important. If an action is risky and has yeld a high reward by chance, this will likely not happen again, and the reward need not be lowered by the taking of a risky action as an upcoming transition will likely give the information that taking this action may not be a good idea. Second, in the case of IRL, recording the action can be difficult. It is possible to record the state an agent is in from an external point of view, but guessing the action from the outside is a supplementary engineering problem we shall get rid of if we can. As transitions follow each others, we also argue that there is no use in taking both states of the transition into account. The following state will be present in the next transition, so the associated reward can be computed then. Plus, needing both states to compute the reward is a sign that the states may not respect the markovian criterion. The $S \rightarrow \mathbb{R}$ mapping that $R$ is is represented as a state indexed column vector.\\

  Actions are defined by the probability with which they make the agent transit from one state to the other. Formally, to each action $a$ is associated a $n\times n$ matrix $P_a$ whose element $(i,j)$ is the probability to transit to state $j$ when taking action $a$ in state $i$. A policy is a mapping $\pi, S\rightarrow A : s \mapsto \pi(s)$ which can be defined the same way, that is to say by associating it with a $P_\pi$ matrix whose $i$-th line is the $i$-th line of the $(P_a, a=\pi(s))$ matrix. We will from now on speak about action and policies using indiscriminately  the notation $a$ and $\pi$ or $P_a$ and $P_\pi$.\\

  Each policy can be associated with its /value function/, a mapping $S\rightarrow \mathbb{R}$ which associates each state with the expected sum of discounted rewards an agent will get by following policy $\pi$ from this state. Formally, given a policy $\pi$, the value function is defined by : $V^\pi(s) = E\left[\left.\sum\limits_t \gamma^tR(s_t)\right|s_0=s,\pi\right]$ and represented, as the reward, by a state indexed column vector.\\

  The RL problem can be casted as finding the /optimal policy/ (often noted $\pi^*$), the value function of which is greater, for every state,than the value function of any other policy : $\forall \pi, V^{\pi^*} \succeq V^\pi$. The value function of this optimal policy is the fixed point of the bellman operator $T$ defined as $TV^\pi=R+\gamma P_{\pi}V^{\pi}$.\\

  The IRL problem is, given an optimal policy $\pi^*$ and the set of actions $A$ from which the policy was drawn, to guess the reward with respect to which this policy is optimal. The reward being the unknown, we will sometimes add the reward in the notation of the value function. Thus, $V^\pi_R$ is the value function of policy $\pi$ under reward $R$.\\

  We denote the set of the optimal policies with respect to a certain reward $R$ as $\Pi^*(R) = \left\{\pi^* | \forall \pi, V^{\pi^*}_R\succeq  V^{\pi}_R\right\}$.\\

  Throughout this paper, we will use the transitive notion of equivalence between rewards : 
  #+begin_definition
  Two rewards $R_1$ and $R_2$ are said to be /equivalent/ if $\Pi^*(R_1)$ = $\Pi^*(R_2)$. This is noted $R_1 \equiv R_2$.
  #+end_definition
  
  A reward is said to be degenerative when it is equivalent to the null vector ($R\equiv\begin{pmatrix}0\\ \vdots\\ 0\end{pmatrix}$). Indeed the null vector admits any policy as an optimal policy.
* Related work
  The IRL problem was first defined in \cite{russell1998learning}, where its ill-posed nature was already noted : the set of reward with respect to which the expert's policy is optimal is far from being a singleton. Furthermore there exists some degenerative rewards that admit every policy as optimal and such bear no useful information.\\

  The work of \cite{russell1998learning} was further pushed in \cite{ng2000algorithms}, where a theorem was given which defines a sufficient and necessary condition for rewards to be solution of the IRL problem. The problem of the non uniqueness and possible degenerativeness of the solution was worked around using a criterion according to which the difference between the value of the expert's actions and the value of the next to best actions is maximized. A penalty term rewarding sparse solution is also introduced. For continuous (or large) problems, a criterion for sampled trajectories is proposed.\\

  The seminal work of (\cite{abbeel2004apprenticeship}) propose an iterative algorithm, where they maximize the difference between the value of the expert and the value of the second best policy. Further work (partially summed up in \cite{neu2009training}) often used the same iterative structure, changing the argument that allows to find a unique solution. In \cite{syed2008game}, \cite{syed2008apprenticeship} and \cite{boularias2011bootstrapping}, the authors use a game theoretic approach, in \cite{ratliff2006maximum}, \cite{ratliff2007boosting} and \cite{ratliff2007imitation} the IRL problem is casted as a multiclass label problem whereas in \cite{neu2007apprenticeship} and \cite{neu2009training} the reward is computed using gradient methods so that the agent's behavior matches the expert's observed behavior. Finally, maximum /a posteriori/ methods have been published ; the work in \cite{ramachandran2007bayesian} being very similar to previous work in \cite{chajewska2001learning}, which was not cast as an IRL problem. Follow ups include \cite{dimitrakakis2011bayesian} and \cite{rothkopf2008modular}. Maximum entropy priors are introduced in \cite{ziebart2008maximum}, \cite{boularias2011relative} and \cite{aghasadeghi2011maximum}.

  This paper proposes a small theorem and begins to explore its potential use in the framework of \cite{ng2000algorithms}.
* Dimensionality reduction
** Theorem
   In this subsection, we will show that there exists a manifold of dimension $n-2$ so that every non degenerative reward is equivalent to at least one element of the manifold.\\

#+begin_lemma
\label{alpha.lemma}
Let $R_1$ be a reward vector, \\
Let $R_2$ be a reward vector so that $\exists \alpha > 0, R_2=\alpha R_1$ \\
The following holds:  $R_1\equiv R_2$
#+end_lemma

#+begin_lemma
\label{lambda.lemma}
Let $R_1$ be a reward vector, \\
Let $\mathbf{1}$ be the column vector whose $n$ elements are all equal to $1$,\\
Let $R_2$ be a reward vector so that $\exists \lambda \in \mathbb{R}, R_2= R_1 + \lambda\mathbf{1}$,\\
The following holds:  $R_1\equiv R_2$
#+end_lemma

   #+begin_theorem
   Let $M = \{R|\mathbf{1}^TR =  0, ||R||_1 = 1\}$ be,\\
   The following holds : $\forall R \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}, \exists R'\in M, R'\equiv R$.
   #+end_theorem

** Linear programming constraints
   In \cite{ng2000algorithms}, we are given a necessary and sufficient condition for a reward $R$ to admit a given policy as optimal : for every action $P_a$ the expert following $P_\pi$ had the possibility to take, the following matrix inequality must be met : 
   \begin{equation}
   \label{ng2000algorithms.eqn}
   (P_\pi-P_{a})(I-\gamma P_\pi)^{-1}R\succeq 0
   \end{equation}

   Although the proof can be found in the formentionned paper, we find useful to recall its main argument here : this inequalities stem from the fact that for every action $a$, the expected value of the next state must be less than or the same as the expected value of the next state for taking action $\pi(s)$. This is written as $P_\pi V^\pi \succeq P_a V^\pi$. Consequently, the $i$-th line of the $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix is a constraint on $X$ that, if satisfied, will make it preferable to choose action $\pi(i)$ over $a$ in state $i$.\\
 
   Equation \ref{ng2000algorithms.eqn} yelds at most $Card(A)\cdot n - n = (Card(A)-1)n$ constraints. There is $Card(A)$ matrices $P_a$, each yelding $n$ constraints. $n$ of there, however, are null because once for each state we will have $a=\pi(s)$ and the resulting line will be filled with zeros.\\

   This is a Linear Programming problem. By adding the supplemantary constraints stemming from the definition of $M$, i.e. that $\mathbf{1}^TR=0$ and $||R||_1=1$, we restrict the solutions to the previously defined $n-2$-dimensional manifold.\\
* Preliminary work
* Conclusion
#+begin_LaTeX
\bibliographystyle{IEEEtran}
\bibliography{../../Biblio/Biblio}
#+end_LaTeX
