\documentclass[xcolor=x11names,12pt]{beamer}
%\usetheme[secheader]{Boadilla} 
\mode<presentation>{\usetheme{I6pd2}}

\usepackage{multicol} 
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{grffile}
\usepackage{listings}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{pifont}
\usepackage{slashbox}
\include{headertikz}
\usetikzlibrary{through,shapes,arrows,decorations.pathmorphing,backgrounds,positioning,fit} 

\newcommand{\argmin}{\mathop{\mathrm{min}}}
\usepackage[orientation=portrait,size=a0,scale=1.4]{beamerposter}


\newcommand{\conf}[1]{\newcommand{\insertconf}{#1}}
\newenvironment{WholeWidthBox}[1]{
  \begin{columns}
    \begin{column}{0.98\textwidth}
      \begin{block}{#1}
        \begin{hfill}
}{
        \end{hfill}
      \end{block}
    \end{column}
  \end{columns}
}

\newcommand{\TwoBoxes}[7]{%Width1 Title1 Content1 Width2 Title2 Content2 MinHeight
  \begin{columns}
    \begin{column}{#1\textwidth}
      \begin{block}{#2}
        \begin{columns}
          \begin{column}{0.001\textwidth}
            \vspace{#7}
          \end{column}
          \begin{column}{\textwidth}
            \centering
            #3
          \end{column}
        \end{columns}       
      \end{block}
    \end{column}
    
    \begin{column}{#4\textwidth}
      \begin{block}{#5}
        \begin{columns}
          \begin{column}{0.001\textwidth}
            \vspace{#7}
          \end{column}
          \begin{column}{\textwidth}
            \centering
            #6
          \end{column}
        \end{columns}       
      \end{block}
    \end{column}
  \end{columns}
}


%Start of document



\tikzstyle{state}=[circle,
thick,
minimum size=1.0cm,
draw=blue!80,
fill=blue!20]
\tikzstyle{action}=[rectangle,thick,
minimum size=1.0cm,
draw=orange!80,
fill=orange!20]
\tikzstyle{element}=[rectangle,
line width=1.5mm,
minimum size=1.0cm,
draw=blue!80,
fill=blue!20]
\tikzstyle{action}=[rectangle,
line width=1.5mm,
minimum size=1.0cm,
draw=orange!80,
fill=orange!20]



\conf{European workshop on reinforcement learning}
\title{Batch, Off-policy and Model-free Apprenticeship Learning}
\author{\underline{Edouard Klein}$^{\dag\ddag}$, Matthieu Geist$^\dag$ and Olivier Pietquin$^\dag$\\\texttt{firstname.lastname@supelec.fr}}
\date{\today}
\institute[Supélec]{$\dag$Supélec UMI 2958 (GeorgiaTech - CNRS), France\\$\ddag$Equipe ABC UMR 7503 (LORIA-CNRS), France}
\newlength{\columnheight}
\setlength{\columnheight}{105cm}


\begin{document}
\begin{frame}

%%%%%%%%%%%%% DEBUT PREMIERE LIGNE %%%%%%%%%%%%%%%%%
\begin{WholeWidthBox}{Introduction}
  Apprenticeship learning can be defined as learning control policies from demonstration by an expert. An efficient framework for it is inverse reinforcement learning
  (IRL). Based on the assumption that the expert maximizes a utility function, IRL aims at learning the underlying reward from example trajectories. Many IRL algorithms rely on the computation of \emph{feature expectations}, which is done through Monte Carlo simulation. In this paper, we introduce a temporal difference method, namely LSTD-$\mu$, to compute these feature expectations. This allows extending apprenticeship learning to a batch and off-policy setting.
\end{WholeWidthBox}

\vfill
%%%%%%%%%%%% DEBUT DEUXIEME LIGNE %%%%%%%%%%%%%%%%%%

\TwoBoxes{.48}{Setting}{
  \begin{columns}
    \begin{column}{.48\textwidth}
      \includegraphics{Agent003.png}
    \end{column}
    \begin{column}{.48\textwidth}
      \begin{itemize}
      \item Expert's trace
      \item MDP : 
        \begin{itemize}
        \item State space $S$
        \item Action space $A$
        \item Unknown reward being maximized by the expert $R$
        \item Discount factor $\gamma$
        \item Transition probabilities $P$
        \end{itemize}
      \item Agent implementing a policy $\pi : S\rightarrow A$
      \end{itemize}
      
    \end{column}
  \end{columns}
}
{.48}{Assumptions and Goal}{
  \begin{itemize}
  \item Assumptions
    \begin{itemize}
    \item The expert is a RL agent
    \item One can access the expert's trace
    \item The agent has the same abilities as the expert
    \end{itemize}
  \item Goal : Infer the expert's reward
    \begin{itemize}
    \item apprenticeship of the expert's task
    \item Generalization of the policy over never-seen-before states
    \item Useful when rewards are hard to tune ({\it e.g.}, driving)
    \end{itemize}
  \end{itemize}
}
{15cm}
\vfill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% TROISIEME LIGNE
\TwoBoxes{.38}{Feature Expectation}{
  \begin{equation*}
    V^\pi(s_t) = E\left[\left.\sum\limits_{i}\gamma^i r_{t+i}\right|\pi\right] = \theta^T\underbrace{E\left[\left.\sum\limits_{i}\gamma^i \phi(s_{t+i})\right|\pi\right]}_{\mu^\pi(s_t)}
  \end{equation*}
}{.58}{Importance of $\Delta \mu$}{
  \begin{equation*}
    \Delta V = |V^E(s_0) - V^\pi(s_0)| = |\theta^T\left(\mu_E(s_0) - \mu^\pi(s_0)\right)| \leq  ||\mu_E(s_0) - \mu^\pi(s_0)||_2 = \Delta\mu
  \end{equation*}
}
{6cm}
\vfill
%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Quatrième ligne
  \begin{columns}
    \begin{column}{.48\textwidth}
    \begin{block}{Abbeel and Ng's IRL}
      \begin{columns}
        \begin{column}{0.001\textwidth}
          \vspace{8cm}
        \end{column}
        \begin{column}{\textwidth}
          \centering

          \begin{tikzpicture} [scale=3]
            \node[element] (trajE) at (0,0) {$\vcenter{\hbox{\includegraphics[height=2cm]{Expert.png}}}$Trajectories} ;
            \node[action] (mech) at (3,-0.7) {\includegraphics[height=2cm]{Moulinette.png}} ;
            \node[element] (policy) at (5,-0.7) {$\vcenter{\hbox{\includegraphics[height=2cm]{Pi.png}}}$Policy} ;
            \node[action] (sim) at (8,-0.7) {$\vcenter{\hbox{\includegraphics[height=2cm]{ML.png}}}$Simulator} ;
            \node[element] (trajA) at (0,-1.4) {$\vcenter{\hbox{\includegraphics[height=2cm]{Agent.png}}}$Trajectories} ;
            \draw [->,line width=1.5mm] (trajE.east) .. controls (2,0) and (2,-0.7) .. (mech.west);
            \draw [->,line width=1.5mm] (trajA.east) .. controls (2,-1.4) and (2,-0.7) .. (mech.west);  
            \draw [->,line width=1.5mm] (mech.east) -- (policy.west);
            \draw [->,line width=1.5mm] (policy.east) -- (sim.west);
            \draw [->,line width=1.5mm] (sim.east) -- (10,-0.7) -- (10,-2.1) -- (0,-2.1) -- (trajA.south);
          \end{tikzpicture}
        \end{column}
      \end{columns}       
      
    \end{block}
    \begin{block}{LSTD-$\mu$}
      \begin{columns}
        \begin{column}{0.001\textwidth}
          \vspace{8cm}
        \end{column}
        \begin{column}{\textwidth}
          \centering
          \begin{tikzpicture}[scale=3]
            \node[element] (trajE) at (0,0) {$\vcenter{\hbox{\includegraphics[height=2cm]{Expert.png}}}$Transitions} ;
            \node[action] (mech) at (3,-0.7) {\includegraphics[height=2cm]{Moulinette.png}} ;
            \node[element] (policy) at (5,-0.7) {$\vcenter{\hbox{\includegraphics[height=2cm]{Pi.png}}}$Policy} ;
            \node[element] (trajA) at (0,-1.4) {$\vcenter{\hbox{\includegraphics[height=2cm]{Agent.png}}}$Transitions} ;
            \draw [->,line width=1.5mm] (trajE.east) .. controls (2,0) and (2,-0.7) .. (mech.west);
            \draw [->,line width=1.5mm] (trajA.east) .. controls (2,-1.4) and (2,-0.7) .. (mech.west);  
            \draw [->,line width=1.5mm] (mech.east) -- (policy.west);
            \draw [->,line width=1.5mm] (policy.east) -- (10,-0.7) -- (10,-1.4) -- (3,-1.4) -- (mech.south);
          \end{tikzpicture}
        \end{column}
      \end{columns}       
    \end{block}
    \end{column}
    
    \begin{column}{.48\textwidth}
\begin{block}{Update step}
  $t = \max\limits_{\theta}\min\limits_{\pi}\theta^T(\mu^{E}(s_0)-\mu^{\pi}(s_0))$
\end{block}

      \begin{block}{Idea}
        
        $V = V^\pi(s_t) = E\left[\left.\sum\limits_{i}\gamma^i r_{t+i}\right|\pi\right]$
        \hfill $\mu^\pi(s_t) = E\left[\left.\sum\limits_i\gamma^i\phi(s_{t+i})\right|\pi\right]$ \\
        \begin{center}
          Transition: $s_t,a_t,s_{t+1},r_t$
        \end{center}
      \end{block}
\begin{columns}
\begin{column}{.5\textwidth}
\begin{block}{LSTD}
\label{sec-3_2_1_2}

     Estimator : $\hat V^\pi(s) = \omega^T\phi(s)$
     \begin{equation*}
     \omega = LSTD_{\color{red}\phi}( \{s_t,a_t,s_{t+1}\}_t,\{{\color{red}r_t}\}_t)
     \end{equation*}
\end{block}
\end{column}
\begin{column}{.5\textwidth}
\begin{block}{LSTD-$\mu$}
\label{sec-3_2_1_3}

     Estimator : $\hat \mu^\pi(s) = \xi^{T}\psi(s)$
     \begin{equation*}
     \xi_i = LSTD_{\color{red}\psi}( \{s_t,a_t,s_{t+1}\}_t, \{{\color{red}\phi_i(s_t)}\}_t )
     \end{equation*}
\end{block}
\end{column}
\end{columns}

      %% \begin{block}{#5}
      %%   \begin{columns}
      %%     \begin{column}{0.001\textwidth}
      %%       \vspace{#7}
      %%     \end{column}
      %%     \begin{column}{\textwidth}
      %%       \centering
      %%       #6
      %%     \end{column}
      %%   \end{columns}       
      %% \end{block}
    \end{column}
  \end{columns}
\vfill
%%%%%%%%%%%%%%%%%%%%%%Cinquième ligne
  \begin{columns}
    \begin{column}{.48\textwidth}
      \begin{block}{Quality Criterion}
        \centering
        \fontsize{11pt}{11pt}\selectfont
        \resizebox{.55\columnwidth}{!}{\input{../CodeJFPDA/GridWorld/criteria_mc}}
      \end{block}
    \end{column}
    \begin{column}{.48\textwidth}
      \begin{block}{Gridworld}
        \centering
        \fontsize{11pt}{11pt}\selectfont
        \resizebox{.55\columnwidth}{!}{\input{../CodeJFPDA/GridWorld/both_error_EB}}
      \end{block}
    \end{column}
  \end{columns}
  \begin{columns}
    \begin{column}{.48\textwidth}
      \begin{block}{Inverted Pendulum (one run)}
        \centering
        \fontsize{11pt}{11pt}\selectfont
        \resizebox{.55\columnwidth}{!}{\input{../CodeJFPDA/InvertedPendulum/threshold}}
      \end{block}
    \end{column}
    \begin{column}{.48\textwidth}
      \begin{block}{Inverted Pendulum}
        \centering
        \fontsize{11pt}{11pt}\selectfont
        \resizebox{.55\columnwidth}{!}{\input{../CodeJFPDA/InvertedPendulum/threshold_EB}}
      \end{block}
    \end{column}
  \end{columns}



\end{frame}
\end{document}
