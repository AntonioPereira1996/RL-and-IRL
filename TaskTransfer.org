#+TITLE: Task Transfer on the GridWorld
#+LATEX_HEADER: \usepackage{blkarray}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{tabularx}
#+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \newtheorem{lemma}{Lemma}

* Context

  In the /Reinforcement Learning/ (RL) framework, an agent is left to find the behavior that maximizes a cumulative reward in the long run. By correctly defining the reward, one can use the RL framewrk to make an agent fulfil a certain task.\\

  Often, the RL problem is solved in a /Markov Decision Process/ setting : the agent is said to be in a state $s_t\in S$ which respect the Markovian criterion (it contains all the information the agent needs) where it has to choose an action $a\in A$ which will make it transit to another state $s_{t+1}$, receiving a reward $R(s_t)$.\\

  A policy (often noted $\pi$) is a mapping $S\rightarrow A$. Each policy can be associated with its /value function/, a mapping $S\rightarrow \mathbb{R}$ which associates each state with the expected sum of discounted rewards an agent will get by following policy $\pi$ from this state. Formally, given a policy $\pi$, the value function is defined by : $V^\pi(s) = E\left[\left.\sum\limits_t \gamma^tR(s_t)\right|s_0=s,\pi\right]$.\\

  The RL problem can be casted as finding the /optimal policy/ (often noted $\pi^*$), the value function of which is greater, for every state, as the value function of any other policy : $\forall \pi, V^{\pi^*} \succeq V^\pi$.\\

  Any optimal policy is the fixed point of a greedy mechanism : by choosing, at each step, the action that maximizes the expected value function of the optimal policy for the next state one falls back to the optimal policy :$\pi^*(s_t) = \arg\max\limits_aE\left[\left.R(s) + \gamma V^{\pi^*}(s_{t+1})\right|s_t,a\right]$. This fixed point is unique, the previous equation can hence be used as a definition for the optimal policy.\\
  
  Luckily, this greedy mechanism is contractile. This means that by repeatedly computing the value function of any policy and then defining a new policy greedily with respect to the computed value function, one will utimately attain the optimal policy and the associated value function.\\

  For a certain  tasks, defining the corresponding reward can be a daunting process. Surely, one has at least an /idea/ about what states are preferable to others. This is however not enough to precisely define the numerical reward that will yeld the desired behavior. An example of such task can be the task of driving a car. Surely, it is preferable not to be too close to the car in front of ours. Surely, it is preferable not to brake too hard in order to avoid being rear-ended. But exactly how much is it preferable not to break too hard over not being too close to the car in front of ours ? Variations between the relative weights of these twe criteria in the reward will certainly lead to very different driving behaviors.\\

  A proposed workaround is the /Inverse Reinforcement Learning/ (IRL) method, where one tries to learn the reward optimized by an expert\footnote{The expert may be non human, in which one hope to use a slow or costly expert system to train a less costly agent.}. Then, the /idea/ about what states are preferable to others can be empirically used by a human expert driving the car. The reward function can be infered from the data of the demonstration.\\

  More formally, given an optimal policy $\pi^*$ and the set of actions $A$ from which the policy was drawn, one tries to guess the reward with respect to which this policy is optimal.\\

  Another step is to acknowledge that the agent may have different abilities than the expert, and use the reward as a piece of information defining a /task/. This will allow the transfer of a task from agent to agent, each fulfilling the task at the best of its abilities. For example in the driving problem, a computer will certainly have a better reflex time and a better stamina than a human, and thus could /hypothetically/ drive better than most human, increasing the overall safety of driving.\\

* Introduction

  The IRL problem was first defined in \cite{russell1998learning}, where its ill-posed nature was already noted : the set of reward with respect to which the expert's policy is optimal is far from being a singleton. Furthermore there exists some degenerative rewards that admit every policy as optimal and such bear no useful information.\\

  The work of \cite{russell1998learning} was further pushed in \cite{ng2000algorithms}, where a theorem was given which defines a sufficient and necessary condition for rewards to be solution of the IRL problem. Consinuous state space were also worked on. The problem of degenerative solutions, however, was not solved but worked around by the use of intuitive heuristics.\\

  The seminal work of (\cite{abbeel2004apprenticeship}) led to innovative algorithms (summed up in \cite{neu2009training}) allowing the portage of the IRL problem to settings where the dynamics of the system remains unknown (\cite{klein2011batch}). The problem of degenerative rewards was attacked with different heuristics.\\

  This paper aims at adding restrictions to the results of \cite{ng2000algorithms} in order to solve the ill-posedness of the IRL problem at its root and propose a view on the problem that allows for a generalization of the intuitive heuristics of the litterature. By doing so we hope to pave the way for ameliorations in the existing algorithms as well as new methods for practical IRL that would enable task transfer in real life settings.\\

* Notations
  
  The reward $R$ is here seen as a mapping from the state space $S$ to $\mathbb{R}$. Working in the discrete case, we use it as a vector $R\in \mathbb{R}^n$ indexed by the state space, the cardinality of which is $n$.\\

  The value function when follwing policy $\pi$ under reward $R$ is also defined as a vector : $V^\pi_R\in \mathbb{R}^n$ indexed by the state space. We specify the reward in the notation because in IRL the reward is the unknown and it can thus be interesting to use the value function of a policy under two different rewards.\\

  Actions are defined by the probability with which they make the agent transit from one state to the other. Formally, to each action $a$ is associated a $n\times n$ matrix $P_a$ whose element $(i,j)$ is the probability to transit to state $j$ when taking action $a$ in state $i$. A policy $\pi, S\rightarrow A : s \mapsto \pi(s)$ can be defined the same way, that is to say by associating it with a $P_\pi$ matrix whose $i$-th line is the $i$-th line of the $(P_a, a=\pi(s))$ matrix. We will from now on speak about action and policies using indiscriminately  the notation $a$ and $\pi$ or $P_a$ and $P_\pi$.\\

  The fixed point definition of the optimal policy can be rewriten as : $\forall s, \pi^*(s) = \arg\max\limits_a\left(R(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_R\right)$ using the notations we just have defined.\\

  We denote the set of the optimal policies with respect to a certain reward $R$ as $\Pi^*(R) = \left\{\pi^* | \forall \pi, V^{\pi^*}_R\succeq  V^{\pi}_R\right\}$.\\

  Throughout this paper, we will use the transitive notion of equivalence between rewards : 
  #+begin_definition
  Two rewards $R_1$ and $R_2$ are said to be /equivalent/ if $\Pi^*(R_1)$ = $\Pi^*(R_2)$. This is noted $R_1 \equiv R_2$.
  #+end_definition
  
  A reward is said to be degenerative when it is equivalent to the null vector ($R\equiv\begin{pmatrix}0\\ \vdots\\ 0\end{pmatrix}$). Indeed the null vector admits any policy as an optimal policy.
  
  #+begin_definition
  \label{startingvalue.def}
  The /starting value/ of a policy with respect to a certain reward is the value of the value function at a certain state $s_0$ where the agent or the expert usually start their trajectories.
  #+end_definition
  
  When training an agent on a problem where the reward is infered from the policy of an expert, the criteria we wish to maximize is the /starting value/ of the agent, with resspect the the /unknown true reward/.
  #+begin_definition
  \label{agentasexpert.def}
  The starting value of an agent with respect to the unknown true reward is refered to as the /agent as expert/ value.
  #+end_definition
  On a real life problem, one can not directly compute this value. It is however possible when benchmarking our algorithm on controlled experiments.
* Reducing the reward space
   In this subsection, we will show that there exists a manifold of dimension $n-2$ so that every non degenerative reward is equivalent to at least one element of the manifold.\\

#+begin_lemma
\label{alpha.lemma}
Let $R_1$ be a reward vector, \\
Let $R_2$ be a reward vector so that $\exists \alpha > 0, R_2=\alpha R_1$ \\
The following holds:  $R_1\equiv R_2$
#+end_lemma
   
   #+begin_proof
   Let $\pi \in \Pi^*(R_2)$ be.\\
   We have : 
   \begin{eqnarray*}
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_{R_2}\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_2(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(\alpha R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^t\alpha R_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\alpha\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)
   \end{eqnarray*}
   as $\alpha >0$, this is the same as :
   \begin{equation*}
   \forall s, \pi^*(s) = \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \end{equation*}
   which means that $\pi \in \Pi^*(R_1)$.\\
   The inverse path can be demonstrated by replacing $R_1$ by $R_2$ and using $1\over \alpha$, therefore $\pi \in \Pi^*(R_2) \Leftrightarrow \pi \in \Pi^*(R_1)$.
   #+end_proof
   
#+begin_lemma
\label{lambda.lemma}
Let $R_1$ be a reward vector, \\
Let $\mathbf{1}$ be the column vector whose $n$ elements are all equal to $1$,\\
Let $R_2$ be a reward vector so that $\exists \lambda \in \mathbb{R}, R_2= R_1 + \lambda\mathbf{1}$,\\
The following holds:  $R_1\equiv R_2$
#+end_lemma
  
   #+begin_proof
   Let $\pi \in \Pi^*(R_2)$ be.\\
   We have : 
   \begin{eqnarray*}
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_{R_2}\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_2(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^t\left(R_1(s_t)+\lambda\right)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]+ \gamma E\left[\left.\sum\limits_t\gamma^t\lambda\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]+ \gamma\sum\limits_t\gamma^t\lambda\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right] + \lambda + \gamma\sum\limits_t\gamma^t\lambda\right)\\
   \end{eqnarray*}
   as $\lambda + \gamma\sum\limits_t\gamma^t\lambda$ does not depend on $a$, this is the same as :
   \begin{equation*}
   \forall s, \pi^*(s) = \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \end{equation*}
   which means that $\pi \in \Pi^*(R_1)$.\\
   The inverse path can be demonstrated by replacing $R_1$ by $R_2$ and using $-\lambda$, therefore $\pi \in \Pi^*(R_2) \Leftrightarrow \pi \in \Pi^*(R_1)$.
   #+end_proof

   #+begin_theorem
   Let $M = \{R|\mathbf{1}^TR =  0, ||R||_1 = 1\}$ be,\\
   The following holds : $\forall R \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}, \exists R'\in M, R'\equiv R$.
   #+end_theorem

   #+begin_proof
   Let $R \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}$ be,\\
   Let $\lambda = -{\mathbf{1}^TR\over n}$ be,\\
   Let $\alpha = {1\over ||R+\lambda\mathbf{1}||_1}$ be,\\
   Let $R' = \alpha(R+\lambda\mathbf{1})$ be,\\
   We have :
   \begin{eqnarray}
   \mathbf{1}^TR' &=& \mathbf{1}^T\alpha(R+\lambda\mathbf{1})\\
   \mathbf{1}^TR' &=& \alpha\mathbf{1}^TR + \alpha\lambda\mathbf{1}^T\mathbf{1}\\
   \mathbf{1}^TR' &=& \alpha\mathbf{1}^TR + \alpha\lambda n\\
   \mathbf{1}^TR' &=& \alpha\mathbf{1}^TR - \alpha{\mathbf{1}^TR\over n}n\\
   \mathbf{1}^TR' &=& 0
   \end{eqnarray}
   and :
   \begin{eqnarray}
   ||R'||_1 &=& ||\alpha(R+\lambda\mathbf{1})||_1\\
   ||R'||_1 &=& \alpha||R+\lambda\mathbf{1}||_1\\
   ||R'||_1 &=& {1\over ||R+\lambda\mathbf{1}||_1}||R+\lambda\mathbf{1}||_1\\
   ||R'||_1 &=& 1\\
   \end{eqnarray}
   So $R'\in M$.

   Also, according to lemma \ref{lambda.lemma}, we have $R + \lambda\mathbf{1} \equiv R$, and according to lemma \ref{alpha.lemma}, we have $R' = \alpha(R+\lambda\mathbf{1}) \equiv R+\lambda\mathbf{1}$ which by transitivity yelds : $R' \equiv R$.
   #+end_proof

* Finding interesting rewards
   In \cite{ng2000algorithms}, we are given a necessary and sufficient condition for a reward $R$ to admit a given policy as optimal : for every action $P_a$ the expert following $P_\pi$ had the possibility to take, the following matrix inequality must be met : 
   \begin{equation}
   \label{ng2000algorithms.eqn}
   (P_\pi-P_{a})(I-\gamma P_\pi)^{-1}R\succeq 0
   \end{equation}

   Although the proof can be found in the formentionned paper, we find useful to recall its main argument here : this inequalities stem from the fact that for every action $a$, the expected value of the next state must be less than or the same as the expected value of the next state for taking action $\pi(s)$. This is written as $P_\pi V^\pi \succeq P_a V^\pi$. Consequently, the $i$-th line of the $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix is a constraint on $X$ that, if satisfied, will make it preferable to choose action $\pi(i)$ over $a$ in state $i$.\\
 
   Equation \ref{ng2000algorithms.eqn} yelds at most $Card(A)\cdot n - n = (Card(A)-1)n$ constraints. There is $Card(A)$ matrices $P_a$, each yelding $n$ constraints. $n$ of there, however, are null because once for each state we will have $a=\pi(s)$ and the resulting line will be filled with zeros.\\

   This is a Linear Programming problem. By adding the supplemantary constraints stemming from the definition of $M$, i.e. that $\mathbf{1}^TR=0$ and $||R||_1=1$, we restrict the solutions to the previously defined $n-2$-dimensional manifold.\\

   Drawing ideas from the simplex algorithm of the LP framework (http://www2.isye.gatech.edu/~spyros/LP/LP.html), we propose a compact and exhaustive description of the solutions of this augmented LP problem.\\

   Let $C$ be the matrix created by slecting the $m$ non null lines of the constraints matrices $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$, so that all lines are normalized (this does not change the constraints) and every line is unique. In order to improve the computational complexity of the algorithm, it may be possible to further restrict what goes in the $C$ matrix, but this is a little off-topic and not necessary.\\

   Let $X$ be the unknown vector. All the forementionned constraints can now be expressed as follow :
   \begin{eqnarray}
   &CX \succeq 0\\
   &\mathbf{1}^TR=0\\
   &||X||_1=1
   \end{eqnarray}
** Standard form
   
   These constraints will now be put in the /standard form/ $AX=b, X\succeq 0$, typically used in the simplex algorithm (which is not fully applicable here because we don't have a cost function).\\
  
  The $X\in \mathbb{R}^n$ part is not satisfactory because in the standard form all unknowns must be grater than or equal to 0. To get to this form, for every component $x_i$ of $X$, we define $x_i^+\geq0$ and $x_i^-\geq0$ so that $x_i = x_i^+ - x_i^-$. We now define the $X'$ matrix as 
  \begin{equation}
  X'=\begin{pmatrix} x_1^+\\x_1^-\\ \dots \\ x_n^+\\x_n^- \end{pmatrix}
  \end{equation}
  Accordingly, the $C'$ matrix is defined as (using $c_i$ to denote the $i$-th column of C) :
  \begin{equation}
  C'=\begin{pmatrix} c_1 | -c_1 | c_2 | -c_2 | \dots |c_n|-c_n \end{pmatrix}
  \end{equation}
  and the $\mathbf{1}' \in \mathbb{R}^{2n}$ vectoris defined as :
  \begin{equation}
  \mathbf{1}'=\begin{pmatrix} 1,-1, 1, -1\dots 1,-1\end{pmatrix}^T
  \end{equation}

  We are now a bit closer to the standard form :
  \begin{eqnarray}
  &C'X' \succeq 0 \\
  &\mathbf{1}'^TX'=0\\
  &||X||_1=1\\
  &X'\succeq 0\\
  \end{eqnarray}
  
  The last thing to do is to introduce $m$ positive slack variables in the form of a matrix
  \begin{equation}
  S = \begin{pmatrix}s_1\\ \vdots\\ s_{m}\end{pmatrix} \succeq 0
  \end{equation}
  This allows us to change the inequality into an equality : if $a\geq b$, then $\exists s \geq 0, a-s = b$. We finally obtain something in the standard form, the last two lines of which respectively represents the constraints $\mathbf{1}'^TX'=0$ and $||X||_1=1$ :
  \begin{eqnarray}
  \label{LPStandardForm.eqn}
  \begin{blockarray}{(cc)}
  \begin{block*}{c|c}
  C'& -Id_m  \\
  \cline{1-2}
  \begin{block*}{c|c}
  \mathbf{1}'^T&0 \\
  \end{block*}
  \cline{1-2}
  \begin{block*}{c|c}
  \mathbf{1}^T\mathbf{1}^T&0 \\
  \end{block*}
  \end{block*}
  \end{blockarray} 
  \begin{blockarray}{(c)}
  \begin{block*}{c}
  X' \\
  \cline{1-1}
  \begin{block*}{c}
  S\\
  \end{block*}
  \end{block*}
  \end{blockarray}
  = 
  \begin{blockarray}{(c)}
  \begin{block*}{c}
  0 \\
  \vdots \\
  0 \\
  1\\
  \end{block*}
  \end{block*}
  \end{blockarray}\\
  \label{C1.eqn}
  X'\succeq 0\\
  \label{C2.eqn}
  S \succeq 0
  \end{eqnarray}

** Basic feasible solutions
   We now have $m+2$ equations in $2n+m$ variables. This is not solvable by usual means. Let us not forget that appart for the last two lines added in order to make the solution unique and thus the whole system solvable, all the constraints are inequalities. As the solution to such a system of inequalities is not unique, we use the trick from the simplex algorithm that consist in choosing $m+2$ columns and solving the resulting linear system (if possible). The remaining variables are set to zero. This dictates which inequalities are binding are which are not : if a column concerning a slack variable (say, $s_i$) is not choosen, the corresponding inequality becomes an equality ($C^iX = 0$, with $C^i$ being the $i$-th line of C). If both column concerning a state (say, $x_j^+$ and $x_j^-$ are not choosen, the corresponding two lines of the $X'\succeq 0$ condition become binding and thus $x_j = 0$.\\

   Every solvable $(m+2) \times (m+2)$ system, that is to say every system resulting from a selection that does not make two incompatible inequalities binding will result in what is called a /basic feasible solution/ (bfs) in the LP literature. From every /bfs/ abiding by the additional constraints of inequalities \ref{C1.eqn} and \ref{C2.eqn}, it is easy to go back to the corresponding reward $R^{bfs}$ by undoing the transformation that allowed us to get to the standard form. This is simply done by :
   \begin{equation}
   R^{bfs}_i = X'^{bfs}_{2i-1} - X'^{bfs}_{2i}
   \end{equation}
   
   
   A patient systematic enumaration of all the $(m+2) \times (m+2)$ systems will yeld a small set of interesting rewards (see section [[Experiments]]).\\

   If the computational comlpexity of the exhaustive enumaration is too high, one can try to reduce it by diverse means, such as not even enumerating the obviously not solvable systems (for example one involving both $x_j^+$ and $x_j^-$ for a any given $j$) or removing useless constraints (such as constraints being implied by others) in order to decrease $m$.\\

   Another course of action is to use heuristics. In \cite{ng2000algorithms}, the authors suggest adding a cost function to the linear program in order to find meaningful rewards. The search space they were dealing with was open, and thus a cost function was necessary to lead the search. They used natural criterion in this cost function that made the search successful. Limiting the search to a finite object as we propose makes this cost function no longer necessary, although we can still select the kind of reward we want by carefully selecting which column to include when building a $(m+2) \times (m+2)$ system.\\

   One natural criteria introduced in \cite{ng2000algorithms}, and later ingrained at the core of \cite{abbeel2004apprenticeship} is to assume that in each state the expert did not had much of a choice and had to take the action it took, because the corresponding state action value was actually higher (and not just equal to) any other action. In our framework, a reward satisfying this criteria will be found by solving the systems were as much slack variables as possible are selected (it is always possible to select all the slack variables, are there are only $m$ of them, but nothing proves such a system will be solvable and one will have to resort to selecting all slack variables but one and so on).\\

   Another criteria mentioned in \cite{ng2000algorithms} is the simplicity of the reward. In a MDP with a non completely erratic behavior, selecting as less slack variables as possible will lead to a scarce reward (after a wise translation of vector $\lambda \mathbf{1}$). Such a reward will probably admit as optimal more policies than only the expert's, and we argue that this kind of reward is more prone to represent the task at sake than the ones satisfying the other criteria, which tend to lead to an imitation of the expert. Another argument for this is that human defined reward on known problems tend to be scarce.\\

* Experiments
** /Free slacks/ search on the GridWorld
   
*** Goal
   The goal of this  experiment is to reproduce some of the results of \cite{ng2000algorithms} with the /free slacks/ search method and to investigate task transfer on this setting.

*** Protocol

   The setting is a noisy $5\times 5$ Gridworld : the expert can choose from the $4$ compass directions, and there is a $30\%$ chance of a noisy execution, sending it in one of the other $3$ directions. The reward on which the expert is trained is a reward null everywhere, except for one corner, where it is 1. By convention, the corner opposite to where the reward is non null is viewed as the starting state, noted $s_0$. The optimal policy is computed by a /Dynamic programming/ algorithm and differs from the one in \cite{ng2000algorithms}.\\

   The optimal policy, along with the description of the four possible actions is fed to our algorithm in its /free slacks/ variant.\\

   The agent to be trained in order to test the task transfer is more powerful than the expert, in the sense that on top of moving in the $4$ compass directions it can also move in the $4$ diagonal directions. The agent has the same movements as a king in the game of chess. The action is still noisy : every movement has a 72% probability of sucess. If it fails the agent will move in any other direction.\\

   This agent is first trained on the true reward (the same that has been used to train the expert) and the /agent as expert/ value (see Def. \ref{agentasexpert.def}) is computed.\\

   The agent is then trained on all the rewards output by our algorithm, and the corresponding starting values (see Def. \ref{startingvalue.def}) are compared to the /agent as expert/ value.\\
*** Results

    The three rewards our algorithm has found are shown Fig. \ref{slacksfreeR.fig}. Trained on any of these rewards, the expert would exhibit the same performance as if trained on the true reward.\\

    The comparison between the /agent as expert/ value and the starting values of the agent when trained on the found rewards can be found Table \ref{slacksfreeR.table}. For every reward, the agent is better than the expert, which is to be expected when the agent is more potent than the expert. For two rewards out of three, the agent is as good as if it had been trained on the true reward. When observing the policies, one can notice the heavy use the agent make of its ability to move diagonally. This is a successful example of task transfer.

   #+begin_figure
\centering

\subfigure[Reward 1]{
   \label{slacksfreeR1.fig}
   \includegraphics[width=0.4\textwidth] {TT_5x5_R1.pdf}
 }
\subfigure[Reward 2]{
   \label{slacksfreeR2.fig}
   \includegraphics[width=0.4\textwidth] {TT_5x5_R2.pdf}
 }
\subfigure[Reward 3]{
   \label{slacksfreeR3.fig}
   \includegraphics[width=0.4\textwidth] {TT_5x5_R3.pdf}
 }

\caption{The three rewards output by the /slacks free/ search method on the $5\times 5$ Gridworld}
\label{slacksfreeR.fig}
   #+end_figure

    #+LABEL: slacksfreeR.table
    #+CAPTION: /Agent as expert/ value and starting value for all rewards
    #+ATTR_LaTeX: tabularx align=|X|X|X|X|X| width=\textwidth
    |---------------+-------------------+----------------------------------------+----------------------------------------+----------------------------------------|
    | Reward        | $R$ (true reward) | $R_1$ (see Fig.\ref{slacksfreeR1.fig}) | $R_2$ (see Fig.\ref{slacksfreeR2.fig}) | $R_3$ (see Fig.\ref{slacksfreeR3.fig}) |
    |---------------+-------------------+----------------------------------------+----------------------------------------+----------------------------------------|
    | Stating value |          4.814387 |                               4.374254 |                               4.814387 |                               4.814387 |
    |---------------+-------------------+----------------------------------------+----------------------------------------+----------------------------------------|
*** Code
    
    States are indexed from 0 to 24 in the reading order. The action and policy matrices for the expert are created using the following piece of code, which describes the same setting as in \cite{ng2000algorithms} : 
    #+begin_src python :tangle TT_5x5_expertPGen.py
from numpy import *
import scipy
from a2str import*
from TT_DP import*

P_north = zeros((25,25))
P_east = zeros((25,25))
P_south = zeros((25,25))
P_west = zeros((25,25))

for a in range(0,4):
    P_a = zeros((25,25))
    for x in range(0,5):
        for y in range(0,5):
            index = x+5*y
            x_north = x
            y_north = 0
            if( y != 0 ):
                y_north = y-1
            index_north = x_north + 5*y_north
                
            x_south = x
            y_south = 4
            if( y != 4 ):
                y_south = y+1
            index_south = x_south + 5*y_south

            y_west = y
            x_west = 0
            if( x != 0 ):
                x_west = x-1
            index_west = x_west + 5*y_west

            y_east = y
            x_east = 4
            if( x != 4 ):
                x_east = x+1
            index_east = x_east + 5*y_east

            main_i = -1
            others = [-1,-1,-1]
            filename = "stderr"
            if( a == 0 ):
                main_i = index_north
                others = [index_south,index_west,index_east]
            elif( a == 1):
                main_i = index_east
                others = [index_south,index_west,index_north]
            elif( a == 2):
                main_i = index_south
                others = [index_north,index_west,index_east]
            elif( a == 3):
                main_i = index_west
                others = [index_south,index_north,index_east]
            
            P_a[index,main_i] +=0.7
            for i in others:
                P_a[index,i] +=0.1
            
            if( a == 0 ):
                P_north = P_a.copy()
            elif( a == 1):
                P_east = P_a.copy()
            elif( a == 2):
                P_south = P_a.copy()
            elif( a == 3):
                P_west = P_a.copy()

    if( a == 0 ):
        filename = "TT_5x5_PENorth.mat"
    elif( a == 1):
        filename = "TT_5x5_PEEast.mat"
    elif( a == 2):
        filename = "TT_5x5_PESouth.mat"
    elif( a == 3):
        filename = "TT_5x5_PEWest.mat"
    f = open( filename, "w" )
    f.write( a2str(P_a) )
    f.close()

R = zeros((25,1))
R[4,0] = 1
P_pi = TT_DP( R, (P_north, P_south, P_west, P_east) )
f = open( "TT_5x5_Ppi.mat", "w" )
f.write( a2str(P_pi) )
f.close()
        #+end_src

   The constraint matrix is then computed from this information :
#+srcname: TT_exp1_make
#+begin_src makefile
TT_exp1:
	python TT_5x5_expertPGen.py 
	python Constraint.py TT_5x5_Ppi.mat TT_5x5_PENorth.mat TT_5x5_PEWest.mat TT_5x5_PESouth.mat TT_5x5_PEEast.mat > TT_5x5_C.mat
#+end_src

   And fed to the TaskTransfer program using the /slacks free/ method :
#+begin_src python :noweb yes :tangle TaskTransfer_SF.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
import itertools
from sets import *
from a2str import *

#import pdb

g_mC = genfromtxt(sys.argv[1])
g_iN = g_mC.shape[1]
g_iM = g_mC.shape[0]

#print "C matrix : %d equations in %d variables"% (g_iM,g_iN)
<<TT_linesCreation_py>>

#print A
BFS = Set()
index = 0
standard_sols=[]
<<TT_slacksfree_py>>
<<TT_linearSystem_py>>

#print "All solutions are : "
toPrint = ""
for R in BFS:
    toPrint+=R
print toPrint
#+end_src
#+srcname: TT_exp1_make
#+begin_src makefile
	python TaskTransfer_SF.py TT_5x5_C.mat > TT_5x5_Rewards.mat
#+end_src

   We get 3 rewards we can plot.

   First, a small utility to make the reward be displayed in a form GNUplot can understand
   #+begin_src python :tangle TT_5x5_mat2gp.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
import itertools
from sets import *
from a2str import *

#import pdb

R = genfromtxt(sys.argv[1])
index = int(sys.argv[2])

R = R[index,:]

for x in range(0,5):
    for y in range(0,5):
        print "%d %d %f"%(x,y,R[x+5*y])
    print ""
   #+end_src
   We use it for the three rewards :
#+srcname: TT_exp1_make
#+begin_src makefile
	python TT_5x5_mat2gp.py TT_5x5_Rewards.mat 0 > TT_5x5_R1.mat
	python TT_5x5_mat2gp.py TT_5x5_Rewards.mat 1 > TT_5x5_R2.mat
	python TT_5x5_mat2gp.py TT_5x5_Rewards.mat 2 > TT_5x5_R3.mat
   #+end_src

   The GNUplot instructions :

   #+begin_src text :tangle TT_5x5_R1.gp
set term postscript enhanced color
set output "TT_5x5_R1.ps"
set view 64,236
set pm3d
splot "TT_5x5_R1.mat" notitle
   #+end_src

   #+begin_src text :tangle TT_5x5_R2.gp
set term postscript enhanced color
set output "TT_5x5_R2.ps"
set view 64,236
set pm3d
splot "TT_5x5_R2.mat" notitle
   #+end_src

   #+begin_src text :tangle TT_5x5_R3.gp
set term postscript enhanced color
set output "TT_5x5_R3.ps"
set view 64,236
set pm3d
splot "TT_5x5_R3.mat" notitle
   #+end_src

   PDF files are produced :
#+srcname: TT_exp1_make
#+begin_src makefile
	gnuplot TT_5x5_R1.gp
	ps2pdf TT_5x5_R1.ps
	gnuplot TT_5x5_R2.gp
	ps2pdf TT_5x5_R2.ps
	gnuplot TT_5x5_R3.gp
	ps2pdf TT_5x5_R3.ps
#+end_src
   
   The figures are now ready.

   The action matrices for the expert are created using the following piece of code.
    #+begin_src python :tangle TT_5x5_agentAsExpert.py
from numpy import *
import scipy
from a2str import*
from TT_DP import*

P_N = zeros((25,25))
P_E = zeros((25,25))
P_S = zeros((25,25))
P_W = zeros((25,25))
P_NE = zeros((25,25))
P_NW = zeros((25,25))
P_SE = zeros((25,25))
P_SW = zeros((25,25))

for a in range(0,8):
    P_a = zeros((25,25))
    for x in range(0,5):
        for y in range(0,5):
            index = x+5*y
            x_north = x
            y_north = 0
            if( y != 0 ):
                y_north = y-1
            index_north = x_north + 5*y_north
                
            x_south = x
            y_south = 4
            if( y != 4 ):
                y_south = y+1
            index_south = x_south + 5*y_south

            y_west = y
            x_west = 0
            if( x != 0 ):
                x_west = x-1
            index_west = x_west + 5*y_west

            y_east = y
            x_east = 4
            if( x != 4 ):
                x_east = x+1
            index_east = x_east + 5*y_east

            x_NE = x
            y_NE = y
            if( y != 0 and x!=4 ):
                y_NE = y-1
                x_NE = x+1
            index_NE = x_NE + 5*y_NE
                
            x_NW = x
            y_NW = y
            if( y != 0 and x!= 0 ):
                y_NW = y-1
                x_NW = x-1
            index_NW = x_NW + 5*y_NW

            y_SW = y
            x_SW = x
            if( x != 0 and y != 4 ):
                x_SW = x-1
                y_SW = y+1
            index_SW = x_SW + 5*y_SW

            y_SE = y
            x_SE = x
            if( x != 4 and y != 4 ):
                x_SE = x+1
                y_SE = y+1
            index_SE = x_SE + 5*y_SE

            main_i = -1
            others = [-1,-1,-1,-1,-1,-1,-1]
            filename = "stderr"
            if( a == 0 ):
                main_i = index_north
                others = [index_south,index_west,index_east,index_SE,index_SW,index_NE,index_NW]
            elif( a == 1):
                main_i = index_east
                others = [index_south,index_west,index_north,index_SE,index_SW,index_NE,index_NW]
            elif( a == 2):
                main_i = index_south
                others = [index_north,index_west,index_east,index_SE,index_SW,index_NE,index_NW]
            elif( a == 3):
                main_i = index_west
                others = [index_south,index_north,index_east,index_SE,index_SW,index_NE,index_NW]
            elif( a == 4 ):
                main_i = index_SE
                others = [index_south,index_west,index_east,index_north,index_SW,index_NE,index_NW]
            elif( a == 5):
                main_i = index_SW
                others = [index_south,index_west,index_north,index_SE,index_east,index_NE,index_NW]
            elif( a == 6):
                main_i = index_NW
                others = [index_north,index_west,index_east,index_SE,index_SW,index_NE,index_south]
            elif( a == 7):
                main_i = index_NE
                others = [index_south,index_north,index_east,index_SE,index_SW,index_east,index_NW]
            
            P_a[index,main_i] +=0.72
            for i in others:
                P_a[index,i] +=0.04
            
            if( a == 0 ):
                P_N = P_a.copy()
            elif( a == 1):
                P_E = P_a.copy()
            elif( a == 2):
                P_S = P_a.copy()
            elif( a == 3):
                P_W = P_a.copy()
            elif( a == 4 ):
                P_SE = P_a.copy()
            elif( a == 5):
                P_SW = P_a.copy()
            elif( a == 6):
                P_NW = P_a.copy()
            elif( a == 7):
                P_NE = P_a.copy()

    if( a == 0 ):
        filename = "TT_5x5_PANorth.mat"
    elif( a == 1):
        filename = "TT_5x5_PAEast.mat"
    elif( a == 2):
        filename = "TT_5x5_PASouth.mat"
    elif( a == 3):
        filename = "TT_5x5_PAWest.mat"
    elif( a == 4 ):
        filename = "TT_5x5_PASE.mat"
    elif( a == 5):
        filename = "TT_5x5_PASW.mat"
    elif( a == 6):
        filename = "TT_5x5_PANW.mat"
    elif( a == 7):
        filename = "TT_5x5_PANE.mat"
    f = open( filename, "w" )
    f.write( a2str(P_a) )
    f.close()

    #+end_src

   Then, the agent as expert value is computed, along with the starting value corresponding to the other three rewards :
    #+begin_src python :tangle TT_5x5_agentAsExpert.py
R = zeros((25,1))
R[4,0] = 1
P_piAaE = TT_DP( R, (P_N, P_S, P_W, P_E, P_NE, P_NW, P_SW, P_SE) )
foundR = genfromtxt( "TT_5x5_Rewards.mat" )
P_pi1 = TT_DP( foundR[0,:], (P_N, P_S, P_W, P_E, P_NE, P_NW, P_SW, P_SE) )
P_pi2 = TT_DP( foundR[1,:], (P_N, P_S, P_W, P_E, P_NE, P_NW, P_SW, P_SE) )
P_pi3 = TT_DP( foundR[2,:], (P_N, P_S, P_W, P_E, P_NE, P_NW, P_SW, P_SE) )
VAaE = dot(linalg.inv(identity(25) - 0.9*P_piAaE),R)
V1 = dot(linalg.inv(identity(25) - 0.9*P_pi1),R)
V2 = dot(linalg.inv(identity(25) - 0.9*P_pi2),R)
V3 = dot(linalg.inv(identity(25) - 0.9*P_pi3),R)
print "Agent as expert : %f"%VAaE[20]
print "R1 : %f"%V1[20]
print "R2 : %f"%V2[20]
print "R3 : %f"%V3[20]
        #+end_src

#+srcname: TT_exp1_make
#+begin_src makefile
	python TT_5x5_agentAsExpert.py
#+end_src
** Open questions
   What easy-to-compute criteria corresponds to a reward that induce high-value behaviors ?
   
   
   Assuming an infinite supply of experts, will the true reward function be retrieved ?
   
   
   Is the /Agent as expert/ value always attainable by maximizing one of the output reward ?
   
   
   Is there such a thing as a lousy expert that hides information to the algorithm ?


   Is there a setting where a reward exists so that no reward output by our algorithm is equivalent to it ?

   
   Is the naive projection of the true reward to the manifold always present in the reward output by our algorithm ?
** Random rewards and random experts
*** Goal
   The goal of this experiment is to empirically test the hypothesis according to which there always is a reward output by the algorithm so that if the agent maximizes this reward its true value is equal to the true value when it maximizes the true reward function. 
   
   This hypothesis can be reformulated by saying that /the Agent as Expert value is always attainable by maximizing one of the output rewards/. 
*** Protocol
    - Do :
      - Define a random reward $\mathbf{R}$ of size $n$
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
      - Compute the expert's policy by solving the MDP : $\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
      - Compute the Agent as Expert policy by solving the MDP for the true reward function : $\pi \leftarrow DP( R, \{P_{a^A_i}\}_i)$
      - Store the Agent as Expert value $V^{AaE}(s_0)$
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
      - Plot the agent's true values along with the expert's and the Agent as Expert value
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
*** Code
    We rewrite the protocol, adding the corresponding code at each line :
    - Do :
      #+srcname: TT_Exp1_py
      #+begin_src python
iterations = 0
while True:
    iterations+=1
      #+end_src
      - Define a random reward $\mathbf{R}$ of size $n$
	#+srcname: TT_Exp1_py
        #+begin_src python
    R = scipy.rand( n )
        #+end_src
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
	#+srcname: TT_Exp1_py
        #+begin_src python
    ExpertsActions = []
    for i in range(0,m_E):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        ExpertsActions.append(P_i)
        #+end_src
      - Compute the expert's policy's transition probabilities by solving the MDP : $P_\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    P_pi = TT_DP( R, ExpertsActions )
        #+end_src
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    ttRewards = TT( P_pi, ExpertsActions )
    if( len( ttRewards.shape) == 1 ): #If there is only one reward
        ttRewards = asarray([ttRewards]) #Cast as a matrix anyway, the code below expects a matrix and not a vector
        #+end_src
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
	#+srcname: TT_Exp1_py
        #+begin_src python
    AgentsActions = []
    for i in range(0,m_A):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        AgentsActions.append(P_i)

        #+end_src
      - Compute the Agent as Expert policy's transition probabilities by solving the MDP for the true reward function : $P_\pi^{AaE} \leftarrow DP( R, \{P_{a^A_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    P_AaE = TT_DP( R, AgentsActions )
        #+end_src
      - Store the Agent as Expert value $V^{AaE}(s_0)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    V_AaE = dot(P_AaE[0],R.transpose()) #0 is the initial state
        #+end_src
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	#+srcname: TT_Exp1_py
        #+begin_src python
    AgentsValues = []
    for reward in ttRewards:
        #+end_src
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	  #+srcname: TT_Exp1_py
          #+begin_src python
        P_pi_l = TT_DP( reward, AgentsActions )
          #+end_src
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
	  #+srcname: TT_Exp1_py
          #+begin_src python
        AgentsValues.append( dot( P_pi_l[0], R.transpose() ))
          #+end_src
      - Plot the agent's true values along with the expert's and the Agent as Expert value
	#+srcname: TT_Exp1_py
        #+begin_src python
    print l2str( AgentsValues )
    print "Expert : %f" % dot(P_pi[0], R.transpose())
    print "Agent as Expert : %f" % V_AaE
        #+end_src
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
      #+srcname: TT_Exp1_py
      #+begin_src python
    if( max( AgentsValues ) < V_AaE or iterations >= 1000 ):
        print "Conditions de quittage"
        print "iterations : %d" % iterations
        print "Récompense : "
        print l2str(R)
        print "Actions de l'expert :"
        for action in ExpertsActions:
            print a2str(action)
            print ""
        print "Actions de l'agent"
        for action in AgentsActions:
            print a2str(action)
            print ""
        break
      #+end_src


    We glue it :
    #+begin_src python :tangle Exp1.py :noweb yes
# -*- coding: utf8 -*-
from numpy import *
import scipy
from TT_DP import *
from a2str import *
from TT import *

n = 4

m_E = 3
m_A = 2

<<TT_Exp1_py>>
    #+end_src
* Code
** Main code
*** Helper code
    We need the matrices to be output so that two equal matrices are output the same way.

  #+begin_src python :tangle a2str.py
#Do not indent
def l2str_fullprecision(l):
	"""Return a string representing line l without los of precision"""
	answer = ""
	for x in l:
		answer+="%e\t"%x
	answer +="\n"
	return answer

def l2str(l):
	"""Return the unique string representing line l"""
	answer = ""
	for x in l:
		if (abs(x)<1e-10): #FIXME : this is not right.
			answer += " 0.00e+00\t"
		elif (x>0):
			answer += " %1.2e\t"%x
		else:
			answer += "%+1.2e\t"%x
	answer +="\n"
	return answer
		
        
def a2str(a):
	"""Return the unique string representing array a"""
	answer = ""
	for l in a:
		answer += l2str( l )
	return answer


  #+end_src

    We output values near 0 as 0. Instead we probably should understand why there are so much near-0 values in the normal output. I think it has to do with the linear system solver.
*** Dynamic programming
    We need a MDP solver. This quick and dity dynamic programming implementation will do the trick :
    #+begin_src python :tangle TT_DP.py
from numpy import *
import scipy
import pdb

g_vReward = []
g_vActions = []
g_vV = []
g_vPi = []
g_fGamma = 0.9

def TT_Q( s, a ):
    return g_vReward[s] + 0.9 * dot( (g_vActions[ a ])[s], g_vV.transpose() )

def TT_DP( Reward, Actions ):
    "Returns the probability matrix corresponding to the optimal policy with respect to the given reward and the given actions. Actions are given in the form of a probability matrix. Probability matrices are so that the $(i,j)$ element gives the probability of transitioning to state $j$ upon taking action $a$ in state $i$"
    #pdb.set_trace()
    global g_vReward
    global g_vActions
    global g_vV
    global g_vPi
    n = len( Reward )
    m = len( Actions )
    g_vReward = Reward
    g_vActions = Actions
    g_vPi = map( int, floor( scipy.rand( n )*m ) )
    g_vV = scipy.rand( n )
    #While things change,
    changed = True
    while changed:
        changed = 0
        #For each state
        for s in range(0,n):
            old_pi = g_vPi[ s ]
            old_V = g_vV[ s ]
            chosen_a = 0
            max_Q = TT_Q( s, chosen_a )
            #Select the action that maximizes Q
            for a in range(0,m):
                Q = TT_Q( s, a )
                if( Q > max_Q ):
                    max_Q = Q
                    chosen_a = a
            g_vPi[ s ] = chosen_a
            g_vV[ s ] = max_Q
            if( g_vPi[ s ] != old_pi or g_vV[ s ] != old_V ):
                changed = 1
    #Construct the pobability matrix from the policy
    Ppi = zeros((n,n))
    for s in range(0,n):
        Ppi[s] = (g_vActions[ g_vPi[s] ])[s]
    return Ppi

    #+end_src
*** Finding the BFS
   This code finds the coordinates of the vertices of the polytope, also known as the /basic feasible solutions/.\\

   We create the $A$ and $b$ matrices of the standard form

#+srcname:TT_linesCreation_py
#+begin_src python
A = zeros((g_iM + 2, 2*g_iN + g_iM))
#C'
for i in range(0,g_iN):
    A[0:g_iM,2*i] = g_mC[:,i]
    A[0:g_iM,2*i+1] = -g_mC[:,i]
#\mathbf{1}'
for i in range(0,g_iN):
    A[g_iM,2*i] = 1
    A[g_iM,2*i+1] = -1
#\mathbf{1}\mathbf{1}
A[g_iM+1,0:2*g_iN] = ones((1,2*g_iN))
#-Id_(m)
A[:g_iM,2*g_iN:] = -identity(g_iM)

b = zeros((g_iM+2,1))
b[g_iM+1] = 1
#print "A and b matrices"
#print a2str(A)
#print a2str(b)
#+end_src

   The previously mentionned heuristics are defined here :
   - the naive enumeration of all the combinations :
     #+srcname: TT_naiveEnumeration_py
     #+begin_src python
for lslice in itertools.combinations(range(0,2*g_iN+g_iM),g_iM+2):
     #+end_src
   - all slack variables and two states (the expert has the best policy) :
     #+srcname: TT_slacksfree_py
     #+begin_src python
for partialLslice in itertools.combinations(range(0,2*g_iN),2):
    lslice = partialLslice + tuple(range(2*g_iN,2*g_iN+g_iM))
     #+end_src
     
   For every $m+2$ combination of columns, we solve the resulting linear system, go back from that solution to the basic feasible solution and store it in a set 
#+srcname:TT_linearSystem_py
#+begin_src python
    if( index % 1000 == 0 ):
        sys.stderr.write("Combinaison No %d\n" % index)
    #print "Combinaison No %d" % index
    index+=1
    #print lslice
    #print "Subsystem"
    #print A[:,lslice]
    if( abs(linalg.det( A[:,lslice] ) ) > 0.00001 ):#Ugly hack for floating point precision
        partialStandardSol = linalg.solve(A[:,lslice],b)
        if( all( partialStandardSol > -0.00000001 ) ): #Ugly hack for floating point precision
            standardSol =  zeros((2*g_iN+g_iM,1))
            standardSol[lslice,:] = partialStandardSol
            #print "Standard solution exists : "
            #print standardSol
            R = zeros((g_iN,1))
            for i in range(0,g_iN):
                R[i] = standardSol[2*i] - standardSol[2*i+1]
            #print "Corresponding Reward : "
            sys.stderr.write("Found a reward on comb %d\n" % index)
            #print linalg.det( A[:,lslice])
            #print R.transpose()
            #if( any(dot( g_mC, R )<-0.00001) ):
                #print "Ne respecte pas les contraintes"
                #print g_mC
                #print R
                #print dot( g_mC, R )
            if( any( abs(R) > 0.001 ) ):
                BFS.add(l2str((R.transpose())[0]))
        #else:
            #print "Negative"
            #print partialStandardSol
    #else:
        #print "No solutions"
        #print linalg.det( A[:,lslice] )
#print "Nb Combinaisons %d" % index

#+end_src

   We glue it together

#+begin_src python :noweb yes :tangle TaskTransfer.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
import itertools
from sets import *
from a2str import *

#import pdb

g_mC = genfromtxt(sys.argv[1])
g_iN = g_mC.shape[1]
g_iM = g_mC.shape[0]

#print "C matrix : %d equations in %d variables"% (g_iM,g_iN)
<<TT_linesCreation_py>>

#print A
BFS = Set()
index = 0
standard_sols=[]
<<TT_naiveEnumeration_py>>
<<TT_linearSystem_py>>

#print "All solutions are : "
toPrint = ""
for R in BFS:
    toPrint+=R
print toPrint
#+end_src

*** Computing the $C$ constraints matrix
    Given a $P_\pi$ matrix and several $P_a$ matrices, this code compute the $C$ constraint matrix consisting of the non null, non repeating lines of $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$, plus the $[1, \dots, 1]$ vector. FIXME : why ?\\

    We add the non null lines of every $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix to a set
#+srcname:TT_PpiMinusPaEtc_py
#+begin_src python
g_sC = Set()
for Pa in g_lActionMatrices:
    A = dot((g_mPpi - Pa),linalg.inv( identity(g_iN)-(g_fGamma*g_mPpi) ))
    for line in A:
        if( any( line != zeros((1,g_iN)) ) ):
               g_sC.add( l2str_fullprecision(line/linalg.norm(line)) )

#+end_src
    
    We glue this.
#+begin_src python :noweb yes :tangle Constraint.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
from sets import *
from a2str import *
  
#import pdb
g_mPpi = genfromtxt( sys.argv[1] )
g_lActionMatrices = []
for i in range(2,len(sys.argv)):
    g_lActionMatrices.append( genfromtxt(sys.argv[i]) )
g_iN = g_mPpi.shape[0]
g_fGamma = 0.9

<<TT_PpiMinusPaEtc_py>>

toPrint = ""
for line in g_sC:
    toPrint += line
print toPrint

#+end_src
    
*** Glue function
    A python function that glue both the constraints computation and the TaskTransfer algorithm is provided :
    #+begin_src python :tangle TT.py
import os
from numpy import *
import scipy

def TT( P_pi, Actions ):
    index = 0
    savetxt( "TT_tmp_%d"%index, P_pi, "%e", "\t" )
    index +=1
    for action in Actions:
        savetxt( "TT_tmp_%d"%index, action, "%e", "\t" )
        index +=1

    cmd = "python2.6 Constraint.py "
    for i in range(0,index):
        cmd+="TT_tmp_%d "%i
    cmd += " > TT_tmpC.mat"
    os.system( cmd )

    cmd = "python2.6 TaskTransfer.py TT_tmpC.mat > TT_tmpR.mat"
    os.system( cmd )

    answer = genfromtxt( "TT_tmpR.mat" )
    return answer
    #+end_src
** Tests
*** Test in 3D
    We test the program in a small setting so that the reward vector only has three component $x$, $y$ and $z$.

    
     First let us define the following constraints matrices :
     - This one means that we must have $x\geq y$ and $y \geq z$ 
       #+begin_src text :tangle test/TT_CT01.mat
1	-1	0
0	1	-1
       #+end_src
     - this one is the same, with an added last line that explicitely specify that $x \geq z$. The last line does not change the meaning of the contraints, but we add it to see if the program works even when fed with a useless constraint
       #+begin_src text :tangle test/TT_CT02.mat
1	-1	0
0	1	-1
1	0	-1
       #+end_src

       
     There are only three kind solutions satisfying these constraints (apart from the degenerative solution $x=y=z$) :
     - $x>y>z$
     - $x>y=z$
     - $x=y>z$
     
       
     As the $L_1$ norm of the anwers must be $1$, the expected output for both input is :
     #+begin_src text :tangle test/TT_expectedOutT0.mat
 2.50e-01	 2.50e-01	-5.00e-01	
 5.00e-01	 0.00e+00	-5.00e-01	
 5.00e-01	-2.50e-01	-2.50e-01	
     #+end_src

We now build Makefile targets that calls the program on the previously defined $C$ matrices and match the output with the expected output. Note the use of the \texttt{sort} command to make sure both output are in the same order and the diff command succeeds.

#+srcname: TT_test0_make
#+begin_src makefile
TT_test0: TaskTransfer.py
	python TaskTransfer.py test/TT_CT01.mat | sort > test/TT_outT01.mat
	python TaskTransfer.py test/TT_CT02.mat | sort > test/TT_outT02.mat
	../Utils/matrix_diff.py test/TT_expectedOutT0.mat test/TT_outT01.mat
	../Utils/matrix_diff.py test/TT_expectedOutT0.mat test/TT_outT02.mat
#+end_src

#+srcname: TT_cleanTest0_make
#+begin_src makefile
TT_cleanTest0:
	rm test/TT_outT01.mat
	rm test/TT_outT02.mat
#+end_src
*** Task Transfer on a 2x2 Gridworld
    In this simple setting we imagine a 2x2 gridworld and two experts. Both experts optimize the same reward, located in the north east corner. Both experts can choose between the same actions at each step : the four compass directions. The first expert's policy is NORTH, EAST, the second one is EAST, NORTH. We want to see in this experiment if the true reward is among the set of reward output by our algorithm.

    The states are indexed fom 0 to 3, in the reading order.


    We begin by defining the two matrices $P_{\pi_1}$ and $P_{\pi_2}$ relative to both expert's policies :
    - $P_{\pi_1}$ is :
      #+begin_src text :tangle test/TT_PPi1.mat
0	1	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src
    - $P_{\pi_1}$ is :    
      #+begin_src text :tangle test/TT_PPi2.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	1	0	0
      #+end_src
      
    
    We then define the four $P_a$ matrices relative to each action :
    - $P_{NORTH}$ is :
      #+begin_src text :tangle test/TT_PNorth.mat
1	0	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src      
    - $P_{EAST}$ is :
      #+begin_src text :tangle test/TT_PEast.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	0	0	1
      #+end_src
     - $P_{SOUTH}$ is :
       #+begin_src text :tangle test/TT_PSouth.mat
 0	0	1	0
 0	0	0	1
 0	0	1	0
 0	0	0	1
       #+end_src
     - $P_{WEST}$ is :
       #+begin_src text :tangle test/TT_PWest.mat
 1	0	0	0
 1	0	0	0
 0	0	1	0
 0	0	1	0
       #+end_src
       

    The constraint matrices relative each expert are computed :
    #+srcname: TT_test1_make
    #+begin_src makefile
TT_test1:
	python Constraint.py test/TT_PPi1.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C1.mat
	python Constraint.py test/TT_PPi2.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C2.mat
    #+end_src

    The conjoint constraint matrix is extracted (duplicate lines are removed) :
    #+srcname: TT_test1_make
    #+begin_src makefile
	cat test/TT_C1.mat test/TT_C2.mat | sort | uniq > test/TT_CBoth.mat
    #+end_src

    The TaskTransfer program is run and its output is compared against what is expected :
    #+srcname: TT_test1_make
    #+begin_src makefile
	python TaskTransfer.py test/TT_CBoth.mat | sort > test/TT_outT1.mat
	../Utils/matrix_diff.py test/TT_expectedOutT1.mat test/TT_outT1.mat
    #+end_src

    #+begin_src text :tangle test/TT_expectedOutT1.mat
0.00e+00	 5.00e-01	-5.00e-01	 0.00e+00	
1.67e-01	 1.67e-01	-5.00e-01	 1.67e-01	
-2.50e-01	 2.76e-01	 2.24e-01	-2.50e-01	
-2.50e-01	 5.00e-01	 0.00e+00	-2.50e-01
    #+end_src

    The expected output is drawn from a run of an early version of the program, it looked consistant and logical.

    #+srcname: TT_cleanTest1_make
    #+begin_src makefile
TT_cleanTest1:
	rm test/TT_C1.mat
	rm test/TT_C2.mat
	rm test/TT_CBoth.mat
	rm test/TT_PPi1.mat
	rm test/TT_PPi2.mat
	rm test/TT_PNorth.mat
	rm test/TT_PSouth.mat
	rm test/TT_PWest.mat
	rm test/TT_PEast.mat
	rm test/TT_outT1.mat
    #+end_src
*** Dynamic pogamming on a 2x2 Gridworld
    We want to test our dynamic programming functions. We use help from the files defined in the [[TaskTransfer on a 2x2 Gridworld]] test.

    We first run the optimization with the action in a certain order,
    #+begin_src python :tangle TT_test2.py
from numpy import *
import scipy
from TT_DP import *
from a2str import *

Actions = []

for file in ['test/TT_PNorth.mat','test/TT_PEast.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( TT_DP( Reward, Actions ) )
    #+end_src
    And then in another order,
    #+begin_src python :tangle TT_test3.py
from numpy import *
import scipy
from TT_DP import *
from a2str import *

Actions = []

for file in ['test/TT_PEast.mat','test/TT_PNorth.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( TT_DP( Reward, Actions ) )
    #+end_src
    This should retrieve the policies of each of our experts, as changing the order of the actions change the default action when state-action values are the same.

    We add the first test to the Makefile
    #+srcname: TT_test2_make
    #+begin_src makefile
TT_test2:
	python TT_test2.py > test/TT_outT2.mat
	../Utils/matrix_diff.py test/TT_PPi1.mat test/TT_outT2.mat
    #+end_src
    
    And the second also,
    #+srcname: TT_test3_make
    #+begin_src makefile
TT_test3:
	python TT_test3.py > test/TT_outT3.mat
	../Utils/matrix_diff.py test/TT_PPi2.mat test/TT_outT3.mat
    #+end_src
* Perspective
  It should possible to reduce the computational complexity with easy tricks.\\

  I want to study how this generalizes to continuous MDPs and MDPs where the probabilites are unknown.\\

  Every other algorithm's heuristics should be expressable as a formal criterion on the systems to be solved. This couls lead to a generalization of the state of the art in IRL.\\

  Reducing the dimensionality of the search space should speed up every existing algorithm.



   \bibliographystyle{plain}
   \bibliography{../Biblio/Biblio.bib}
