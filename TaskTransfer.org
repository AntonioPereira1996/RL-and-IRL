#+TITLE: Task Transfer on the GridWorld
#+LATEX_HEADER: \usepackage{blkarray}
#+LATEX_HEADER: \usepackage{amsmath}

* Context
  
  /Agent as expert/ value : This is the value function of the agent evaluated at the starting state, given that the agent follows an optimal policy with respect to the /true reward function/. No behavior can induce a strictly greater true value than this one.

* Mapping behaviors and rewards
** Reward space

   A reward can be scaled without changing its corresponding optimal policies. 
   Therefore, the space of all rewards $R$ can be reduced to a hypersurface englobing the origin. For example, one can use the unit sphere or the unit cube.
   \\

   Adding the same value to all the component of a reward does not change the set of corresponding optimal policies. This is a translation of vector $\lambda [1,1,1,\dots,1]$.
   Therefore, the forementioned hypersurface can be reduced by half, cutting in the middle with the hyperplane perpendicular to the vector $[1,1,\dots,1]$.   
   \\

   The null reward is not interesting as it admits every policy as an optimal one.
   \\
   
   Having exhausted all the obvious reward shaping methods, we begin the analysis with a reward space reduced to an open and finite hypersurface that can be ununiformly scaled to the unit hemisphere whose wenter pilla is the vector $[1,1,\dots,1]$, with the $\lambda [1,1,\dots,1]$ point removed.
   \\
   
   One such hypersurface is for example the "top" of the simplex defined by le $\lambda$ point $[1,1,\dots,1]$ and the basepoints $[-(n-1),1,\dots,1]$, $[1,-(n-1),\dots,1]$, $\dots$, $[1,1,\dots,-(n-1)]$, where $n$ is the dimension of the reward space, i.e. the cardinal of $S$ (we estrict ourselves to the case where the reward is given only with respect to the state). Some distinguishing features of this particuliar example are :
   - it can be expressed as a set of linear constraints over $R$, namely $e_iR\leq 1$ with $e_i$ being the vector whose coordinates are null except for the $i$-th which is 1
   - every edge that contain the $\lambda$ point can be scanned by changing only one coordinate
   - every other edge can be scanned by changing only two coordinates
   
** Reward mapping

   In \cite{ng2000algorithms}, we are given a superb necessary and sufficient condition for a reward to be /compatible/ with a given policy, that is to say, for this given policy to be optimal with respect to the reward.
   \\

   For every action $a$ the agent had the possibility to take, the following matrix inequality must be met : 
   \begin{equation}
   \label{ng2000algorithms.eqn}
   (P_\pi-P_{a})(I-\gamma P_\pi)^{-1}R\succeq 0
   \end{equation}

   Where $P_\pi$ or $P_a$ is the $N\times N$ matrix such as its $(i,j)$ element gives the probability of transitioning to state $j$ upon taking action $\pi(i)$, resp $a$ in state $i$.

   This is a Linear Programming problem. By adding to these constraints the linear constraints that define the forementionned starting hypersurface, we obtain a set of linear constraints that defines a closed, convex polytope, two remarkable vertices of which are the $\lambda$ point and the origin. The set of all facets that does not contain the origin describes the set of all compatible rewards (that is to say, every compatible reward is a scaling and a $\lambda Id$ translation away of a unique point of one of those facets).

** Reward selecting

   The set of all compatible rewards is defined. However, exploring it and searching in it is tricky. 
   \\
   
   I have the intuition that vertices of this polytopic set are interesting rewards. 
   \\
  
   To find the coordinates of the vertices, a simple solution is to use the tools of the Linear Programming framework (http://www2.isye.gatech.edu/~spyros/LP/LP.html).

   - Let $C$ be the matrix created by selecting the $m$ non null lines of the constraint matrix $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$.
   - Let $e_i$ be the $i$-th vector of the orthonormal base of the reward space, i.e. $e_i = [0,\dots,0,1,0,\dots,0]$, with the 1 at the $i$-th position.
   - Let $X$ be the unknown vector

     
   All the forementionned constraints can now be expressed as follow :
   \begin{eqnarray}
   &CX \succeq 0\\
    \forall i, &e_iX \preceq 1\\
   &X \in \mathbb{R}^n
   \end{eqnarray}

*** Standard form

   This is however not what is expected by the tools of the Linear Programmnig framework. The following operations aim at transforming this natural expression of the problem into the /standard form/ $AX=b, X\succeq 0$, typically used in the simplex algorithm (which is not fully applicable here because we don't have a cost function).
   \\

   First, we add a $-$ sign in front of the $C$ matrix so that both inequalities are the same way. The constraint can now be expressed as :
   \begin{eqnarray}
   &-CX \preceq 0\\
   \forall i, &e_iX \preceq 1\\
   &X \in \mathbb{R}^n
   \end{eqnarray}
   or more succintly as
   \begin{eqnarray}
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   -C \\
   \cline{1-1}
   \begin{block*}{c}
   e_1 \\
   \vdots \\
   e_n \\
   \end{block*}
   \end{block*}
   \end{blockarray} 
   X \preceq 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   0 \\
   \cline{1-1}
   \begin{block*}{c}
   1 \\
   \vdots \\
   1 \\
   \end{block*}
   \end{block*}
   \end{blockarray}\\
   X\in \mathbb{R}^n
   \end{eqnarray}
   
   The $X\in \mathbb{R}^n$ part is not satisfactory because in the standard form all unknowns must be grater than or equal to 0. To get to this form, for every component $x_i$ of $X$, we define $x_i^+\geq0$ and $x_i^-\geq0$ so that $x_i = x_i^+ - x_i^-$. We now define the $X'$ matrix as 
   \begin{equation}
   X'=\begin{pmatrix} x_1^+\\x_1^-\\ \dots \\ x_n^+\\x_n^- \end{pmatrix}
   \end{equation}
   Accordingly, the $C'$ matrix is defined as (using $c_i$ to denote the $i$-th column of C) :
   \begin{equation}
   C'=\begin{pmatrix} c_1 | -c_1 | c_2 | -c_2 | \dots |c_n|-c_n \end{pmatrix}
   \end{equation}
   and the $e'_i$ matrices are defined as :
   \begin{equation}
   e'_i=\begin{pmatrix} 0,0,\dots 1,-1, \dots 0,0 \end{pmatrix}
   \end{equation}
   with $1$ and $-1$ being respectively at indexes $2i-1$ and $2i$.
   \\
   
   We are now a bit closer to the standard form :
   \begin{eqnarray}
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   -C' \\
   \cline{1-1}
   \begin{block*}{c}
   e'_1 \\
   e'_2\\
   \vdots \\
   e'_n \\
   \end{block*}
   \end{block*}
   \end{blockarray} 
   X' \preceq 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   0 \\
   \cline{1-1}
   \begin{block*}{c}
   1 \\
   \vdots \\
   1 \\
   \end{block*}
   \end{block*}
   \end{blockarray}\\
   X'\succeq 0
   \end{eqnarray}
   
   The last thing to do is to introduce $m+n$ positive slack variables in the form of a matrix
   \begin{equation}
   S = \begin{pmatrix}s_1\\ \vdots\\ s_{m+n}\end{pmatrix} \succeq 0
   \end{equation}
   This allows us to change the inequality into an equality : if $a\leq b$, then $\exists s \geq 0, a+s = b$. We finally obtain something in the standard form :
   \begin{eqnarray}
   \begin{blockarray}{(cc)}
   \begin{block*}{c|c}
   -C'&  \\
   \cline{1-1}
   \begin{block*}{c|c}
   e'_1 & \\
   e'_2 & Id_{m+n}\\
   \vdots \\
   e'_n &\\
   \end{block*}
   \end{block*}
   \end{blockarray} 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   X' \\
   \cline{1-1}
   \begin{block*}{c}
   S\\
   \end{block*}
   \end{block*}
   \end{blockarray}
   &= 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   0 \\
   \cline{1-1}
   \begin{block*}{c}
   1 \\
   \vdots \\
   1 \\
   \end{block*}
   \end{block*}
   \end{blockarray}\\
   X'&\succeq 0\\
   S &\succeq 0
   \end{eqnarray}

*** Basic feasible solutions
    In the linear programming framework, vertices of the polytope are often referred to as /basic feasible solutions/. To get these basic feasible solutions, one must cross out a certain number of columns of the $A$ matrix and solve the system for the remaining variables. If the system admits a unique solution, the components of which are all non negative, then such a solution is a basic feasible solution (see for example [[http://www2.isye.gatech.edu/~spyros/LP/node20.html]] to get an explanation).
\\

The number of columns not to take into account is the number of variables minus the number of equations. So the number of columns to take into account is in fact the number of equations. In our case, this is $m+n$.
*** Back to the reward space
    Given a basic feasible solution (/bfs/) of the linear problem $\begin{blockarray}{(c)}\begin{block*}{c}X'^{bfs} \\\cline{1-1}\begin{block*}{c} S^{bfs}\\\end{block*}\end{block*}\end{blockarray}$, we must go back to the corresponding reward $R^{bfs}$ by undoing the transformation that allowed us to get to the standard form. This is simply done by :
    \begin{equation}
    R^{bfs}_i = X'^{bfs}_{2i-1} - X'^{bfs}_{2i}
    \end{equation}


    \bibliographystyle{plain}
    \bibliography{../Biblio/Biblio.bib}
* Code
** Main code
*** Helper code
    We need the matrices to be output so that two equal matrices are output the same way.

  #+begin_src python :tangle a2str.py
#Do not indent
def l2str(l):
	"""Return the unique string representing line l"""
	answer = ""
	for x in l:
		if (abs(x)<1e-10): #FIXME : this is not right.
			answer += " 0.00e+00\t"
		elif (x>0):
			answer += " %1.2e\t"%x
		else:
			answer += "%+1.2e\t"%x
	answer +="\n"
	return answer
		
        
def a2str(a):
	"""Return the unique string representing array a"""
	answer = ""
	for l in a:
		answer += l2str( l )
	return answer


  #+end_src

    We output values near 0 as 0. Instead we probably should understand why there are so much near-0 values in the normal output. I think it has to do with the linear system solver.
*** Dynamic programming
    We need a MDP solver. This quick and dity dynamic programming implementation will do the trick :
    #+begin_src python :tangle TT_DP.py
from numpy import *
import scipy
import pdb

g_vReward = []
g_vActions = []
g_vV = []
g_vPi = []
g_fGamma = 0.9

def TT_Q( s, a ):
    return g_vReward[s] + 0.9 * dot( (g_vActions[ a ])[s], g_vV.transpose() )

def TT_DP( Reward, Actions ):
    "Returns the probability matrix corresponding to the optimal policy with respect to the given reward and the given actions. Actions are given in the form of a probability matrix. Probability matrices are so that the $(i,j)$ element gives the probability of transitioning to state $j$ upon taking action $a$ in state $i$"
    #pdb.set_trace()
    global g_vReward
    global g_vActions
    global g_vV
    global g_vPi
    n = len( Reward )
    m = len( Actions )
    g_vReward = Reward
    g_vActions = Actions
    g_vPi = map( int, floor( scipy.rand( n )*m ) )
    g_vV = scipy.rand( n )
    #While things change,
    changed = True
    while changed:
        changed = 0
        #For each state
        for s in range(0,n):
            old_pi = g_vPi[ s ]
            old_V = g_vV[ s ]
            chosen_a = 0
            max_Q = TT_Q( s, chosen_a )
            #Select the action that maximizes Q
            for a in range(0,m):
                Q = TT_Q( s, a )
                if( Q > max_Q ):
                    max_Q = Q
                    chosen_a = a
            g_vPi[ s ] = chosen_a
            g_vV[ s ] = max_Q
            if( g_vPi[ s ] != old_pi or g_vV[ s ] != old_V ):
                changed = 1
    #Construct the pobability matrix from the policy
    Ppi = zeros((n,n))
    for s in range(0,n):
        Ppi[s] = (g_vActions[ g_vPi[s] ])[s]
    return Ppi

    #+end_src
*** Finding the BFS
   This code finds the coordinates of the vertices of the polytope, also known as the /basic feasible solutions/.\\

   We create the $A$ and $b$ matrices of the standard form

#+srcname:TT_linesCreation_py
#+begin_src python
A = zeros((g_iM + g_iN, 3*g_iN + g_iM))
#-C'
for i in range(0,g_iN):
    A[0:g_iM,2*i] = -g_mC[:,i]
    A[0:g_iM,2*i+1] = g_mC[:,i]
#e'-i
for i in range(0,g_iN):
    A[g_iM+i,2*i] = 1
    A[g_iM+i,2*i+1] = -1
#Id_(n+m)
A[:,2*g_iN:] = identity(g_iN+g_iM)

b = zeros((g_iM+g_iN,1))
b[g_iM:,:] = ones((g_iN,1))
#print "A and b matrices"
#print a2str(A)
#print a2str(b)

#+end_src

   For every $n+m$ combination of columns, we solve the resulting linear system, go back from that solution to the basic feasible solution and store it in a set 
#+srcname:TT_linearSystem_py
#+begin_src python
BFS = Set()
index = 0
standard_sols=[]
for lslice in itertools.combinations(range(0,3*g_iN+g_iM),g_iN+g_iM):
    #print "Combinaison No %d" % index
    index+=1
    #print lslice
    #print "Subsystem"
    #print A[:,lslice]
    if( abs(linalg.det( A[:,lslice] ) ) > 0.00001 ):#Ugly hack for floating point precision
        partialStandardSol = linalg.solve(A[:,lslice],b)
        if( all( partialStandardSol > -0.00000001 ) ): #Ugly hack for floating point precision
            standardSol =  zeros((3*g_iN+g_iM,1))
            standardSol[lslice,:] = partialStandardSol
        #print "Standard solution exists : "
        #print standardSol
            R = zeros((g_iN,1))
            for i in range(0,g_iN):
                R[i] = standardSol[2*i] - standardSol[2*i+1]
        #print "Corresponding Reward : "
                #print linalg.det( A[:,lslice])
                #print R.transpose()
                #if( any(dot( g_mC, R )<-0.00001) ):
                    #print "Ne respecte pas les contraintes"
                    #print g_mC
                    #print R
                    #print dot( g_mC, R )
            BFS.add(l2str((R.transpose())[0]))
        #else:
            #print "Negative"
            #print partialStandardSol
    #else:
        #print "No solutions"
        #print linalg.det( A[:,lslice] )

#+end_src

   We glue it together

#+begin_src python :noweb yes :tangle TaskTransfer.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
import itertools
from sets import *
from a2str import *

#import pdb

g_mC = genfromtxt(sys.argv[1])
g_iN = g_mC.shape[1]
g_iM = g_mC.shape[0]

#print "C matrix : %d equations in %d variables"% (g_iM,g_iN)

<<TT_linesCreation_py>>

#print A
<<TT_linearSystem_py>>

#print "All solutions are : "
toPrint = ""
for R in BFS:
    toPrint+=R
print toPrint
#+end_src

*** Computing the $C$ constraints matrix
    Given a $P_\pi$ matrix and several $P_a$ matrices, this code compute the $C$ constraint matrix consisting of the non null, non repeating lines of $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$, plus the $[1, \dots, 1]$ vector. FIXME : why ?\\

    We add the non null lines of every $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix to a set
#+srcname:TT_PpiMinusPaEtc_py
#+begin_src python
g_sC = Set()
for Pa in g_lActionMatrices:
    A = dot((g_mPpi - Pa),linalg.inv( identity(g_iN)-(g_fGamma*g_mPpi) ))
    for line in A:
        if( any( line != zeros((1,g_iN)) ) ):
               g_sC.add( l2str( line ))

#+end_src
    
    We glue this.
#+begin_src python :noweb yes :tangle Constraint.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
from sets import *
from a2str import *
  
#import pdb
g_mPpi = genfromtxt( sys.argv[1] )
g_lActionMatrices = []
for i in range(2,len(sys.argv)):
    g_lActionMatrices.append( genfromtxt(sys.argv[i]) )
g_iN = g_mPpi.shape[0]
g_fGamma = 0.9

<<TT_PpiMinusPaEtc_py>>

toPrint = ""
for line in g_sC:
    toPrint+=line
toPrint+= l2str( ones( g_iN ) )
print toPrint
#+end_src
    
*** Glue function
    A python function that glue both the constraints computation and the TaskTransfer algorithm is provided :
    #+begin_src python :tangle TT.py
import os
from numpy import *
import scipy

def TT( P_pi, Actions ):
    index = 0
    savetxt( "TT_tmp_%d"%index, P_pi, "%e", "\t" )
    index +=1
    for action in Actions:
        savetxt( "TT_tmp_%d"%index, action, "%e", "\t" )
        index +=1

    cmd = "python Constraint.py "
    for i in range(0,index):
        cmd+="TT_tmp_%d "%i
    cmd += " > TT_tmpC.mat"
    os.system( cmd )

    cmd = "python TaskTransfer.py TT_tmpC.mat > TT_tmpR.mat"
    os.system( cmd )

    answer = genfromtxt( "TT_tmpR.mat" )
    return answer
    #+end_src
** Tests
*** Unit cube test
**** Principle
    Recall that the constraints that corresponds to the reduction of the reward space are of the form $\forall i, e_iX \preceq 1$. This is half the definition of the unit cube, the other half being the same equation with a minus sign in one of the sides.\\ 

    Also recall that the constraint matrix $C$ defines one hyperplane per line, each containing the origin.\\

    The idea behind this test is to feed the program a $C$ constraints matrix so that, when coupled with the constraints defining the reduced reward space, the polytope we get is a simply defined polytope. It must be easy to check wether the output is good or not.\\

    The hyperplanes parallel to those defined by $\forall i,e_iX \preceq 1$ but englobing the origin (i.e. those defined by $\forall i, e_iX \succeq 0$) are good candidates. They define a region that is $1\over 2^{n}$ of the unit cube.\\
    
    For example in dimension 2, they define the intersection of the unit square and the positive-positive quandrant. In dimension 3 they define the eighth of the unit cube where all the components are positive.\\

    The coordinates of the vertices of these regions are easy to compute. We are going to match the output of the program fed with the $C$ matrix : $\forall i, e_iX \succeq 0$ with the manually computed vertices.
**** Implementation
     
     We are going to test in dimensions 2, 3 and 4.

     First let us define the constraints matrices :
     - in dimension 2 : 
       #+begin_src text :tangle test/TT_CD2.mat
1	0
0	1
       #+end_src
     - in dimension 3 : 
       #+begin_src text :tangle test/TT_CD3.mat
1	0	0
0	1	0
0	0	1
       #+end_src
     - in dimension 4 : 
       #+begin_src text :tangle test/TT_CD4.mat
1	0	0	0
0	1	0	0
0	0	1	0
0	0	0	1
       #+end_src

Then, let us write in some files the corresponding expected output, one vertex per line :
     - in dimension 2 : 
       #+begin_src text :tangle test/TT_expectedOutD2.mat
0	0
0	1
1	0
1	1
       #+end_src
     - in dimension 3 : 
       #+begin_src text :tangle test/TT_expectedOutD3.mat
0	0	0
0	0	1
0	1	0
0	1	1
1	0	0
1	0	1
1	1	0
1	1	1
       #+end_src
     - in dimension 4 : 
       #+begin_src text :tangle test/TT_expectedOutD4.mat
0	0	0	0
0	0	0	1
0	0	1	0
0	0	1	1
0	1	0	0
0	1	0	1
0	1	1	0
0	1	1	1
1	0	0	0
1	0	0	1
1	0	1	0
1	0	1	1
1	1	0	0
1	1	0	1
1	1	1	0
1	1	1	1
       #+end_src


We now build Makefile targets that calls the program on the previously defined $C$ matrices and match the output with the expected output. Note the use of the \texttt{sort} command to make sure both output are in the same order and the diff command succeeds.

#+srcname: TT_test0_make
#+begin_src makefile
TT_test0: TaskTransfer.py
	python TaskTransfer.py test/TT_CD2.mat | sort > test/TT_outD2.mat
	python TaskTransfer.py test/TT_CD3.mat | sort > test/TT_outD3.mat
	python TaskTransfer.py test/TT_CD4.mat | sort > test/TT_outD4.mat
	../Utils/matrix_diff.py test/TT_expectedOutD2.mat test/TT_outD2.mat
	../Utils/matrix_diff.py test/TT_expectedOutD3.mat test/TT_outD3.mat
	../Utils/matrix_diff.py test/TT_expectedOutD4.mat test/TT_outD4.mat
#+end_src

#+srcname: TT_cleanTest0_make
#+begin_src makefile
TT_cleanTest0:
	rm test/TT_outD2.mat
	rm test/TT_outD3.mat
	rm test/TT_outD4.mat
#+end_src
*** Task Transfer on a 2x2 Gridworld
    In this simple setting we imagine a 2x2 gridworld and two experts. Both experts optimize the same reward, located in the north east corner. Both experts can choose between the same actions at each step : the four compass directions. The first expert's policy is NORTH, EAST, the second one is EAST, NORTH. We want to see in this experiment if the true reward is among the set of reward output by our algorithm.

    The states are indexed fom 0 to 3, in the reading order.


    We begin by defining the two matrices $P_{\pi_1}$ and $P_{\pi_2}$ relative to both expert's policies :
    - $P_{\pi_1}$ is :
      #+begin_src text :tangle test/TT_PPi1.mat
0	1	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src
    - $P_{\pi_1}$ is :    
      #+begin_src text :tangle test/TT_PPi2.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	1	0	0
      #+end_src
      
    
    We then define the four $P_a$ matrices relative to each action :
    - $P_{NORTH}$ is :
      #+begin_src text :tangle test/TT_PNorth.mat
1	0	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src      
    - $P_{EAST}$ is :
      #+begin_src text :tangle test/TT_PEast.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	0	0	1
      #+end_src
     - $P_{SOUTH}$ is :
      #+begin_src text :tangle test/TT_PSouth.mat
0	0	1	0
0	0	0	1
0	0	1	0
0	0	0	1
      #+end_src
     - $P_{WEST}$ is :
      #+begin_src text :tangle test/TT_PWest.mat
1	0	0	0
1	0	0	0
0	0	1	0
0	0	1	0
      #+end_src
       

    The constraint matrices relative each expert are computed :
    #+srcname: TT_test1_make
    #+begin_src makefile
TT_test1:
	python Constraint.py test/TT_PPi1.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C1.mat
	python Constraint.py test/TT_PPi2.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C2.mat
    #+end_src

    The conjoint constraint matrix is extracted (duplicate lines are removed) :
    #+srcname: TT_test1_make
    #+begin_src makefile
	cat test/TT_C1.mat test/TT_C2.mat | sort | uniq > test/TT_CBoth.mat
    #+end_src

    The TaskTransfer program is run and its output is compared against what is expected :
    #+srcname: TT_test1_make
    #+begin_src makefile
	python TaskTransfer.py test/TT_CBoth.mat > test/TT_outT1.mat
	../Utils/matrix_diff.py test/TT_expectedOutT1.mat test/TT_outT1.mat
    #+end_src

    The expected output is drawn from a run of an early version of the program, it looked consistant and logical.

    #+srcname: TT_cleanTest1_make
    #+begin_src makefile
TT_cleanTest1:
	rm test/TT_C1.mat
	rm test/TT_C2.mat
	rm test/TT_CBoth.mat
	rm test/TT_PPi1.mat
	rm test/TT_PPi2.mat
	rm test/TT_PNorth.mat
	rm test/TT_PSouth.mat
	rm test/TT_PWest.mat
	rm test/TT_PEast.mat
	rm test/TT_outT1.mat
    #+end_src
*** Dynamic pogamming on a 2x2 Gridworld
    We want to test our dynamic programming functions. We use help from the files defined in the [[TaskTransfer on a 2x2 Gridworld]] test.

    We first run the optimization with the action in a certain order,
    #+begin_src python :tangle TT_test2.py
from numpy import *
import scipy
from TT_DP import *
from a2str import *

Actions = []

for file in ['test/TT_PNorth.mat','test/TT_PEast.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( TT_DP( Reward, Actions ) )
    #+end_src
    And then in another order,
    #+begin_src python :tangle TT_test3.py
from numpy import *
import scipy
from TT_DP import *
from a2str import *

Actions = []

for file in ['test/TT_PEast.mat','test/TT_PNorth.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( TT_DP( Reward, Actions ) )
    #+end_src
    This should retrieve the policies of each of our experts, as changing the order of the actions change the default action when state-action values are the same.

    We add the first test to the Makefile
    #+srcname: TT_test2_make
    #+begin_src makefile
TT_test2:
	python TT_test2.py > test/TT_outT2.mat
	../Utils/matrix_diff.py test/TT_PPi1.mat test/TT_outT2.mat
    #+end_src
    
    And the second also,
    #+srcname: TT_test3_make
    #+begin_src makefile
TT_test3:
	python TT_test3.py > test/TT_outT3.mat
	../Utils/matrix_diff.py test/TT_PPi2.mat test/TT_outT3.mat
    #+end_src
        
* Experiments
** Open questions
   What easy-to-compute criteria corresponds to a reward that induce high-value behaviors ?
   
   
   Assuming an infinite supply of experts, will the true reward function be retrieved ?
   
   
   Is the /Agent as expert/ value always attainable by maximizing one of the output reward ?
   
   
   Is there such a thing as a lousy expert that hides information to the algorithm ?
** Random rewards and random experts
*** Goal
   The goal of this experiment is to empirically test the hypothesis according to which there always is a reward output by the algorithm so that if the agent maximizes this reward its true value is equal to the true value when it maximizes the true reward function. 
   
   This hypothesis can be reformulated by saying that /the Agent as Expert value is always attainable by maximizing one of the output rewards/. 
*** Protocol
    - Do :
      - Define a random reward $\mathbf{R}$ of size $n$
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
      - Compute the expert's policy by solving the MDP : $\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
      - Compute the Agent as Expert policy by solving the MDP for the true reward function : $\pi \leftarrow DP( R, \{P_{a^A_i}\}_i)$
      - Store the Agent as Expert value $V^{AaE}(s_0)$
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
      - Plot the agent's true values along with the expert's and the Agent as Expert value
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
*** Code
    We rewrite the protocol, adding the corresponding code at each line :
    - Do :
      #+srcname: TT_Exp1_py
      #+begin_src python
iterations = 0
while True:
    iterations+=1
      #+end_src
      - Define a random reward $\mathbf{R}$ of size $n$
	#+srcname: TT_Exp1_py
        #+begin_src python
    R = scipy.rand( n )
        #+end_src
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
	#+srcname: TT_Exp1_py
        #+begin_src python
    ExpertsActions = []
    for i in range(0,m_E):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        ExpertsActions.append(P_i)
        #+end_src
      - Compute the expert's policy's transition probabilities by solving the MDP : $P_\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    P_pi = TT_DP( R, ExpertsActions )
        #+end_src
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    ttRewards = TT( P_pi, ExpertsActions )
        #+end_src
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
	#+srcname: TT_Exp1_py
        #+begin_src python
    AgentsActions = []
    for i in range(0,m_A):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        AgentsActions.append(P_i)

        #+end_src
      - Compute the Agent as Expert policy's transition probabilities by solving the MDP for the true reward function : $P_\pi^{AaE} \leftarrow DP( R, \{P_{a^A_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    P_AaE = TT_DP( R, AgentsActions )
        #+end_src
      - Store the Agent as Expert value $V^{AaE}(s_0)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    V_AaE = dot(P_AaE[0],R.transpose()) #0 is the initial state
        #+end_src
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	#+srcname: TT_Exp1_py
        #+begin_src python
    AgentsValues = []
    for reward in ttRewards:
        #+end_src
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	  #+srcname: TT_Exp1_py
          #+begin_src python
        P_pi_l = TT_DP( reward, AgentsActions )
          #+end_src
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
	  #+srcname: TT_Exp1_py
          #+begin_src python
        AgentsValues.append( dot( P_pi_l[0], R.transpose() ))
          #+end_src
      - Plot the agent's true values along with the expert's and the Agent as Expert value
	#+srcname: TT_Exp1_py
        #+begin_src python
    print l2str( AgentsValues )
    print "Expert : %f" % dot(P_pi[0], R.transpose())
    print "Agent as Expert : %f" % V_AaE
        #+end_src
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
      #+srcname: TT_Exp1_py
      #+begin_src python
    if( max( AgentsValues ) < V_AaE or iterations >= 100 ):
        print "Conditions de quittage"
        print "iterations : %d" % iterations
        print "Récompense : "
        print l2str(R)
        print "Actions de l'expert :"
        for action in ExpertsActions:
            print a2str(action)
            print ""
        print "Actions de l'agent"
        for action in AgentsActions:
            print a2str(action)
            print ""
        break
      #+end_src


    We glue it :
    #+begin_src python :tangle Exp1.py :noweb yes
# -*- coding: utf8 -*-
from numpy import *
import scipy
from TT_DP import *
from a2str import *
from TT import *

n = 6

m_E = 3
m_A = 2

<<TT_Exp1_py>>
    #+end_src
