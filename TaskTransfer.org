#+TITLE: Task Transfer on the GridWorld
#+EXPORT_EXCLUDE_TAGS: code
#+LATEX_HEADER: \usepackage{blkarray}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{tabularx}
#+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \newtheorem{lemma}{Lemma}

* Context

  In the /Reinforcement Learning/ (RL) framework, an agent is left to find the behavior that maximizes a cumulative reward in the long run. By correctly defining the reward, one can use the RL framewrk to make an agent fulfil a certain task.\\

  Often, the RL problem is solved in a /Markov Decision Process/ setting : the agent is said to be in a state $s_t\in S$ which respect the Markovian criterion (it contains all the information the agent needs) where it has to choose an action $a\in A$ which will make it transit to another state $s_{t+1}$, receiving a reward $R(s_t)$.\\

  A policy (often noted $\pi$) is a mapping $S\rightarrow A$. Each policy can be associated with its /value function/, a mapping $S\rightarrow \mathbb{R}$ which associates each state with the expected sum of discounted rewards an agent will get by following policy $\pi$ from this state. Formally, given a policy $\pi$, the value function is defined by : $V^\pi(s) = E\left[\left.\sum\limits_t \gamma^tR(s_t)\right|s_0=s,\pi\right]$.\\

  The RL problem can be casted as finding the /optimal policy/ (often noted $\pi^*$), the value function of which is greater, for every state, as the value function of any other policy : $\forall \pi, V^{\pi^*} \succeq V^\pi$.\\

  Any optimal policy is the fixed point of a greedy mechanism : by choosing, at each step, the action that maximizes the expected value function of the optimal policy for the next state one falls back to the optimal policy :$\pi^*(s_t) = \arg\max\limits_aE\left[\left.R(s) + \gamma V^{\pi^*}(s_{t+1})\right|s_t,a\right]$. This fixed point is unique, the previous equation can hence be used as a definition for the optimal policy.\\
  
  Luckily, this greedy mechanism is contractile. This means that by repeatedly computing the value function of any policy and then defining a new policy greedily with respect to the computed value function, one will utimately attain the optimal policy and the associated value function.\\

  For a certain  tasks, defining the corresponding reward can be a daunting process. Surely, one has at least an /idea/ about what states are preferable to others. This is however not enough to precisely define the numerical reward that will yeld the desired behavior. An example of such task can be the task of driving a car. Surely, it is preferable not to be too close to the car in front of ours. Surely, it is preferable not to brake too hard in order to avoid being rear-ended. But exactly how much is it preferable not to break too hard over not being too close to the car in front of ours ? Variations between the relative weights of these twe criteria in the reward will certainly lead to very different driving behaviors.\\

  A proposed workaround is the /Inverse Reinforcement Learning/ (IRL) method, where one tries to learn the reward optimized by an expert\footnote{The expert may be non human, in which one hope to use a slow or costly expert system to train a less costly agent.}. Then, the /idea/ about what states are preferable to others can be empirically used by a human expert driving the car. The reward function can be infered from the data of the demonstration.\\

  More formally, given an optimal policy $\pi^*$ and the set of actions $A$ from which the policy was drawn, one tries to guess the reward with respect to which this policy is optimal.\\

  Another step is to acknowledge that the agent may have different abilities than the expert, and use the reward as a piece of information defining a /task/. This will allow the transfer of a task from agent to agent, each fulfilling the task at the best of its abilities. For example in the driving problem, a computer will certainly have a better reflex time and a better stamina than a human, and thus could /hypothetically/ drive better than most human, increasing the overall safety of driving.\\

* Introduction

  The IRL problem was first defined in \cite{russell1998learning}, where its ill-posed nature was already noted : the set of reward with respect to which the expert's policy is optimal is far from being a singleton. Furthermore there exists some degenerative rewards that admit every policy as optimal and such bear no useful information.\\

  The work of \cite{russell1998learning} was further pushed in \cite{ng2000algorithms}, where a theorem was given which defines a sufficient and necessary condition for rewards to be solution of the IRL problem. Consinuous state space were also worked on. The problem of degenerative solutions, however, was not solved but worked around by the use of intuitive heuristics.\\

  The seminal work of (\cite{abbeel2004apprenticeship}) led to innovative algorithms (summed up in \cite{neu2009training}) allowing the portage of the IRL problem to settings where the dynamics of the system remains unknown (\cite{klein2011batch}). The problem of degenerative rewards was attacked with different heuristics.\\

  This paper aims at adding restrictions to the results of \cite{ng2000algorithms} in order to solve the ill-posedness of the IRL problem at its root and propose a view on the problem that allows for a generalization of the intuitive heuristics of the litterature. By doing so we hope to pave the way for ameliorations in the existing algorithms as well as new methods for practical IRL that would enable task transfer in real life settings.\\

* Notations
  
  The reward $R$ is here seen as a mapping from the state space $S$ to $\mathbb{R}$. Working in the discrete case, we use it as a vector $R\in \mathbb{R}^n$ indexed by the state space, the cardinality of which is $n$.\\

  The value function when follwing policy $\pi$ under reward $R$ is also defined as a vector : $V^\pi_R\in \mathbb{R}^n$ indexed by the state space. We specify the reward in the notation because in IRL the reward is the unknown and it can thus be interesting to use the value function of a policy under two different rewards.\\

  Actions are defined by the probability with which they make the agent transit from one state to the other. Formally, to each action $a$ is associated a $n\times n$ matrix $P_a$ whose element $(i,j)$ is the probability to transit to state $j$ when taking action $a$ in state $i$. A policy $\pi, S\rightarrow A : s \mapsto \pi(s)$ can be defined the same way, that is to say by associating it with a $P_\pi$ matrix whose $i$-th line is the $i$-th line of the $(P_a, a=\pi(s))$ matrix. We will from now on speak about action and policies using indiscriminately  the notation $a$ and $\pi$ or $P_a$ and $P_\pi$.\\

  The fixed point definition of the optimal policy can be rewriten as : $\forall s, \pi^*(s) = \arg\max\limits_a\left(R(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_R\right)$ using the notations we just have defined.\\

  We denote the set of the optimal policies with respect to a certain reward $R$ as $\Pi^*(R) = \left\{\pi^* | \forall \pi, V^{\pi^*}_R\succeq  V^{\pi}_R\right\}$.\\

  Throughout this paper, we will use the transitive notion of equivalence between rewards : 
  #+begin_definition
  Two rewards $R_1$ and $R_2$ are said to be /equivalent/ if $\Pi^*(R_1)$ = $\Pi^*(R_2)$. This is noted $R_1 \equiv R_2$.
  #+end_definition
  
  A reward is said to be degenerative when it is equivalent to the null vector ($R\equiv\begin{pmatrix}0\\ \vdots\\ 0\end{pmatrix}$). Indeed the null vector admits any policy as an optimal policy.
  
  #+begin_definition
  \label{startingvalue.def}
  The /starting value/ of a policy with respect to a certain reward is the value of the value function at a certain state $s_0$ where the agent or the expert usually start their trajectories.
  #+end_definition
  
  When training an agent on a problem where the reward is infered from the policy of an expert, the criteria we wish to maximize is the /starting value/ of the agent, with resspect the the /unknown true reward/.
  #+begin_definition
  \label{agentasexpert.def}
  The starting value of an agent with respect to the unknown true reward is refered to as the /agent as expert/ value.
  #+end_definition
  On a real life problem, one can not directly compute this value. It is however possible when benchmarking our algorithm on controlled experiments.
* Reducing the reward space
   In this subsection, we will show that there exists a manifold of dimension $n-2$ so that every non degenerative reward is equivalent to at least one element of the manifold.\\

#+begin_lemma
\label{alpha.lemma}
Let $R_1$ be a reward vector, \\
Let $R_2$ be a reward vector so that $\exists \alpha > 0, R_2=\alpha R_1$ \\
The following holds:  $R_1\equiv R_2$
#+end_lemma
   
   #+begin_proof
   Let $\pi \in \Pi^*(R_2)$ be.\\
   We have : 
   \begin{eqnarray*}
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_{R_2}\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_2(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(\alpha R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^t\alpha R_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\alpha\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)
   \end{eqnarray*}
   as $\alpha >0$, this is the same as :
   \begin{equation*}
   \forall s, \pi^*(s) = \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \end{equation*}
   which means that $\pi \in \Pi^*(R_1)$.\\
   The inverse path can be demonstrated by replacing $R_1$ by $R_2$ and using $1\over \alpha$, therefore $\pi \in \Pi^*(R_2) \Leftrightarrow \pi \in \Pi^*(R_1)$.
   #+end_proof
   
#+begin_lemma
\label{lambda.lemma}
Let $R_1$ be a reward vector, \\
Let $\mathbf{1}$ be the column vector whose $n$ elements are all equal to $1$,\\
Let $R_2$ be a reward vector so that $\exists \lambda \in \mathbb{R}, R_2= R_1 + \lambda\mathbf{1}$,\\
The following holds:  $R_1\equiv R_2$
#+end_lemma
  
   #+begin_proof
   Let $\pi \in \Pi^*(R_2)$ be.\\
   We have : 
   \begin{eqnarray*}
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_{R_2}\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_2(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^t\left(R_1(s_t)+\lambda\right)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]+ \gamma E\left[\left.\sum\limits_t\gamma^t\lambda\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]+ \gamma\sum\limits_t\gamma^t\lambda\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right] + \lambda + \gamma\sum\limits_t\gamma^t\lambda\right)\\
   \end{eqnarray*}
   as $\lambda + \gamma\sum\limits_t\gamma^t\lambda$ does not depend on $a$, this is the same as :
   \begin{equation*}
   \forall s, \pi^*(s) = \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \end{equation*}
   which means that $\pi \in \Pi^*(R_1)$.\\
   The inverse path can be demonstrated by replacing $R_1$ by $R_2$ and using $-\lambda$, therefore $\pi \in \Pi^*(R_2) \Leftrightarrow \pi \in \Pi^*(R_1)$.
   #+end_proof

   #+begin_theorem
   Let $M = \{R|\mathbf{1}^TR =  0, ||R||_1 = 1\}$ be,\\
   The following holds : $\forall R \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}, \exists R'\in M, R'\equiv R$.
   #+end_theorem

   #+begin_proof
   Let $R \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}$ be,\\
   Let $\lambda = -{\mathbf{1}^TR\over n}$ be,\\
   Let $\alpha = {1\over ||R+\lambda\mathbf{1}||_1}$ be,\\
   Let $R' = \alpha(R+\lambda\mathbf{1})$ be,\\
   We have :
   \begin{eqnarray}
   \mathbf{1}^TR' &=& \mathbf{1}^T\alpha(R+\lambda\mathbf{1})\\
   \mathbf{1}^TR' &=& \alpha\mathbf{1}^TR + \alpha\lambda\mathbf{1}^T\mathbf{1}\\
   \mathbf{1}^TR' &=& \alpha\mathbf{1}^TR + \alpha\lambda n\\
   \mathbf{1}^TR' &=& \alpha\mathbf{1}^TR - \alpha{\mathbf{1}^TR\over n}n\\
   \mathbf{1}^TR' &=& 0
   \end{eqnarray}
   and :
   \begin{eqnarray}
   ||R'||_1 &=& ||\alpha(R+\lambda\mathbf{1})||_1\\
   ||R'||_1 &=& \alpha||R+\lambda\mathbf{1}||_1\\
   ||R'||_1 &=& {1\over ||R+\lambda\mathbf{1}||_1}||R+\lambda\mathbf{1}||_1\\
   ||R'||_1 &=& 1\\
   \end{eqnarray}
   So $R'\in M$.

   Also, according to lemma \ref{lambda.lemma}, we have $R + \lambda\mathbf{1} \equiv R$, and according to lemma \ref{alpha.lemma}, we have $R' = \alpha(R+\lambda\mathbf{1}) \equiv R+\lambda\mathbf{1}$ which by transitivity yelds : $R' \equiv R$.
   #+end_proof

* Finding interesting rewards
   In \cite{ng2000algorithms}, we are given a necessary and sufficient condition for a reward $R$ to admit a given policy as optimal : for every action $P_a$ the expert following $P_\pi$ had the possibility to take, the following matrix inequality must be met : 
   \begin{equation}
   \label{ng2000algorithms.eqn}
   (P_\pi-P_{a})(I-\gamma P_\pi)^{-1}R\succeq 0
   \end{equation}

   Although the proof can be found in the formentionned paper, we find useful to recall its main argument here : this inequalities stem from the fact that for every action $a$, the expected value of the next state must be less than or the same as the expected value of the next state for taking action $\pi(s)$. This is written as $P_\pi V^\pi \succeq P_a V^\pi$. Consequently, the $i$-th line of the $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix is a constraint on $X$ that, if satisfied, will make it preferable to choose action $\pi(i)$ over $a$ in state $i$.\\
 
   Equation \ref{ng2000algorithms.eqn} yelds at most $Card(A)\cdot n - n = (Card(A)-1)n$ constraints. There is $Card(A)$ matrices $P_a$, each yelding $n$ constraints. $n$ of there, however, are null because once for each state we will have $a=\pi(s)$ and the resulting line will be filled with zeros.\\

   This is a Linear Programming problem. By adding the supplemantary constraints stemming from the definition of $M$, i.e. that $\mathbf{1}^TR=0$ and $||R||_1=1$, we restrict the solutions to the previously defined $n-2$-dimensional manifold.\\

   Drawing ideas from the simplex algorithm of the LP framework (http://www2.isye.gatech.edu/~spyros/LP/LP.html), we propose a compact and exhaustive description of the solutions of this augmented LP problem.\\

   Let $C$ be the matrix created by slecting the $m$ non null lines of the constraints matrices $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$, so that all lines are normalized (this does not change the constraints) and every line is unique. In order to improve the computational complexity of the algorithm, it may be possible to further restrict what goes in the $C$ matrix, but this is a little off-topic and not necessary.\\

   Let $X$ be the unknown vector. All the forementionned constraints can now be expressed as follow :
   \begin{eqnarray}
   &CX \succeq 0\\
   &\mathbf{1}^TR=0\\
   &||X||_1=1
   \end{eqnarray}
** Standard form
   
   These constraints will now be put in the /standard form/ $AX=b, X\succeq 0$, typically used in the simplex algorithm (which is not fully applicable here because we don't have a cost function).\\
  
  The $X\in \mathbb{R}^n$ part is not satisfactory because in the standard form all unknowns must be grater than or equal to 0. To get to this form, for every component $x_i$ of $X$, we define $x_i^+\geq0$ and $x_i^-\geq0$ so that $x_i = x_i^+ - x_i^-$. We now define the $X'$ matrix as 
  \begin{equation}
  X'=\begin{pmatrix} x_1^+\\x_1^-\\ \dots \\ x_n^+\\x_n^- \end{pmatrix}
  \end{equation}
  Accordingly, the $C'$ matrix is defined as (using $c_i$ to denote the $i$-th column of C) :
  \begin{equation}
  C'=\begin{pmatrix} c_1 | -c_1 | c_2 | -c_2 | \dots |c_n|-c_n \end{pmatrix}
  \end{equation}
  and the $\mathbf{1}' \in \mathbb{R}^{2n}$ vectoris defined as :
  \begin{equation}
  \mathbf{1}'=\begin{pmatrix} 1,-1, 1, -1\dots 1,-1\end{pmatrix}^T
  \end{equation}

  We are now a bit closer to the standard form :
  \begin{eqnarray}
  &C'X' \succeq 0 \\
  &\mathbf{1}'^TX'=0\\
  &||X||_1=1\\
  &X'\succeq 0\\
  \end{eqnarray}
  
  The last thing to do is to introduce $m$ positive slack variables in the form of a matrix
  \begin{equation}
  S = \begin{pmatrix}s_1\\ \vdots\\ s_{m}\end{pmatrix} \succeq 0
  \end{equation}
  This allows us to change the inequality into an equality : if $a\geq b$, then $\exists s \geq 0, a-s = b$. We finally obtain something in the standard form, the last two lines of which respectively represents the constraints $\mathbf{1}'^TX'=0$ and $||X||_1=1$ :
  \begin{eqnarray}
  \label{LPStandardForm.eqn}
  \begin{blockarray}{(cc)}
  \begin{block*}{c|c}
  C'& -Id_m  \\
  \cline{1-2}
  \begin{block*}{c|c}
  \mathbf{1}'^T&0 \\
  \end{block*}
  \cline{1-2}
  \begin{block*}{c|c}
  \mathbf{1}^T\mathbf{1}^T&0 \\
  \end{block*}
  \end{block*}
  \end{blockarray} 
  \begin{blockarray}{(c)}
  \begin{block*}{c}
  X' \\
  \cline{1-1}
  \begin{block*}{c}
  S\\
  \end{block*}
  \end{block*}
  \end{blockarray}
  = 
  \begin{blockarray}{(c)}
  \begin{block*}{c}
  0 \\
  \vdots \\
  0 \\
  1\\
  \end{block*}
  \end{block*}
  \end{blockarray}\\
  \label{C1.eqn}
  X'\succeq 0\\
  \label{C2.eqn}
  S \succeq 0
  \end{eqnarray}

** Basic feasible solutions
   We now have $m+2$ equations in $2n+m$ variables. This is not solvable by usual means. Let us not forget that appart for the last two lines added in order to make the solution unique and thus the whole system solvable, all the constraints are inequalities. As the solution to such a system of inequalities is not unique, we use the trick from the simplex algorithm that consist in choosing $m+2$ columns and solving the resulting linear system (if possible). The remaining variables are set to zero. This dictates which inequalities are binding are which are not : if a column concerning a slack variable (say, $s_i$) is not choosen, the corresponding inequality becomes an equality ($C^iX = 0$, with $C^i$ being the $i$-th line of C). If both column concerning a state (say, $x_j^+$ and $x_j^-$ are not choosen, the corresponding two lines of the $X'\succeq 0$ condition become binding and thus $x_j = 0$.\\

   Every solvable $(m+2) \times (m+2)$ system, that is to say every system resulting from a selection that does not make two incompatible inequalities binding will result in what is called a /basic feasible solution/ (bfs) in the LP literature. From every /bfs/ abiding by the additional constraints of inequalities \ref{C1.eqn} and \ref{C2.eqn}, it is easy to go back to the corresponding reward $R^{bfs}$ by undoing the transformation that allowed us to get to the standard form. This is simply done by :
   \begin{equation}
   R^{bfs}_i = X'^{bfs}_{2i-1} - X'^{bfs}_{2i}
   \end{equation}
   
   
   A patient systematic enumaration of all the $(m+2) \times (m+2)$ systems will yeld a small set of interesting rewards (see section [[Experiments]]).\\

   If the computational comlpexity of the exhaustive enumaration is too high, one can try to reduce it by diverse means, such as not even enumerating the obviously not solvable systems (for example one involving both $x_j^+$ and $x_j^-$ for a any given $j$) or removing useless constraints (such as constraints being implied by others) in order to decrease $m$. This is however not tractable and the even a relatively small problem like a $5 \times 5$ gridworld makes the combinatorial explosion of the number of possible systems difficult to avoid.\\

   Another course of action is to use heuristics. In \cite{ng2000algorithms}, the authors suggest adding a cost function to the linear program in order to find meaningful rewards. They used natural criterion in this cost function that made the search successful. Limiting the search to a finite object as we propose sometimes make this cost function no longer necessary, as we can still select the kind of reward we want by carefully selecting which column to include when building a $(m+2) \times (m+2)$ system.\\

   One natural criteria introduced in \cite{ng2000algorithms}, and later ingrained at the core of \cite{abbeel2004apprenticeship} is to assume that in each state the expert did not had much of a choice and had to take the action it took, because the corresponding state action value was actually higher (and not just equal to) any other action. In our framework, a reward satisfying this criteria will be found by solving the systems were as much slack variables as possible are selected (it is always possible to select all the slack variables, are there are only $m$ of them, but such a system will not always be solvable).\\

   Another criteria mentioned in \cite{ng2000algorithms} is the simplicity of the reward. In a MDP with a non completely erratic behavior, selecting as less slack variables as possible will lead to a scarce reward (after a wise translation of vector $\lambda \mathbf{1}$). Such a reward will probably admit as optimal more policies than only the expert's, and we argue that this kind of reward is more prone to represent the task at sake than the ones satisfying the other criteria, which tend to lead to an imitation of the expert. Another argument for this is that human defined reward on known problems tend to be scarce and allow for more than one optimal policy. Sadly, the number of system to try is also guided by a combinatoric mechanism and is almost as intractable as the naive enumeration.\\

   The technique of letting all the slack variables be free (from now on refered to as the /all slacks/ variant) can prove useful (and indeed it worked on the gridworld, see section [[Experiments]]). As $m$ slack variables are selected, only $2$ columns corresponding to a component of the reward are also selected. This means that if such a system is solvable, we get a scarce reward (null everywhere but in two states) that justify the expert's behavior by a certan margin.\\

   Sadly, other experiments proved that the /all slacks/ variant does not always yeld an answer. Furthermore, the patient enumeration does not always lead to rewards allowing for a perfect task transfer, but only for an almost perfect task transfer.\\

   Further research is needed in the light of those experiment.

* Experiments
** Gridworld
   See [[file:Gridworld/TT_Exp1.org][here]].
** Random rewards and random experts
*** Goal
    
   The goal of these experiments is to empirically test some hypothesis about our method. An empirical experiment is no mathematical proof, except when the empirical evidence can be used as a counter example to demonstrate the falsehood of a hypothesis. What we try here is to gain some hindsight on the way our method work, and not use the empirical evidence to strictly affirm that it does.
   
   The two hypothesis we want to test here are :
   - The /slacks free/ method always yeld at least one reward.
   - The naive enumartion method always yeld at least one reward for which the agent as expert value is attained.
*** Protocol
    The two hypothesis will be tested separately. In both case, for a certain number $n$ of states, a reward vector $R\in \mathbb{R}^n$ will be randomly choosen. The action of the expert or the agent will also be randomly dranw, by randomly defining the associated probability matrices.\\

    The protocol of the first experiment to test whether the /slacks free/ method always yeld at least one reward is the following :
    - for a few different $n$ :
      - do a certain number of times :
	- $R\leftarrow rand(n)$
	- do $m_E$ times :
	  - $A_E \leftarrow A_E \cup {rand(n,n)}$, with the sum over the lines is always 1
	- $P_\pi \leftarrow DP( R, A_E )$
	- $Rewards \leftarrow TaskTransferSlacksFree( P_\pi, A_E )$
	- print $card(Rewards)$
	  

    The second experiment, to test whether the naive enumartion method always yeld at least one reward for which the agent as expert value is attained, goes like this :
    - Do :
      - Define a random reward $\mathbf{R}$ of size $n$
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
      - Compute the expert's policy by solving the MDP : $\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
      - Compute the Agent as Expert policy by solving the MDP for the true reward function : $\pi \leftarrow DP( R, \{P_{a^A_i}\}_i)$
      - Store the Agent as Expert value $V^{AaE}(s_0)$
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
      - Plot the agent's true values along with the expert's and the Agent as Expert value
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
*** Results
    The two experiments disproved both hypothesis. The /slack free/ method yeld one or more reward in a surprisingly small number of case. Further investigation is needed to understand why, and how it could be changed.

    The naive enumeration does not always yeld a reward for which the agent as expert value will be met. However, the best starting value attained by the agent is always very close to the agent as expert value, and the proportion of trials where the agent as expert value is not reached is quite small.

    These experiments, by providing counter examples, formally rejects our hypothesis and pave the way for future research.

*** Code :code:
**** Exp 2
    The code of the first protocol is : 
    #+begin_src python :tangle TT_Exp2.py
from numpy import *
import scipy
from DP import *
from a2str import *
from TT import *
import sys

m_E = 4
m_A = 4
for n in range(4,7):
    meanNbRewards = 0
    for j in range(0,10):
        R = scipy.rand( n )
        ExpertsActions = []
        for i in range(0,m_E):
            P_i = scipy.rand(n,n)
            for line in P_i:
                line /= sum(line) #Sum of proba = 1, so we normalize the random line
            ExpertsActions.append(P_i)
        P_pi = DP( R, ExpertsActions )
        #import pdb;pdb.set_trace()
        ttRewards = TT_SF( P_pi, ExpertsActions )
        nbRewards = 0
        if( ttRewards == None ):
            nbRewards = 0
        elif( len( ttRewards.shape) == 1 ): #If there is only one reward
            ttRewards = asarray([ttRewards]) #Cast as a matrix anyway, the code below expects a matrix and not a vector
            nbRewards = 1
        else:
            nbRewards = ttRewards.shape[0]
        sys.stderr.write("n = %d, j=%d, nbRewards = %d\n"%(n,j,nbRewards))
        meanNbRewards+=nbRewards
    meanNbRewards/=10.
    print "%d %f" % (n,meanNbRewards)
    #+end_src

    It can be tangled with:
    #+srcname: TT_Exp2_code_make
    #+begin_src makefile
TT_Exp2.py: TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
    #+end_src

    It can be run with :
    #+srcname: TT_Exp2_make
    #+begin_src makefile
TT_Exp2: TT_Exp2.py DP.py a2str.py TT.py
	python TT_Exp2.py
    #+end_src

    And cleaned with :
    #+srcname: TT_Exp2_clean_make
    #+begin_src makefile
TT_Exp2_clean:
	find . -maxdepth 1 -iname "TT_Exp2.py"   | xargs $(XARGS_OPT) rm
    #+end_src

**** Exp 3
    We rewrite the second protocol, adding the corresponding code at each line :
    - Do :
      #+srcname: TT_Exp3_py
      #+begin_src python
iterations = 0
while True:
    iterations+=1
      #+end_src
      - Define a random reward $\mathbf{R}$ of size $n$
	#+srcname: TT_Exp3_py
        #+begin_src python
    R = scipy.rand( n )
        #+end_src
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
	#+srcname: TT_Exp3_py
        #+begin_src python
    ExpertsActions = []
    for i in range(0,m_E):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        ExpertsActions.append(P_i)
        #+end_src
      - Compute the expert's policy's transition probabilities by solving the MDP : $P_\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp3_py
        #+begin_src python
    P_pi = DP( R, ExpertsActions )
        #+end_src
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp3_py
        #+begin_src python
    ttRewards = TT( P_pi, ExpertsActions )
    if( len( ttRewards.shape) == 1 ): #If there is only one reward
        ttRewards = asarray([ttRewards]) #Cast as a matrix anyway, the code below expects a matrix and not a vector
        #+end_src
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
	#+srcname: TT_Exp3_py
        #+begin_src python
    AgentsActions = []
    for i in range(0,m_A):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        AgentsActions.append(P_i)

        #+end_src
      - Compute the Agent as Expert policy's transition probabilities by solving the MDP for the true reward function : $P_\pi^{AaE} \leftarrow DP( R, \{P_{a^A_i}\}_i)$
	#+srcname: TT_Exp3_py
        #+begin_src python
    P_AaE = DP( R, AgentsActions )
        #+end_src
      - Store the Agent as Expert value $V^{AaE}(s_0)$
	#+srcname: TT_Exp3_py
        #+begin_src python
    V_AaE = dot(linalg.inv(identity(n) - 0.9*P_AaE),R.transpose())[0] #0 is the initial state
        #+end_src
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	#+srcname: TT_Exp3_py
        #+begin_src python
    AgentsValues = []
    for reward in ttRewards:
        #+end_src
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	  #+srcname: TT_Exp3_py
          #+begin_src python
        P_pi_l = DP( reward, AgentsActions )
          #+end_src
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
	  #+srcname: TT_Exp3_py
          #+begin_src python
        AgentsValues.append( dot( linalg.inv(identity(n) - 0.9*P_pi_l), R.transpose() )[0])
          #+end_src
      - Plot the agent's true values along with the expert's and the Agent as Expert value
	#+srcname: TT_Exp3_py
        #+begin_src python
    print "Expert : %f" % dot( linalg.inv(identity(n) - 0.9*P_pi), R.transpose())[0]
    print "Agent as Expert : %f" % V_AaE
    print "Best of Agent : %f" % max( AgentsValues )
        #+end_src
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
      #+srcname: TT_Exp3_py
      #+begin_src python
    if( max( AgentsValues ) < V_AaE or iterations >= 10 ):
        print "Conditions de quittage"
        print "iterations : %d" % iterations
        print "Recompense : "
        print l2str(R)
        print "Actions de l'expert :"
        for action in ExpertsActions:
            print a2str(action)
            print ""
        print "Actions de l'agent"
        for action in AgentsActions:
            print a2str(action)
            print ""
        break

      #+end_src


    We glue it :
    #+begin_src python :tangle TT_Exp3.py :noweb yes
from numpy import *
import scipy
from DP import *
from a2str import *
from TT import *

n = 4

m_E = 3
m_A = 2

<<TT_Exp3_py>>
    #+end_src

    It can be tangled with :
    #+srcname: TT_Exp3_code_make
    #+begin_src makefile
TT_Exp3.py: TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
    #+end_src
    
    It can be run with :
    #+srcname: TT_Exp3_make
    #+begin_src makefile
TT_Exp3: TT_Exp3.py DP.py a2str.py TT.py
	python TT_Exp3.py
    #+end_src
    And cleaned by :
    #+srcname: TT_Exp3_clean_make
    #+begin_src makefile
TT_Exp3_clean:
	find . -maxdepth 1 -iname "TT_Exp3.py"   | xargs $(XARGS_OPT) rm
    #+end_src

* Code :code:
** Main code
*** Dynamic programming
    We need a MDP solver. This quick and dirty dynamic programming implementation will do the trick :
    #+begin_src python :tangle DP.py
from numpy import *
import scipy
import pdb

g_vReward = []
g_vActions = []
g_vV = []
g_vPi = []
g_fGamma = 0.9

def Q( s, a ):
    return g_vReward[s] + 0.9 * dot( (g_vActions[ a ])[s], g_vV.transpose() )

def DP( Reward, Actions ):
    "Returns the probability matrix corresponding to the optimal policy with respect to the given reward and the given actions. Actions are given in the form of a probability matrix. Probability matrices are so that the $(i,j)$ element gives the probability of transitioning to state $j$ upon taking action $a$ in state $i$"
    #pdb.set_trace()
    global g_vReward
    global g_vActions
    global g_vV
    global g_vPi
    n = len( Reward )
    m = len( Actions )
    g_vReward = Reward
    g_vActions = Actions
    g_vPi = map( int, floor( scipy.rand( n )*m ) )
    g_vV = scipy.rand( n )
    #While things change,
    changed = True
    while changed:
        changed = 0
        #For each state
        for s in range(0,n):
            old_pi = g_vPi[ s ]
            old_V = g_vV[ s ]
            chosen_a = 0
            max_Q = Q( s, chosen_a )
            #Select the action that maximizes Q
            for a in range(0,m):
                fQ = Q( s, a )
                if( fQ > max_Q ):
                    max_Q = fQ
                    chosen_a = a
            g_vPi[ s ] = chosen_a
            g_vV[ s ] = max_Q
            if( g_vPi[ s ] != old_pi or g_vV[ s ] != old_V ):
                changed = 1
    #Construct the pobability matrix from the policy
    Ppi = zeros((n,n))
    for s in range(0,n):
        Ppi[s] = (g_vActions[ g_vPi[s] ])[s]
    return Ppi


def DP_txt( Reward, Actions, Filename ):
    "Same as DP, mais sauve la fonction de valeur dans un fichier."
    #pdb.set_trace()
    global g_vReward
    global g_vActions
    global g_vV
    global g_vPi
    n = len( Reward )
    m = len( Actions )
    g_vReward = Reward
    g_vActions = Actions
    g_vPi = map( int, floor( scipy.rand( n )*m ) )
    g_vV = scipy.rand( n )
    #While things change,
    changed = True
    while changed:
        changed = 0
        #For each state
        for s in range(0,n):
            old_pi = g_vPi[ s ]
            old_V = g_vV[ s ]
            chosen_a = 0
            max_Q = Q( s, chosen_a )
            #Select the action that maximizes Q
            for a in range(0,m):
                fQ = Q( s, a )
                if( fQ > max_Q ):
                    max_Q = fQ
                    chosen_a = a
            g_vPi[ s ] = chosen_a
            g_vV[ s ] = max_Q
            if( g_vPi[ s ] != old_pi or g_vV[ s ] != old_V ):
                changed = 1
    #Construct the pobability matrix from the policy
    Ppi = zeros((n,n))
    for s in range(0,n):
        Ppi[s] = (g_vActions[ g_vPi[s] ])[s]
    savetxt( Filename, g_vV, "%e", "\t" )
    return Ppi

    #+end_src
*** Finding the BFS
   This code finds the coordinates of the vertices of the polytope, also known as the /basic feasible solutions/.\\

   We create the $A$ and $b$ matrices of the standard form

#+srcname:TT_linesCreation_py
#+begin_src python
A = zeros((g_iM + 2, 2*g_iN + g_iM))
#C'
for i in range(0,g_iN):
    A[0:g_iM,2*i] = g_mC[:,i]
    A[0:g_iM,2*i+1] = -g_mC[:,i]
#\mathbf{1}'
for i in range(0,g_iN):
    A[g_iM,2*i] = 1
    A[g_iM,2*i+1] = -1
#\mathbf{1}\mathbf{1}
A[g_iM+1,0:2*g_iN] = ones((1,2*g_iN))
#-Id_(m)
A[:g_iM,2*g_iN:] = -identity(g_iM)

b = zeros((g_iM+2,1))
b[g_iM+1] = 1
#print "A and b matrices"
#print a2str(A)
#print a2str(b)
#+end_src

   The previously mentionned heuristics are defined here :
   - the naive enumeration of all the combinations :
     #+srcname: TT_naiveEnumeration_py
     #+begin_src python
for lslice in itertools.combinations(range(0,2*g_iN+g_iM),g_iM+2):
     #+end_src
   - all slack variables and two states (the expert has the best policy) :
     #+srcname: TT_slacksfree_py
     #+begin_src python
for partialLslice in itertools.combinations(range(0,2*g_iN),2):
    lslice = partialLslice + tuple(range(2*g_iN,2*g_iN+g_iM))
     #+end_src
     
   For every $m+2$ combination of columns, we solve the resulting linear system, go back from that solution to the basic feasible solution and store it in a set 
#+srcname:TT_linearSystem_py
#+begin_src python
    #if( index % 1000 == 0 ):
        #sys.stderr.write("Combinaison No %d\n" % index)
    #print "Combinaison No %d" % index
    index+=1
    #print lslice
    #print "Subsystem"
    #print A[:,lslice]
    if( abs(linalg.det( A[:,lslice] ) ) > 0.00001 ):#Ugly hack for floating point precision
        partialStandardSol = linalg.solve(A[:,lslice],b)
        if( all( partialStandardSol > -0.00000001 ) ): #Ugly hack for floating point precision
            standardSol =  zeros((2*g_iN+g_iM,1))
            standardSol[lslice,:] = partialStandardSol
            #print "Standard solution exists : "
            #print standardSol
            R = zeros((g_iN,1))
            for i in range(0,g_iN):
                R[i] = standardSol[2*i] - standardSol[2*i+1]
            #print "Corresponding Reward : "
            #sys.stderr.write("Found a reward on comb %d\n" % index)
            #print linalg.det( A[:,lslice])
            #print R.transpose()
            #if( any(dot( g_mC, R )<-0.00001) ):
                #print "Ne respecte pas les contraintes"
                #print g_mC
                #print R
                #print dot( g_mC, R )
            if( any( abs(R) > 0.001 ) ):
                BFS.add(l2str((R.transpose())[0]))
        #else:
            #print "Negative"
            #print partialStandardSol
    #else:
        #print "No solutions"
        #print linalg.det( A[:,lslice] )
#print "Nb Combinaisons %d" % index

#+end_src

   We glue it together

#+begin_src python :noweb yes :tangle TaskTransfer.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
import itertools
from sets import *
from a2str import *

#import pdb

g_mC = genfromtxt(sys.argv[1])
g_iN = g_mC.shape[1]
g_iM = g_mC.shape[0]

#print "C matrix : %d equations in %d variables"% (g_iM,g_iN)
<<TT_linesCreation_py>>

#print A
BFS = Set()
index = 0
standard_sols=[]
<<TT_naiveEnumeration_py>>
<<TT_linearSystem_py>>

#print "All solutions are : "
toPrint = ""
for R in BFS:
    toPrint+=R
print toPrint
#+end_src


   We also provide a version with the slacks free method
#+begin_src python :noweb yes :tangle TaskTransfer_SF.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
import itertools
from sets import *
from a2str import *

#import pdb

g_mC = genfromtxt(sys.argv[1])
g_iN = g_mC.shape[1]
g_iM = g_mC.shape[0]

#print "C matrix : %d equations in %d variables"% (g_iM,g_iN)
<<TT_linesCreation_py>>

#print A
BFS = Set()
index = 0
standard_sols=[]
<<TT_slacksfree_py>>
<<TT_linearSystem_py>>

#print "All solutions are : "
toPrint = ""
empty = True
for R in BFS:
    toPrint+=R
    empty = False
print toPrint
if empty:
    exit(1)
else:
    exit(0)
#+end_src

*** Computing the $C$ constraints matrix
    Given a $P_\pi$ matrix and several $P_a$ matrices, this code compute the $C$ constraint matrix consisting of the non null, non repeating lines of $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$, plus the $[1, \dots, 1]$ vector. FIXME : why ?\\

    We add the non null lines of every $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix to a set
#+srcname:TT_PpiMinusPaEtc_py
#+begin_src python
g_sC = Set()
for Pa in g_lActionMatrices:
    A = dot((g_mPpi - Pa),linalg.inv( identity(g_iN)-(g_fGamma*g_mPpi) ))
    for line in A:
        if( any( line != zeros((1,g_iN)) ) ):
               g_sC.add( l2str_fullprecision(line/linalg.norm(line)) )

#+end_src
    
    We glue this.
#+begin_src python :noweb yes :tangle Constraint.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
from sets import *
from a2str import *
  
#import pdb
g_mPpi = genfromtxt( sys.argv[1] )
g_lActionMatrices = []
for i in range(2,len(sys.argv)):
    g_lActionMatrices.append( genfromtxt(sys.argv[i]) )
g_iN = g_mPpi.shape[0]
g_fGamma = 0.9

<<TT_PpiMinusPaEtc_py>>

toPrint = ""
for line in g_sC:
    toPrint += line
print toPrint

#+end_src
    
*** Glue function
    A python function that glue both the constraints computation and the TaskTransfer algorithm is provided :
    #+begin_src python :tangle TT.py
import os
from numpy import *
import scipy

def TT( P_pi, Actions ):
    index = 0
    savetxt( "TT_tmp_%d"%index, P_pi, "%e", "\t" )
    index +=1
    for action in Actions:
        savetxt( "TT_tmp_%d"%index, action, "%e", "\t" )
        index +=1

    cmd = "python Constraint.py "
    for i in range(0,index):
        cmd+="TT_tmp_%d "%i
    cmd += " > TT_tmpC.mat"
    os.system( cmd )

    cmd = "python TaskTransfer.py TT_tmpC.mat > TT_tmpR.mat"
    ret = os.system( cmd )
    
    if( ret == 0):
        answer = genfromtxt( "TT_tmpR.mat" )
    else:
        answer = None

    cmd = "rm TT_tmpC.mat TT_tmpR.mat TT_tmp_0"
    index = 1
    for action in Actions:
        cmd+= " TT_tmp_%d"%index
        index +=1
    os.system( cmd )

    return answer

def TT_SF( P_pi, Actions ):
    index = 0
    savetxt( "TT_tmp_%d"%index, P_pi, "%e", "\t" )
    index +=1
    for action in Actions:
        savetxt( "TT_tmp_%d"%index, action, "%e", "\t" )
        index +=1

    cmd = "python Constraint.py "
    for i in range(0,index):
        cmd+="TT_tmp_%d "%i
    cmd += " > TT_tmpC.mat"
    os.system( cmd )

    cmd = "python TaskTransfer_SF.py TT_tmpC.mat > TT_tmpR.mat"
    ret = os.system( cmd )

    if( ret == 0):
        answer = genfromtxt( "TT_tmpR.mat" )
    else:
        answer = None

    cmd = "rm TT_tmpC.mat TT_tmpR.mat TT_tmp_0"
    index = 1
    for action in Actions:
        cmd+= " TT_tmp_%d"%index
        index +=1
    os.system( cmd )

    return answer

    #+end_src
** Tests
*** Test in 3D
    We test the program in a small setting so that the reward vector only has three component $x$, $y$ and $z$.

    
     First let us define the following constraints matrices :
     - This one means that we must have $x\geq y$ and $y \geq z$ 
       #+begin_src text :tangle test/TT_CT01.mat
1	-1	0
0	1	-1
       #+end_src
     - this one is the same, with an added last line that explicitely specify that $x \geq z$. The last line does not change the meaning of the contraints, but we add it to see if the program works even when fed with a useless constraint
       #+begin_src text :tangle test/TT_CT02.mat
1	-1	0
0	1	-1
1	0	-1
       #+end_src

       
     There are only three kind solutions satisfying these constraints (apart from the degenerative solution $x=y=z$) :
     - $x>y>z$
     - $x>y=z$
     - $x=y>z$
     
       
     As the $L_1$ norm of the anwers must be $1$, the expected output for both input is :
     #+begin_src text :tangle test/TT_expectedOutT0.mat
 2.50e-01	 2.50e-01	-5.00e-01	
 5.00e-01	 0.00e+00	-5.00e-01	
 5.00e-01	-2.50e-01	-2.50e-01	
     #+end_src

We now build Makefile targets that calls the program on the previously defined $C$ matrices and match the output with the expected output. Note the use of the \texttt{sort} command to make sure both output are in the same order and the diff command succeeds.

#+srcname: TT_Test0_make
#+begin_src makefile
TT_test0: TaskTransfer.py test/TT_CT01.mat test/TT_CT02.mat test/TT_expectedOutT0.mat a2str.py
	python TaskTransfer.py test/TT_CT01.mat | sort > test/TT_outT01.mat
	python TaskTransfer.py test/TT_CT02.mat | sort > test/TT_outT02.mat
	../Utils/matrix_diff.py test/TT_expectedOutT0.mat test/TT_outT01.mat
	../Utils/matrix_diff.py test/TT_expectedOutT0.mat test/TT_outT02.mat
	rm test/TT_outT01.mat
	rm test/TT_outT02.mat
#+end_src
Tangling : 
#+srcname: TT_Test0_code_make
#+begin_src makefile
test/TT_CT01.mat: TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

test/TT_CT02.mat:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

test/TT_expectedOutT0.mat:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

#+end_src
Cleaning :
#+srcname: TT_Test0_clean_make
#+begin_src makefile
TT_Test0_clean:
	find test -maxdepth 1 -iname "TT_CT01.mat"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_CT02.mat" | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_expectedOutT0.mat" | xargs $(XARGS_OPT) rm
#+end_src
*** Task Transfer on a 2x2 Gridworld
    In this simple setting we imagine a 2x2 gridworld and two experts. Both experts optimize the same reward, located in the north east corner. Both experts can choose between the same actions at each step : the four compass directions. The first expert's policy is NORTH, EAST, the second one is EAST, NORTH. We want to see in this experiment if the true reward is among the set of reward output by our algorithm.

    The states are indexed fom 0 to 3, in the reading order.


    We begin by defining the two matrices $P_{\pi_1}$ and $P_{\pi_2}$ relative to both expert's policies :
    - $P_{\pi_1}$ is :
      #+begin_src text :tangle test/TT_PPi1.mat
0	1	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src
    - $P_{\pi_1}$ is :    
      #+begin_src text :tangle test/TT_PPi2.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	1	0	0
      #+end_src
      
    
    We then define the four $P_a$ matrices relative to each action :
    - $P_{NORTH}$ is :
      #+begin_src text :tangle test/TT_PNorth.mat
1	0	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src      
    - $P_{EAST}$ is :
      #+begin_src text :tangle test/TT_PEast.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	0	0	1
      #+end_src
     - $P_{SOUTH}$ is :
       #+begin_src text :tangle test/TT_PSouth.mat
 0	0	1	0
 0	0	0	1
 0	0	1	0
 0	0	0	1
       #+end_src
     - $P_{WEST}$ is :
       #+begin_src text :tangle test/TT_PWest.mat
 1	0	0	0
 1	0	0	0
 0	0	1	0
 0	0	1	0
       #+end_src
       

    The constraint matrices relative each expert are computed :
    #+srcname: TT_Test1_make
    #+begin_src makefile
TT_test1: Constraint.py test/TT_PPi1.mat test/TT_PPi2.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat TaskTransfer.py test/TT_expectedOutT1.mat a2str.py
	python Constraint.py test/TT_PPi1.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C1.mat
	python Constraint.py test/TT_PPi2.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C2.mat

    #+end_src

    The conjoint constraint matrix is extracted (duplicate lines are removed) :
    #+srcname: TT_Test1_make
    #+begin_src makefile
	cat test/TT_C1.mat test/TT_C2.mat | sort | uniq > test/TT_CBoth.mat

    #+end_src

    The TaskTransfer program is run and its output is compared against what is expected :
    #+srcname: TT_Test1_make
    #+begin_src makefile
	python TaskTransfer.py test/TT_CBoth.mat | sort > test/TT_outT1.mat
	../Utils/matrix_diff.py test/TT_expectedOutT1.mat test/TT_outT1.mat
	rm test/TT_C1.mat test/TT_C2.mat test/TT_CBoth.mat test/TT_outT1.mat
    #+end_src

    #+begin_src text :tangle test/TT_expectedOutT1.mat
0.00e+00	 5.00e-01	-5.00e-01	 0.00e+00	
1.67e-01	 1.67e-01	-5.00e-01	 1.67e-01	
-2.50e-01	 2.76e-01	 2.24e-01	-2.50e-01	
-2.50e-01	 5.00e-01	 0.00e+00	-2.50e-01
    #+end_src

    The expected output is drawn from a run of an early version of the program, it looked consistant and logical.

Tangling : 
#+srcname: TT_Test1_code_make
#+begin_src makefile
test/TT_PPi1.mat: TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
test/TT_PPi2.mat: TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

test/TT_PEast.mat:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
test/TT_PWest.mat:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
test/TT_PNorth.mat:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
test/TT_PSouth.mat:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

test/TT_expectedOutT1.mat:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
#+end_src
Cleaning :
#+srcname: TT_Test1_clean_make
#+begin_src makefile
TT_Test1_clean:
	find test -maxdepth 1 -iname "TT_PPi1.mat"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PPi2.mat" | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PPEast.mat" | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PPWest.mat" | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PPNorth.mat" | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PPSouth.mat" | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_expectedOutT1.mat" | xargs $(XARGS_OPT) rm
#+end_src

*** Dynamic pogamming on a 2x2 Gridworld
    We want to test our dynamic programming functions. We use help from the files defined in the [[TaskTransfer on a 2x2 Gridworld]] test.

    We first run the optimization with the action in a certain order,
    #+begin_src python :tangle TT_Test2.py
from numpy import *
import scipy
from DP import *
from a2str import *

Actions = []

for file in ['test/TT_PNorth.mat','test/TT_PEast.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( DP( Reward, Actions ) )
    #+end_src
    And then in another order,
    #+begin_src python :tangle TT_Test3.py
from numpy import *
import scipy
from DP import *
from a2str import *

Actions = []

for file in ['test/TT_PEast.mat','test/TT_PNorth.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( DP( Reward, Actions ) )
    #+end_src
    This should retrieve the policies of each of our experts, as changing the order of the actions change the default action when state-action values are the same.

    We add the first test to the Makefile
    #+srcname: TT_Test2_make
    #+begin_src makefile
TT_test2: TT_Test2.py test/TT_PPi1.mat a2str.py
	python TT_Test2.py > test/TT_outT2.mat
	../Utils/matrix_diff.py test/TT_PPi1.mat test/TT_outT2.mat
	rm test/TT_outT2.mat
    #+end_src
    
    And the second also,
    #+srcname: TT_Test3_make
    #+begin_src makefile
TT_test3:TT_Test3.py test/TT_PPi2.mat a2str.py
	python TT_Test3.py > test/TT_outT3.mat
	../Utils/matrix_diff.py test/TT_PPi2.mat test/TT_outT3.mat
	rm test/TT_outT3.mat
    #+end_src

    The rules to tangle them:
  #+srcname: TT_Test23_code_make
  #+begin_src makefile
TT_Test2.py:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
TT_Test3.py:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
  #+end_src
    And to clean them:
  #+srcname: TT_Test23_clean_make
  #+begin_src makefile
TT_Test23_clean:
	find . -maxdepth 1 -iname "TT_Test2.py"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "TT_Test3.py"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PPi1.mat"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PPi2.mat"   | xargs $(XARGS_OPT) rm
  #+end_src

** Makefile rules
  Tangling : 
  #+srcname: TT_code_make
  #+begin_src makefile
DP.py:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

TaskTransfer.py:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

TaskTransfer_SF.py:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

Constraint.py:TaskTransfer.org
	$(call tangle,"TaskTransfer.org")

TT.py :TaskTransfer.org
	$(call tangle,"TaskTransfer.org")
  #+end_src

  Cleaning :
  #+srcname: TT_clean_make
  #+begin_src makefile
TT_clean:
	find . -maxdepth 1 -iname "DP.py"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "TaskTransfer.py"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "TaskTransfer_SF.py"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "Constraint.py"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "TT.py"   | xargs $(XARGS_OPT) rm
	find . -maxdepth 1 -iname "TT.pyc"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PEast.mat"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PWest.mat"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PNorth.mat"   | xargs $(XARGS_OPT) rm
	find test -maxdepth 1 -iname "TT_PSouth.mat"   | xargs $(XARGS_OPT) rm
  #+end_src

* Perspective
** Open questions

   Closed questions are also kept here for history

   - About the agent's starting value :
     - What easy-to-compute criteria corresponds to a reward that induce high-value behaviors ?
     - Is the /Agent as expert/ value always attainable by maximizing one of the output reward ?\\
       The answer is no.
     - Under which conditions is the agent as expert value always attainable ?
   
   - About the true reward function :
     - What are the consequences of adding more constraints (for example stemming from different expert demonstration) ?
     - Is there such a thing as a lousy expert that hides information to the algorithm ?
     - Knowing the transition probabilities, is there a way to tell if two rewards are equivalent ?
     - Is there a setting where a reward exists so that no reward output by our algorithm is equivalent to it ?
     - Is the naive projection of the true reward to the manifold always present in the reward output by our algorithm ?
       The answer is no.

   - About the /all slacks/ method
     - Does the sparseness of the transition proabilities play a role in the rate of success of the /all slacks/ method ?
     - Under which conditions does the /all slacks/ method always yeld a result ?\\
       J'ai déjà bossé un peu de dessus, j'ai une CNs pas super utile, mais qui sait ?
     - Can automatic feature detection help in making the /all slacks/ method always succeed ?

** Perspective for this work

   The reduction of the space to be searched can be used virtually everywhere.\\

   Work is needed on the /all slacks/ method.\\

   \bibliographystyle{plain}
   \bibliography{../Biblio/Biblio.bib}
