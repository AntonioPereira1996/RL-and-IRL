#+TITLE: Task Transfer on the GridWorld
#+LATEX_HEADER: \usepackage{blkarray}
#+LATEX_HEADER: \usepackage{amsmath}

* Context
  

* Mapping behaviors and rewards

** Reward space

   A reward can be scaled without changing its corresponding optimal policies. 
   Therefore, the space of all rewards $R$ can be reduced to a hypersurface englobing the origin. For example, one can use the unit sphere or the unit cube.
   \\

   Adding the same value to all the component of a reward does not change the set of corresponding optimal policies. This is a translation of vector $\lambda [1,1,1,\dots,1]$.
   Therefore, the forementioned hypersurface can be reduced by half, cutting in the middle with the hyperplane perpendicular to the vector $[1,1,\dots,1]$.   
   \\

   The null reward is not interesting as it admits every policy as an optimal one.
   \\
   
   Having exhausted all the obvious reward shaping methods, we begin the analysis with a reward space reduced to an open and finite hypersurface that can be ununiformly scaled to the unit hemisphere whose wenter pilla is the vector $[1,1,\dots,1]$, with the $\lambda [1,1,\dots,1]$ point removed.
   \\
   
   One such hypersurface is for example the "top" of the simplex defined by le $\lambda$ point $[1,1,\dots,1]$ and the basepoints $[-(n-1),1,\dots,1]$, $[1,-(n-1),\dots,1]$, $\dots$, $[1,1,\dots,-(n-1)]$, where $n$ is the dimension of the reward space, i.e. the cardinal of $S$ (we estrict ourselves to the case where the reward is given only with respect to the state). Some distinguishing features of this particuliar example are :
   - it can be expressed as a set of linear constraints over $R$, namely $e_iR\leq 1$ with $e_i$ being the vector whose coordinates are null except for the $i$-th which is 1
   - every edge that contain the $\lambda$ point can be scanned by changing only one coordinate
   - every other edge can be scanned by changing only two coordinates
   
        
** Reward mapping

   In \cite{ng2000algorithms}, we are given a superb necessary and sufficient condition for a reward to be /compatible/ with a given policy, that is to say, for this given policy to be optimal with respect to the reward.
   \\

   For every action $a$ the agent had the possibility to take, the following matrix inequality must be met : 
   \begin{equation}
   \label{ng2000algorithms.eqn}
   (P_\pi-P_{a})(I-\gamma P_\pi)^{-1}R\succeq 0
   \end{equation}

   This is a Linear Programming problem. By adding to these constraints the linear constraints that define the forementionned starting hypersurface, we obtain a set of linear constraints that defines a closed, convex polytope, two remarkable vertices of which are the $\lambda$ point and the origin. The set of all facets that does not contain the origin describes the set of all compatible rewards (that is to say, every compatible reward is a scaling and a $\lambda Id$ translation away of a unique point of one of those facets).

** Reward selecting

   The set of all compatible rewards is defined. However, exploring it and searching in it is tricky. 
   \\
   
   I have the intuition that vertices of this polytopic set are interesting rewards. 
   \\
  
   To find the coordinates of the vertices, a simple solution is to use the tools of the Linear Programming framework (http://www2.isye.gatech.edu/~spyros/LP/LP.html).

   - Let $C$ be the matrix created by selecting the $m$ non null lines of the constraint matrix $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$.
   - Let $e_i$ be the $i$-th vector of the orthonormal base of the reward space, i.e. $e_i = [0,\dots,0,1,0,\dots,0]$, with the 1 at the $i$-th position.
   - Let $X$ be the unknown vector

     
   All the forementionned constraints can now be expressed as follow :
   \begin{eqnarray}
   &CX \succeq 0\\
    \forall i, &e_iX \preceq 1\\
   &X \in \mathbb{R}^n
   \end{eqnarray}

*** Standard form

   This is however not what is expected by the tools of the Linear Programmnig framework. The following operations aim at transforming this natural expression of the problem into the /standard form/ $AX=b, X\succeq 0$, typically used in the simplex algorithm (which is not fully applicable here because we don't have a cost function).
   \\

   First, we add a $-$ sign in front of the $C$ matrix so that both inequalities are the same way. The constraint can now be expressed as :
   \begin{eqnarray}
   &-CX \preceq 0\\
   \forall i, &e_iX \preceq 1\\
   &X \in \mathbb{R}^n
   \end{eqnarray}
   or more succintly as
   \begin{eqnarray}
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   -C \\
   \cline{1-1}
   \begin{block*}{c}
   e_1 \\
   \vdots \\
   e_n \\
   \end{block*}
   \end{block*}
   \end{blockarray} 
   X \preceq 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   0 \\
   \cline{1-1}
   \begin{block*}{c}
   1 \\
   \vdots \\
   1 \\
   \end{block*}
   \end{block*}
   \end{blockarray}\\
   X\in \mathbb{R}^n
   \end{eqnarray}
   
   The $X\in \mathbb{R}^n$ part is not satisfactory because in the standard form all unknowns must be grater than or equal to 0. To get to this form, for every component $x_i$ of $X$, we define $x_i^+\geq0$ and $x_i^-\geq0$ so that $x_i = x_i^+ - x_i^-$. We now define the $X'$ matrix as 
   \begin{equation}
   X'=\begin{pmatrix} x_1^+\\x_1^-\\ \dots \\ x_n^+\\x_n^- \end{pmatrix}
   \end{equation}
   Accordingly, the $C'$ matrix is defined as (using $c_i$ to denote the $i$-th column of C) :
   \begin{equation}
   C'=\begin{pmatrix} c_1 | c_1 | c_2 | c_2 | \dots |c_n|c_n \end{pmatrix}
   \end{equation}
   and the $e'_i$ matrices are defined as :
   \begin{equation}
   e'_i=\begin{pmatrix} 0,0,\dots 1,1, \dots 0,0 \end{pmatrix}
   \end{equation}
   with the $1$ being at indexes $2i-1$ and $2i$.
   \\
   
   We are now a bit closer to the standard form :
   \begin{eqnarray}
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   -C' \\
   \cline{1-1}
   \begin{block*}{c}
   e'_1 \\
   e'_2\\
   \vdots \\
   e'_n \\
   \end{block*}
   \end{block*}
   \end{blockarray} 
   X' \preceq 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   0 \\
   \cline{1-1}
   \begin{block*}{c}
   1 \\
   \vdots \\
   1 \\
   \end{block*}
   \end{block*}
   \end{blockarray}\\
   X'\succeq 0
   \end{eqnarray}
   
   The last thing to do is to introduce $n$ positive slack variables in the form of a matrix
   \begin{equation}
   S = \begin{pmatrix}s_1\\ \vdots\\ s_n\end{pmatrix} \succeq 0
   \end{equation}
   This allows us to change the inequality into an equality, thus finally obtaining something in the standard form :
   \begin{eqnarray}
   \begin{blockarray}{(cc)}
   \begin{block*}{c|c}
   -C'&  \\
   \cline{1-1}
   \begin{block*}{c|c}
   e'_1 & \\
   e'_2 & -Id\\
   \vdots \\
   e'_n &\\
   \end{block*}
   \end{block*}
   \end{blockarray} 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   X' \\
   \cline{1-1}
   \begin{block*}{c}
   S\\
   \end{block*}
   \end{block*}
   \end{blockarray}
   &= 
   \begin{blockarray}{(c)}
   \begin{block*}{c}
   0 \\
   \cline{1-1}
   \begin{block*}{c}
   1 \\
   \vdots \\
   1 \\
   \end{block*}
   \end{block*}
   \end{blockarray}\\
   X'&\succeq 0\\
   S &\succeq 0
   \end{eqnarray}

*** Basic feasible solutions
    In the linear programming framework, vertices of the polytope are often referred to as /basic feasible solutions/. To get these basic feasible solutions, one should set $3n-(2m+n)$ of the coordinates of the $\begin{blockarray}{(c)}\begin{block*}{c}X' \\\cline{1-1}\begin{block*}{c} S\\\end{block*}\end{block*}\end{blockarray}$ matrix to 0 and solve the system for the remaining variables. If the system admits a unique solution with all the variables being non negative, then such a solution is a basic feasible solution (see for example [[http://www2.isye.gatech.edu/~spyros/LP/node20.html]] to get an explanation).
\\

*** Back to the reward space
    Given a basic feasible solution (/bfs/) of the linear problem $\begin{blockarray}{(c)}\begin{block*}{c}X'^{bfs} \\\cline{1-1}\begin{block*}{c} S^{bfs}\\\end{block*}\end{block*}\end{blockarray}$, we must go back to the corresponding reward $R^{bfs}$ by undoing the transformation that allowed us to get to the standard form. This is done by :
    \begin{equation}
    R^{bfs}_i = X'^{bfs}_{2i-1} - X'^{bfs}_{2i} - S^{bfs}_i
    \end{equation}


    \bibliographystyle{plain}
    \bibliography{../Biblio/Biblio.bib}
* Code
** Tests and examples
*** Unit cube test
**** Principle
    Recall that the constraints that corresponds to the reduction of the reward space are of the form $\forall i, e_iX \preceq 1$. This is half the definition of the unit cube, the other half being the same equation with a minus sign in one of the sides.\\ 

    Also recall that the constraint matrix $C$ defines one hyperplane per line, each containing the origin.\\

    The idea behind this test is to feed the program a $C$ constraints matrix so that, when coupled with the constraints defining the reduced reward space, the polytope we get is a simply defined polytope. It must be easy to check wether the output is good or not.\\

    The hyperplanes parallel to those defined by $\forall i,&e_iX \preceq 1$ but englobing the origin (i.e. those defined by $\forall i, e_iX \succeq 0$) are good candidates. They define a region that is $1\over 2^{n}$ of the unit cube.\\
    
    For example in dimension 2, they define the intersection of the unit square and the positive-positive quandrant. In dimension 3 they define the eighth of the unit cube where all the components are positive.\\

    The coordinates of the vertices of these regions are easy to compute. We are going to match the output of the program fed with the $C$ matrix : $\forall i, e_iX \succeq 0$ with the manually computed vertices.
**** Implementation
     
     We are going to test in dimensions 2, 3 and 4.

     First let us define the constraints matrices :
     - in dimension 2 : 
       #+begin_src text :tangle test/C_d2.mat
1	0
0	1
       #+end_src
     - in dimension 3 : 
       #+begin_src text :tangle test/C_d3.mat
1	0	0
0	1	0
0	0	1
       #+end_src
     - in dimension 4 : 
       #+begin_src text :tangle test/C_d4.mat
1	0	0	0
0	1	0	0
0	0	1	0
0	0	0	1
       #+end_src

Then, let us write in some files the corresponding expected output, one vertex per line :
     - in dimension 2 : 
       #+begin_src text :tangle test/EO_d2.mat
0	0
0	1
1	0
1	1
       #+end_src
     - in dimension 3 : 
       #+begin_src text :tangle test/EO_d3.mat
0	0	0
0	0	1
0	1	0
0	1	1
1	0	0
1	0	1
1	1	0
1	1	1
       #+end_src
     - in dimension 4 : 
       #+begin_src text :tangle test/EO_d4.mat
0	0	0	0
0	0	0	1
0	0	1	0
0	0	1	1
0	1	0	0
0	1	0	1
0	1	1	0
0	1	1	1
1	0	0	0
1	0	0	1
1	0	1	0
1	0	1	1
1	1	0	0
1	1	0	1
1	1	1	0
1	1	1	1
       #+end_src


We now build Makefile targets that calls the program on the previously defined $C$ matrices and match the output with the expected output. Note the use of the \texttt{sort} command to make sure both output are in the same order.

#+src_name test_makefile1
#+begin_src makefile
#Ceci est le test 1
#+end_src

#+src_name test_makefile0
#+begin_src makefile
#Ceci doit être écrit avant
#+end_src
http://thread.gmane.org/gmane.emacs.orgmode/42636/focus=42960

