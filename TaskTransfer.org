#+TITLE: Task Transfer on the GridWorld
#+LATEX_HEADER: \usepackage{blkarray}
#+LATEX_HEADER: \usepackage{amsmath}
#+LATEX_HEADER: \usepackage{amsthm}
#+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \newtheorem{lemma}{Lemma}

* Context

  In the /Reinforcement Learning/ (RL) framework, an agent is left to find the behavior that maximizes a reward in the long run. By correctly defining the reward, one can use the RL framewrk to make an agent fulfil a certain task.\\

  Often, the RL problem is solved in a /Markov Decision Process/ setting : the agent is said to be in a state $s_t\in S$ which respect the Markovian criterion (it contains all the information the agent needs) where it has to choose an action $a\in A$ which will make it transit to another state $s_{t+1}$, receiving a reward $R(s_t)$. The optimal policy $\pi^*$ is a mapping $S\rightarrow A$ which maximizes the sum of the expected discounted reward in every state $V(s) = E\left[\left.\sum\limits_t \gamma^tR(s_t)\right|s_0=s,\pi\right]$.  This last term is called the /Value function/.\\

  The reward $R$ is here seen as a mapping from the state space $S$ to $\mathbb{R}$. Working in the discrete case, we use it as a vector $R\in \mathbb{R}^n$ indexed by the state space the cardinality of which is $n$.\\

  The value function, when follwing policy $\pi$ under reward $R$ is also defined as a vector : $V^\pi_R\in \mathbb{R}^n$ indexed by the state space.\\

  Actions are defined by the probability with which they make the agent transit from one state to the other. Formally, to each action $a$ is associated a $n\times n$ matrix $P_a$ whose element $(i,j)$ is the probability to transit to state $j$ when taking action $a$ in state $i$. A policy $\pi, S\leftarrow A : s \mapsto \pi(s)$ can be defined the same way, that is to say by associating it with a $P_\pi$ matrix whose line $i$ is the line $i$ of the $(P_a, a=\pi(s))$ matrix.\\

  To be considered optimal, a policy $\pi^*$ must then meet the criterion $\forall s, \pi^*(s) = \arg\max\limits_a\left(R(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_R\right)$.\\

  Sometimes, defining the reward associated with a task can be difficult. A proposed workaround is the /Inverse Reinforcement Learning/ (IRL) method, where one tries to learn the reward optimized by an expert. More formally, given an optimal policy $P_{\pi^*}$ and the set of actions $\{P_a\}_a$ from which the policy was drawn, on tries to guess the reward with respect to which this policy is optimal.\\

  This knowledge is useful for example for /Task Transfer/, where an agent whose abilities may differ from the expert's optimizes the same reward as the expert, in its own way.\\

  The IRL problem is an ill-posed one, as already noted in the litterature. [Insérer état de l'art]. This paper stems from a result published in \cite{ng2000algorithms}, where the set of rewards for which the expert's behavior is optimal was caracterized by a necessary and sufficient condition expressable as a Linear Programming (LP) problem. The contribution of this paper is to further restrict this set (in fact the space in which to look for rewards is now two dimension smaller than before) and to express the problem in a way that allow for on one hand a compact and exhaustive description of the set of all possible solutions and on the other hand for the formalization and rationalization of the heuristics introduced by others.

* New notions

  We denote the set of the policies optimal with respect to a certain reward $R$ as $\Pi^*(R)$.\\

  Throughout this paper, we will use the transitive notion of equivalence between rewards : 
  #+begin_definition
  Two rewards $R_1$ and $R_2$ are said to be /equivalent/ if $\Pi^*(R_1)$ = $\Pi^*(R_2)$
  #+end_definition
  
  A reward is said to be degenerative when it is equivalent to the null vector. Indeed the null vector admits any policy as an optimal policy.
  
* Reducing the reward space
   In this subsection, we will show that there exists a manifold of dimension $n-2$ so that every non degenerative reward is equivalent to at least one element of the manifold.\\

#+begin_lemma
\label{alpha.lemma}
Let $R_1$ be a reward vector, \\
Let $R_2$ be a reward vector so that $\exists \alpha > 0, R_2=\alpha R_1$ \\
The following holds:  $R_1\equiv R_2$
#+end_lemma
   
   #+begin_proof
   Let $\pi \in \Pi^*(R_2)$ be.\\
   We have : 
   \begin{eqnarray*}
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_{R_2}\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_2(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(\alpha R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^t\alpha R_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\alpha\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)
   \end{eqnarray*}
   as $\alpha >0$, this is the same as :
   \begin{equation*}
   \forall s, \pi^*(s) = \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \end{equation*}
   which means that $\pi \in \Pi^*(R_1)$.\\
   The inverse path can be demonstrated by replacing $R_1$ by $R_2$ and using $1\over \alpha$, therefore $\pi \in \Pi^*(R_2) \Leftrightarrow \pi \in \Pi^*(R_1)$.
   #+end_proof
   
#+begin_lemma
\label{lambda.lemma}
Let $R_1$ be a reward vector, \\
Let $\mathbf{1}$ be the column vector whose $n$ elements are all equal to $1$,\\
Let $R_2$ be a reward vector so that $\exists \lambda \in \mathbb{R}, R_2= R_1 + \lambda\mathbf{1}$,\\
The following holds:  $R_1\equiv R_2$
#+end_lemma
  
   #+begin_proof
   Let $\pi \in \Pi^*(R_2)$ be.\\
   We have : 
   \begin{eqnarray*}
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma P_{\pi^*}(s)V^{\pi^*}_{R_2}\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_2(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_2(s_t)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^t\left(R_1(s_t)+\lambda\right)\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]+ \gamma E\left[\left.\sum\limits_t\gamma^t\lambda\right|s_0=s,\pi\right]\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \lambda + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]+ \gamma\sum\limits_t\gamma^t\lambda\right)\\
   \forall s, \pi^*(s) &=& \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right] + \lambda + \gamma\sum\limits_t\gamma^t\lambda\right)\\
   \end{eqnarray*}
   as $\lambda + \gamma\sum\limits_t\gamma^t\lambda$ does not depend on $a$, this is the same as :
   \begin{equation*}
   \forall s, \pi^*(s) = \arg\max\limits_a\left(R_1(s) + \gamma E\left[\left.\sum\limits_t \gamma^tR_1(s_t)\right|s_0=s,\pi\right]\right)\\
   \end{equation*}
   which means that $\pi \in \Pi^*(R_1)$.\\
   The inverse path can be demonstrated by replacing $R_1$ by $R_2$ and using $-\lambda$, therefore $\pi \in \Pi^*(R_2) \Leftrightarrow \pi \in \Pi^*(R_1)$.
   #+end_proof

   #+begin_theorem
   Let $M$ be a closed $n-2$-dimensional manifold so that :\\
   $\forall R \in M, \mathbf{1}^TR\geq 0$,\\
   The origin is inside the area delimited by the manifold,\\
   The following holds : $\forall R \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}, \exists R'\in M, R'\equiv R$.
   #+end_theorem

   An exemple of such a manifold is the intersection of the $||.||_1$ unit sphere and the hyperplane perpendicular to $\mathbf{1}$.\\

   #+begin_proof
   Let $R \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}$ be,\\
   Let $\lambda = -{\mathbf{1}^TR\over n}$ be,\\
   According to lemma \ref{lambda.lemma}, we have $R + \lambda\mathbf{1} \equiv R$.\\
   We also have :
   \begin{eqnarray}
   \mathbf{1}^T(R+\lambda\mathbf{1}) &=& \mathbf{1}^TR + \lambda\mathbf{1}^T\mathbf{1}\\
   \mathbf{1}^T(R+\lambda\mathbf{1}) &=& \mathbf{1}^TR + \lambda n\\
   \mathbf{1}^T(R+\lambda\mathbf{1}) &=& \mathbf{1}^TR + -{\mathbf{1}^TR\over n}n)\\
   \mathbf{1}^T(R+\lambda\mathbf{1}) &=& 0
   \end{eqnarray}
   
   Now, whatever the shape of the manifold on the hyperplane perpendicular to $\mathbf{1}$, as long as the origin is inside drawing a line from the origin to $R+\lambda\mathbf{1}$ will cross the manifold at some point, which means that there exists an $\alpha$ strictly positive so that $\alpha(R+\lambda\mathbf{1})$ is a point of the manifold (for example, if the manifold was, as suggested, the intersection of the $||.||_1$ unit sphere and the hyperplane perpendicular to $\mathbf{1}$, we would have $\alpha ={1\over ||R+\lambda\mathbf{1}||_1}$). According to lemma \ref{alpha.lemma}, we have $\alpha(R+\lambda\mathbf{1}) \equiv R+\lambda\mathbf{1}$ which by transitivity yelds : $\alpha(R+\lambda\mathbf{1}) \equiv R$.
   #+end_proof

* Finding interesting rewards
   In \cite{ng2000algorithms}, we are given a necessary and sufficient condition for a reward $R$ to admit a given policy as optimal : for every action $a$ the expert $\pi$ had the possibility to take, the following matrix inequality must be met : 
   \begin{equation}
   \label{ng2000algorithms.eqn}
   (P_\pi-P_{a})(I-\gamma P_\pi)^{-1}R\succeq 0
   \end{equation}

   This is a Linear Programming problem. By adding the supplemantary constraints that $\mathbf{1}^TR=0$ and $||R||_1=1$, we restrict the solutions to the previously defined $n-2$-dimensional manifold.\\

   Drawing ideas from the simplex algorithm of the LP framework (http://www2.isye.gatech.edu/~spyros/LP/LP.html), we propose a compact and exhaustive description of the solutions of this augmented LP problem.\\

   Let $C$ b the matrix created by slecting the $m$ non null lines of the constraints matrices $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$, so that all lines are normalized (this does not change the constraints) and every line is unique. In order to improve the computational complexity of the algorithm, it may be possible to further restrict waht goes in the $C$ matrix, but this is a little off-topic and not necessary.\\

   Let $X$ be the unknown vector.\\
   
   The $i$-th line of the $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix is a constraint on $X$ that, if satisfied, will make it preferable to choose action $\pi(i)$ over $a$ in state $i$ (see \cite{ng2000algorithms}'s proof of Theorem 3 for why).
   
   All the forementionned constraints can now be expressed as follow :
   \begin{eqnarray}
   &CX \succeq 0\\
   &\mathbf{1}^TR=0\\
   &X \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}\\
   &||X||_1=1
   \end{eqnarray}
** Standard form
   
   These constraints will now be put in the /standard form/ $AX=b, X\succeq 0$, typically used in the simplex algorithm (which is not fully applicable here because we don't have a cost function).\\
  
  The $X\in \mathbb{R}^n$ part is not satisfactory because in the standard form all unknowns must be grater than or equal to 0. To get to this form, for every component $x_i$ of $X$, we define $x_i^+\geq0$ and $x_i^-\geq0$ so that $x_i = x_i^+ - x_i^-$. We now define the $X'$ matrix as 
  \begin{equation}
  X'=\begin{pmatrix} x_1^+\\x_1^-\\ \dots \\ x_n^+\\x_n^- \end{pmatrix}
  \end{equation}
  Accordingly, the $C'$ matrix is defined as (using $c_i$ to denote the $i$-th column of C) :
  \begin{equation}
  C'=\begin{pmatrix} c_1 | -c_1 | c_2 | -c_2 | \dots |c_n|-c_n \end{pmatrix}
  \end{equation}
  and the $\mathbf{1}' \in \mathbb{R}^{2n}$ vectoris defined as :
  \begin{equation}
  \mathbf{1}'=\begin{pmatrix} 1,-1, 1, -1\dots 1,-1\end{pmatrix}^T
  \end{equation}

  We are now a bit closer to the standard form :
  \begin{eqnarray}
  &C'X' \succeq 0 \\
  &\mathbf{1}'^TX'=0\\
  &X'\succeq 0\\
  &X \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}\\
  &||X||_1=1
  \end{eqnarray}
  
  The last thing to do is to introduce $m$ positive slack variables in the form of a matrix
  \begin{equation}
  S = \begin{pmatrix}s_1\\ \vdots\\ s_{m}\end{pmatrix} \succeq 0
  \end{equation}
  This allows us to change the inequality into an equality : if $a\geq b$, then $\exists s \geq 0, a-s = b$. We finally obtain something in the standard form, the last two lines of which respectively represents the constraints $\mathbf{1}'^TX'=0$ and $||X||_1=1$ :
  \begin{eqnarray}
  \label{LPStandardForm.eqn}
  \begin{blockarray}{(cc)}
  \begin{block*}{c|c}
  C'& -Id_m  \\
  \cline{1-2}
  \begin{block*}{c|c}
  \mathbf{1}'^T&0 \\
  \end{block*}
  \cline{1-2}
  \begin{block*}{c|c}
  \mathbf{1}^T\mathbf{1}^T&0 \\
  \end{block*}
  \end{block*}
  \end{blockarray} 
  \begin{blockarray}{(c)}
  \begin{block*}{c}
  X' \\
  \cline{1-1}
  \begin{block*}{c}
  S\\
  \end{block*}
  \end{block*}
  \end{blockarray}
  = 
  \begin{blockarray}{(c)}
  \begin{block*}{c}
  0 \\
  \vdots \\
  0 \\
  1\\
  \end{block*}
  \end{block*}
  \end{blockarray}\\
  \label{C1.eqn}
  X'\succeq 0\\
  \label{C2.eqn}
  S \succeq 0\\
  \label{C3.eqn}
  X \in \mathbb{R}^n\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}\\
  \end{eqnarray}

** Basic feasible solutions
   We now have $m+2$ equations in $2n+m$ variables. By selecting $m+2$ columns in equation \ref{LPStandardForm.eqn} and setting the unselected variables to 0, we obtain a system that may be solvable. Every solvable system that yelds a solution abiding by the additional constraints of equations \ref{C1.eqn}, \ref{C2.eqn}, and \ref{C3.eqn} is a reward with respect to which the expert's policy is optimal.\\

   A patient enumaration of all the $m+2 \times m+2$ systems will yeld a small set of interesting rewards (see section [[Experiments]]).\\

   If the computational comlpexity of the exhaustive enumaration is too high, one can either try to reduce it by divers means (see section [[Perspective]]), or use heuristics as virtually everybody else does.\\

   For example the intuitive heuristics according to which as much as possible, every action of the expert should be preferable over the others by a non null margin (i.e. the number of other optimal policies is limited as much as possible) will be yeld by solving the systems where all the columns of the stack variables are selected. Indeed, these slack variables indicates the amount by which the expert's expected value is greater than the other.\\

   This heuristics stems from the will to get as far away as possible from the degenerative rewards. This is not necessary here, and in fact the opposite heuristics, that is the one consisting in looking for the non degenerative reward that admits as many optimal policy as possible may prove interesting, as in a smooth MDP it may probably lead to scarce rewards.\\

   This is an improvement over the state of the art, where sometimes those two opposite heuristics are combined together !

** Back to the reward space
   Given a basic feasible solution (/bfs/) of the linear problem, we must go back to the corresponding reward $R^{bfs}$ by undoing the transformation that allowed us to get to the standard form. This is simply done by :
   \begin{equation}
   R^{bfs}_i = X'^{bfs}_{2i-1} - X'^{bfs}_{2i}
   \end{equation}

* Experiments

  /Agent as expert/ value : This is the value function of the agent evaluated at the starting state, given that the agent follows an optimal policy with respect to the /true reward function/. No behavior can induce a strictly greater true value than this one.

** Open questions
   What easy-to-compute criteria corresponds to a reward that induce high-value behaviors ?
   
   
   Assuming an infinite supply of experts, will the true reward function be retrieved ?
   
   
   Is the /Agent as expert/ value always attainable by maximizing one of the output reward ?
   
   
   Is there such a thing as a lousy expert that hides information to the algorithm ?


   Is there a setting where a reward exists so that no reward output by our algorithm is equivalent to it ?

   
   Is the naive projection of the true reward to the manifold always present in the reward output by our algorithm ?
** Random rewards and random experts
*** Goal
   The goal of this experiment is to empirically test the hypothesis according to which there always is a reward output by the algorithm so that if the agent maximizes this reward its true value is equal to the true value when it maximizes the true reward function. 
   
   This hypothesis can be reformulated by saying that /the Agent as Expert value is always attainable by maximizing one of the output rewards/. 
*** Protocol
    - Do :
      - Define a random reward $\mathbf{R}$ of size $n$
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
      - Compute the expert's policy by solving the MDP : $\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
      - Compute the Agent as Expert policy by solving the MDP for the true reward function : $\pi \leftarrow DP( R, \{P_{a^A_i}\}_i)$
      - Store the Agent as Expert value $V^{AaE}(s_0)$
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
      - Plot the agent's true values along with the expert's and the Agent as Expert value
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
*** Code
    We rewrite the protocol, adding the corresponding code at each line :
    - Do :
      #+srcname: TT_Exp1_py
      #+begin_src python
iterations = 0
while True:
    iterations+=1
      #+end_src
      - Define a random reward $\mathbf{R}$ of size $n$
	#+srcname: TT_Exp1_py
        #+begin_src python
    R = scipy.rand( n )
        #+end_src
      - Create $m_E$ random matrices $P_{a^E_i}$ corresponding to $m_E$ different actions the expert can choose among
	#+srcname: TT_Exp1_py
        #+begin_src python
    ExpertsActions = []
    for i in range(0,m_E):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        ExpertsActions.append(P_i)
        #+end_src
      - Compute the expert's policy's transition probabilities by solving the MDP : $P_\pi \leftarrow DP( R, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    P_pi = TT_DP( R, ExpertsActions )
        #+end_src
      - Run the TaskTransfer Algorithm : $\{R_j\}_j = TaskTransfer( P_\pi, \{P_{a^E_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    ttRewards = TT( P_pi, ExpertsActions )
        #+end_src
      - Create $m_A$ random matrices $P_{a_k^A}$ corresponding to $m_A$ different actions the agent can choose among
	#+srcname: TT_Exp1_py
        #+begin_src python
    AgentsActions = []
    for i in range(0,m_A):
        P_i = scipy.rand(n,n)
        for line in P_i:
            line /= sum(line) #Sum of proba = 1, so we normalize the random line
        AgentsActions.append(P_i)

        #+end_src
      - Compute the Agent as Expert policy's transition probabilities by solving the MDP for the true reward function : $P_\pi^{AaE} \leftarrow DP( R, \{P_{a^A_i}\}_i)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    P_AaE = TT_DP( R, AgentsActions )
        #+end_src
      - Store the Agent as Expert value $V^{AaE}(s_0)$
	#+srcname: TT_Exp1_py
        #+begin_src python
    V_AaE = dot(P_AaE[0],R.transpose()) #0 is the initial state
        #+end_src
      - For every reward vector $R_j$ output by the TaskTransfer Algorithm,
	#+srcname: TT_Exp1_py
        #+begin_src python
    AgentsValues = []
    for reward in ttRewards:
        #+end_src
	- Solve the MDP : $\pi_l \leftarrow DP(R^j, \{P_{a^A_k}\}_k )$
	  #+srcname: TT_Exp1_py
          #+begin_src python
        P_pi_l = TT_DP( reward, AgentsActions )
          #+end_src
	- Store the true value of the agent's behavior : $V\leftarrow V \cup \{V^{\pi_l}(s_0)\}$
	  #+srcname: TT_Exp1_py
          #+begin_src python
        AgentsValues.append( dot( P_pi_l[0], R.transpose() ))
          #+end_src
      - Plot the agent's true values along with the expert's and the Agent as Expert value
	#+srcname: TT_Exp1_py
        #+begin_src python
    print l2str( AgentsValues )
    print "Expert : %f" % dot(P_pi[0], R.transpose())
    print "Agent as Expert : %f" % V_AaE
        #+end_src
    - While $\max\limits_lV^{\pi_l}(s_0) = V^{AaE}(s_0)$ or the maximum number of iteration is attained
      #+srcname: TT_Exp1_py
      #+begin_src python
    if( max( AgentsValues ) < V_AaE or iterations >= 1000 ):
        print "Conditions de quittage"
        print "iterations : %d" % iterations
        print "Récompense : "
        print l2str(R)
        print "Actions de l'expert :"
        for action in ExpertsActions:
            print a2str(action)
            print ""
        print "Actions de l'agent"
        for action in AgentsActions:
            print a2str(action)
            print ""
        break
      #+end_src


    We glue it :
    #+begin_src python :tangle Exp1.py :noweb yes
# -*- coding: utf8 -*-
from numpy import *
import scipy
from TT_DP import *
from a2str import *
from TT import *

n = 4

m_E = 3
m_A = 2

<<TT_Exp1_py>>
    #+end_src
* Code
** Main code
*** Helper code
    We need the matrices to be output so that two equal matrices are output the same way.

  #+begin_src python :tangle a2str.py
#Do not indent
def l2str(l):
	"""Return the unique string representing line l"""
	answer = ""
	for x in l:
		if (abs(x)<1e-10): #FIXME : this is not right.
			answer += " 0.00e+00\t"
		elif (x>0):
			answer += " %1.2e\t"%x
		else:
			answer += "%+1.2e\t"%x
	answer +="\n"
	return answer
		
        
def a2str(a):
	"""Return the unique string representing array a"""
	answer = ""
	for l in a:
		answer += l2str( l )
	return answer


  #+end_src

    We output values near 0 as 0. Instead we probably should understand why there are so much near-0 values in the normal output. I think it has to do with the linear system solver.
*** Dynamic programming
    We need a MDP solver. This quick and dity dynamic programming implementation will do the trick :
    #+begin_src python :tangle TT_DP.py
from numpy import *
import scipy
import pdb

g_vReward = []
g_vActions = []
g_vV = []
g_vPi = []
g_fGamma = 0.9

def TT_Q( s, a ):
    return g_vReward[s] + 0.9 * dot( (g_vActions[ a ])[s], g_vV.transpose() )

def TT_DP( Reward, Actions ):
    "Returns the probability matrix corresponding to the optimal policy with respect to the given reward and the given actions. Actions are given in the form of a probability matrix. Probability matrices are so that the $(i,j)$ element gives the probability of transitioning to state $j$ upon taking action $a$ in state $i$"
    #pdb.set_trace()
    global g_vReward
    global g_vActions
    global g_vV
    global g_vPi
    n = len( Reward )
    m = len( Actions )
    g_vReward = Reward
    g_vActions = Actions
    g_vPi = map( int, floor( scipy.rand( n )*m ) )
    g_vV = scipy.rand( n )
    #While things change,
    changed = True
    while changed:
        changed = 0
        #For each state
        for s in range(0,n):
            old_pi = g_vPi[ s ]
            old_V = g_vV[ s ]
            chosen_a = 0
            max_Q = TT_Q( s, chosen_a )
            #Select the action that maximizes Q
            for a in range(0,m):
                Q = TT_Q( s, a )
                if( Q > max_Q ):
                    max_Q = Q
                    chosen_a = a
            g_vPi[ s ] = chosen_a
            g_vV[ s ] = max_Q
            if( g_vPi[ s ] != old_pi or g_vV[ s ] != old_V ):
                changed = 1
    #Construct the pobability matrix from the policy
    Ppi = zeros((n,n))
    for s in range(0,n):
        Ppi[s] = (g_vActions[ g_vPi[s] ])[s]
    return Ppi

    #+end_src
*** Finding the BFS
   This code finds the coordinates of the vertices of the polytope, also known as the /basic feasible solutions/.\\

   We create the $A$ and $b$ matrices of the standard form

#+srcname:TT_linesCreation_py
#+begin_src python
A = zeros((g_iM + 2, 2*g_iN + g_iM))
#C'
for i in range(0,g_iN):
    A[0:g_iM,2*i] = g_mC[:,i]
    A[0:g_iM,2*i+1] = -g_mC[:,i]
#\mathbf{1}'
for i in range(0,g_iN):
    A[g_iM,2*i] = 1
    A[g_iM,2*i+1] = -1
#\mathbf{1}\mathbf{1}
A[g_iM+1,0:2*g_iN] = ones((1,2*g_iN))
#-Id_(m)
A[:g_iM,2*g_iN:] = -identity(g_iM)

b = zeros((g_iM+2,1))
b[g_iM+1] = 1
#print "A and b matrices"
#print a2str(A)
#print a2str(b)
#+end_src

   For every $m+2$ combination of columns, we solve the resulting linear system, go back from that solution to the basic feasible solution and store it in a set 
#+srcname:TT_linearSystem_py
#+begin_src python
BFS = Set()
index = 0
standard_sols=[]
for lslice in itertools.combinations(range(0,2*g_iN+g_iM),g_iM+2):
    #sys.stderr.write("Combinaison No %d\n" % index)
    #print "Combinaison No %d" % index
    index+=1
    #print lslice
    #print "Subsystem"
    #print A[:,lslice]
    if( abs(linalg.det( A[:,lslice] ) ) > 0.00001 ):#Ugly hack for floating point precision
        partialStandardSol = linalg.solve(A[:,lslice],b)
        if( all( partialStandardSol > -0.00000001 ) ): #Ugly hack for floating point precision
            standardSol =  zeros((2*g_iN+g_iM,1))
            standardSol[lslice,:] = partialStandardSol
            #print "Standard solution exists : "
            #print standardSol
            R = zeros((g_iN,1))
            for i in range(0,g_iN):
                R[i] = standardSol[2*i] - standardSol[2*i+1]
            #print "Corresponding Reward : "
            #print linalg.det( A[:,lslice])
            #print R.transpose()
            #if( any(dot( g_mC, R )<-0.00001) ):
                #print "Ne respecte pas les contraintes"
                #print g_mC
                #print R
                #print dot( g_mC, R )
            if any( R != 0 ):
                BFS.add(l2str((R.transpose())[0]))
        #else:
            #print "Negative"
            #print partialStandardSol
    #else:
        #print "No solutions"
        #print linalg.det( A[:,lslice] )
#print "Nb Combinaisons %d" % index

#+end_src

   We glue it together

#+begin_src python :noweb yes :tangle TaskTransfer.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
import itertools
from sets import *
from a2str import *

#import pdb

g_mC = genfromtxt(sys.argv[1])
g_iN = g_mC.shape[1]
g_iM = g_mC.shape[0]

#print "C matrix : %d equations in %d variables"% (g_iM,g_iN)

<<TT_linesCreation_py>>

#print A
<<TT_linearSystem_py>>

#print "All solutions are : "
toPrint = ""
for R in BFS:
    toPrint+=R
print toPrint
#+end_src

*** Computing the $C$ constraints matrix
    Given a $P_\pi$ matrix and several $P_a$ matrices, this code compute the $C$ constraint matrix consisting of the non null, non repeating lines of $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$, plus the $[1, \dots, 1]$ vector. FIXME : why ?\\

    We add the non null lines of every $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix to a set
#+srcname:TT_PpiMinusPaEtc_py
#+begin_src python
g_sC = Set()
for Pa in g_lActionMatrices:
    A = dot((g_mPpi - Pa),linalg.inv( identity(g_iN)-(g_fGamma*g_mPpi) ))
    for line in A:
        if( any( line != zeros((1,g_iN)) ) ):
               g_sC.add( l2str( line/linalg.norm(line) ))

#+end_src
    
    We glue this.
#+begin_src python :noweb yes :tangle Constraint.py
#!/usr/bin/python
import sys
from numpy import *
import scipy
from sets import *
from a2str import *
  
#import pdb
g_mPpi = genfromtxt( sys.argv[1] )
g_lActionMatrices = []
for i in range(2,len(sys.argv)):
    g_lActionMatrices.append( genfromtxt(sys.argv[i]) )
g_iN = g_mPpi.shape[0]
g_fGamma = 0.9

<<TT_PpiMinusPaEtc_py>>

toPrint = ""
for line in g_sC:
    toPrint+=line
toPrint+= l2str( ones( g_iN ) )
print toPrint
#+end_src
    
*** Glue function
    A python function that glue both the constraints computation and the TaskTransfer algorithm is provided :
    #+begin_src python :tangle TT.py
import os
from numpy import *
import scipy

def TT( P_pi, Actions ):
    index = 0
    savetxt( "TT_tmp_%d"%index, P_pi, "%e", "\t" )
    index +=1
    for action in Actions:
        savetxt( "TT_tmp_%d"%index, action, "%e", "\t" )
        index +=1

    cmd = "python Constraint.py "
    for i in range(0,index):
        cmd+="TT_tmp_%d "%i
    cmd += " > TT_tmpC.mat"
    os.system( cmd )

    cmd = "python TaskTransfer.py TT_tmpC.mat > TT_tmpR.mat"
    os.system( cmd )

    answer = genfromtxt( "TT_tmpR.mat" )
    return answer
    #+end_src
** Tests
*** Test in 3D
    We test the program in a small setting o that the eward vector only has three component $x$, $y$ and $z$.

    
     First let us define the following constraints matrices :
     - This one means that we must have $x\geq y$ and $y \geq z$ 
       #+begin_src text :tangle test/TT_CT11.mat
1	-1	0
0	1	-1
       #+end_src
     - this one is the same, with an added last line that explicitely specify that $x \geq z$. The last line does not change the meaning of the contraints, but we add it to see if the program works even when fed with a useless constraint
       #+begin_src text :tangle test/TT_CT12.mat
1	-1	0
0	1	-1
1	0	-1
       #+end_src

       
     There are only three kind solutions satisfying these constraints (apart from the degenerative solution $x=y=z$) :
     - $x>y>z$
     - $x>y=z$
     - $x=y>z$
     
       
     As the $L_1$ norm of the anwers must be $1$, the expected output for both input is :
     #+begin_src text :tangle test/TT_expectedOutT1.mat
 2.50e-01	 2.50e-01	-5.00e-01	
 5.00e-01	 0.00e+00	-5.00e-01	
 5.00e-01	-2.50e-01	-2.50e-01	
     #+end_src

We now build Makefile targets that calls the program on the previously defined $C$ matrices and match the output with the expected output. Note the use of the \texttt{sort} command to make sure both output are in the same order and the diff command succeeds.

#+srcname: TT_test0_make
#+begin_src makefile
TT_test0: TaskTransfer.py
	python TaskTransfer.py test/TT_CT11.mat | sort > test/TT_outT11.mat
	python TaskTransfer.py test/TT_CT12.mat | sort > test/TT_outT12.mat
	../Utils/matrix_diff.py test/TT_expectedOutT1.mat test/TT_outT11.mat
	../Utils/matrix_diff.py test/TT_expectedOutT1.mat test/TT_outT12.mat
#+end_src

#+srcname: TT_cleanTest0_make
#+begin_src makefile
TT_cleanTest0:
	rm test/TT_outT11.mat
	rm test/TT_outT12.mat
#+end_src
*** Task Transfer on a 2x2 Gridworld
    In this simple setting we imagine a 2x2 gridworld and two experts. Both experts optimize the same reward, located in the north east corner. Both experts can choose between the same actions at each step : the four compass directions. The first expert's policy is NORTH, EAST, the second one is EAST, NORTH. We want to see in this experiment if the true reward is among the set of reward output by our algorithm.

    The states are indexed fom 0 to 3, in the reading order.


    We begin by defining the two matrices $P_{\pi_1}$ and $P_{\pi_2}$ relative to both expert's policies :
    - $P_{\pi_1}$ is :
      #+begin_src text :tangle test/TT_PPi1.mat
0	1	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src
    - $P_{\pi_1}$ is :    
      #+begin_src text :tangle test/TT_PPi2.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	1	0	0
      #+end_src
      
    
    We then define the four $P_a$ matrices relative to each action :
    - $P_{NORTH}$ is :
      #+begin_src text :tangle test/TT_PNorth.mat
1	0	0	0
0	1	0	0
1	0	0	0
0	1	0	0
      #+end_src      
    - $P_{EAST}$ is :
      #+begin_src text :tangle test/TT_PEast.mat
0	1	0	0
0	1	0	0
0	0	0	1
0	0	0	1
      #+end_src
     - $P_{SOUTH}$ is :
      #+begin_src text :tangle test/TT_PSouth.mat
0	0	1	0
0	0	0	1
0	0	1	0
0	0	0	1
      #+end_src
     - $P_{WEST}$ is :
      #+begin_src text :tangle test/TT_PWest.mat
1	0	0	0
1	0	0	0
0	0	1	0
0	0	1	0
      #+end_src
       

    The constraint matrices relative each expert are computed :
    #+srcname: TT_test1_make
    #+begin_src makefile
TT_test1:
	python Constraint.py test/TT_PPi1.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C1.mat
	python Constraint.py test/TT_PPi2.mat test/TT_PEast.mat test/TT_PWest.mat test/TT_PSouth.mat test/TT_PNorth.mat > test/TT_C2.mat
    #+end_src

    The conjoint constraint matrix is extracted (duplicate lines are removed) :
    #+srcname: TT_test1_make
    #+begin_src makefile
	cat test/TT_C1.mat test/TT_C2.mat | sort | uniq > test/TT_CBoth.mat
    #+end_src

    The TaskTransfer program is run and its output is compared against what is expected :
    #+srcname: TT_test1_make
    #+begin_src makefile
	python TaskTransfer.py test/TT_CBoth.mat > test/TT_outT1.mat
	../Utils/matrix_diff.py test/TT_expectedOutT1.mat test/TT_outT1.mat
    #+end_src

    The expected output is drawn from a run of an early version of the program, it looked consistant and logical.

    #+srcname: TT_cleanTest1_make
    #+begin_src makefile
TT_cleanTest1:
	rm test/TT_C1.mat
	rm test/TT_C2.mat
	rm test/TT_CBoth.mat
	rm test/TT_PPi1.mat
	rm test/TT_PPi2.mat
	rm test/TT_PNorth.mat
	rm test/TT_PSouth.mat
	rm test/TT_PWest.mat
	rm test/TT_PEast.mat
	rm test/TT_outT1.mat
    #+end_src
*** Dynamic pogamming on a 2x2 Gridworld
    We want to test our dynamic programming functions. We use help from the files defined in the [[TaskTransfer on a 2x2 Gridworld]] test.

    We first run the optimization with the action in a certain order,
    #+begin_src python :tangle TT_test2.py
from numpy import *
import scipy
from TT_DP import *
from a2str import *

Actions = []

for file in ['test/TT_PNorth.mat','test/TT_PEast.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( TT_DP( Reward, Actions ) )
    #+end_src
    And then in another order,
    #+begin_src python :tangle TT_test3.py
from numpy import *
import scipy
from TT_DP import *
from a2str import *

Actions = []

for file in ['test/TT_PEast.mat','test/TT_PNorth.mat','test/TT_PSouth.mat','test/TT_PWest.mat']:
    P_pi = genfromtxt( file )
    Actions.append( P_pi )

Reward = [0,1,0,0]

print a2str( TT_DP( Reward, Actions ) )
    #+end_src
    This should retrieve the policies of each of our experts, as changing the order of the actions change the default action when state-action values are the same.

    We add the first test to the Makefile
    #+srcname: TT_test2_make
    #+begin_src makefile
TT_test2:
	python TT_test2.py > test/TT_outT2.mat
	../Utils/matrix_diff.py test/TT_PPi1.mat test/TT_outT2.mat
    #+end_src
    
    And the second also,
    #+srcname: TT_test3_make
    #+begin_src makefile
TT_test3:
	python TT_test3.py > test/TT_outT3.mat
	../Utils/matrix_diff.py test/TT_PPi2.mat test/TT_outT3.mat
    #+end_src
* Perspective
  It should possible to reduce the computational complexity with easy tricks.\\

  I want to study how this generalizes to continuous MDPs and MDPs where the probabilites are unknown.\\

  Every other algorithm's heuristics should be expressable as a formal criterion on the systems to be solved. This couls lead to a generalization of the state of the art in IRL.\\

  Reducing the dimensionality of the search space should speed up every existing algorithm.



   \bibliographystyle{plain}
   \bibliography{../Biblio/Biblio.bib}
