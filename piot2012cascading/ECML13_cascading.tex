% Created 2012-10-15 lun. 14:49
\documentclass[smallextended]{svjour3}
\smartqed 
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage[integrals]{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{algorithm}
\usepackage{algorithmic}
\tolerance=1000
\usepackage{amsmath}
%\usepackage{amsthm}
%\newtheorem{definition}{Definition}
%\newtheorem{theorem}{Theorem}
\usepackage{dsfont}
\providecommand{\alert}[1]{\textbf{#1}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\argmax}{\operatorname*{argmax}} %\operatorname* pour les op. pouvant admettre des limites...
\begin{document}
\title{Learning a reward function from demonstrations: a cascaded supervised learning approach}
\author{Edouard Klein$^{1,2}$ \and Bilal Piot $^{2,3}$\and Matthieu Geist $^{2}$\and Olivier Pietquin$^{2,3}$}
\institute{
 1. ABC Team\\
 LORIA-CNRS, France.\\
\and
2.Supélec-Metz Campus\\
 MaLIS Research group, France\\
 \texttt{firstname.lastname@supelec.fr}\\
\and 
3. UMI 2958 CNRS\\
GeorgiaTech, France\\
}
\date{\today}


\maketitle

\begin{abstract}
  This paper considers the Inverse Reinforcement Learning (IRL) problem, that is inferring a reward function for which a demonstrated expert policy is optimal.
We propose to break the IRL problem down into two Supervised Learning (SL) steps: this is the cascaded approach. A classification step aiming to imitate the expert's policy gives a score function we see as an optimal action value function. In the second step, this score function is used together with the bellman evaluation equation to create samples then fed to a regressor that outputs a reward function.
This cascaded approach is justified by an analysis which shows that the demonstrated expert policy is near-optimal for the reward function the regression step outputs. We explain some advantages of this approach over existing algorithms, mainly not needing to repeatedly solve the Reinforcement Learning (RL) direct problem.
Finally, up to the use some heuristics, we empirically show the algorithm to work with only transitions sampled according to the demonstrated expert policy.
  \end{abstract}
\section{Introduction}
\label{sec-2}

Inverse Reinforcement Learning (IRL)~\cite{russell1998learning} aims at inferring a reward function for which a demonstrated expert policy is optimal. An important part of the literature (see Section~\ref{sec:related} for a brief review) tries to find a reward function such that the associated optimal policy induces a distribution over trajectories (or some measure of this distribution) which matches the one induced by the expert: this is called apprenticeship learning. Most algorithms of apprenticeship learning use Reinforcement
Learning (RL) algorithms as subroutines, thus their performance depends strongly on the quality of these subroutines. Consequently, apprenticeship learning algorithms suffer from the
same challenges of large state spaces, exploration vs. exploitation trade-offs as RL. In order to avoid to solve repeatedly RL problems, we decide to take a different point of view.

Having in mind that there is a one to one relation between a reward function and its associated optimal quality function (via the inverse Bellman equation), it is worth thinking of a method which is able to output a quality function for which the greedy policy is the demonstrated expert policy. Thus, the demonstrated expert policy will be optimal for the reward function associated to the quality function. As a method which is able to output a quality function for which the greedy policy is the demonstrated expert policy, we propose a score-based classification (see Section~\ref{sec:algo}). Besides, in order to retrieve, via the inverse Bellman equation, the reward associated to the score function the classification outputs, we propose a regression (see Section~\ref{sec:algo}). That is why the method is called the Cascaded Supervised Approach (CSA). This method is justified by a theoretical analysis (see Section~\ref{sec:analysis}) where it is shown that the demonstrated expert policy is near-optimal for the reward the regression step outputs.

Finally, an instantiation of CSA is proposed~\ref{sec:experiments}. This algorithm does not need to solve iteratively an RL problem and needs only sampled transitions from expert and non-experts policies as inputs. Moreover, up to the use of some heuristics, the algorithm is able to be trained only with transitions sampled from the demonstrated expert policy. The algorithm is tested on a car driving simulator and compared to an apprenticeship learning algorithm, to a pure classification algorithm and to a recent successfull IRL method.

\section{Background and notations}
\label{sec:background}
%% \begin{itemize}
%% \item Parler du domaine de la prise de décision séquentielle
%% \item Expliquer le problème direct, en profiter pour introduire l'équation d'évaluation de Bellman et l'équation d'optimalité de Bellman
%% \begin{itemize}
%% \item $RL(MDP=\{S,A,\gamma,(P),R\}) = \pi^*:S\times A \rightarrow \mathbb{R}$
%% \item $\forall \pi, Q^\pi(s,a) = R(s,a) + \sum_{s'}P(s'|s,a)Q^\pi(s',\pi(s'))$
%% \item $\pi^* = \arg\max_aQ^*(s,a)$
%% \end{itemize}
%% \item Introduire le problème inverse dans une forme mathématique relativement propre
%% \begin{itemize}
%% \item $IRL( MDP\backslash R = \{S,A,\gamma,(P)\} \cup \{\pi^E\} = R : S\times A \rightarrow \mathbb{R}$ telle que $\pi^E$ est optimale pour $R$
%% \item Mentionner la nature mal posée du problème sans s'appesentir dessus. Préciser toutefois qu'aucune approche ne le résoud de manière mathématiquement satisfaisante à cause de ça.
%% \item insister sur l'optimalité de l'expert comme hypothèse. Un expert sous optimal, c'est un autre champ de littérature, éventuellement donner deux trois refs.
%% \end{itemize}
%% \end{itemize}
First, we introduce some general notations.
Let $E$ and $F$ be two non-empty sets, $E^F$ is the set of functions from $F$ to $E$.
We note $\Delta_X$ the set of distributions over $X$.
Let $\alpha\in\mathbb{R}^X$ and $\beta\in\mathbb{R}^X$: $\alpha\leq\beta \Leftrightarrow \forall x\in X, \alpha(x) \leq \beta(x)$. We will often slightly abuse the notations and consider (where applicable) most objects as if they were matrices and vectors indexed by the set they operate upon.

We frame the sequential decision problem in a finite \emph{Markov Decision Process} (MDP), that is a tuple $\{S,A,P,R,\gamma\}$. $S$ is a finite state space, $A$ is a finite action space, $R\in\mathbb{R}^{S\times A}$ is a reward-function, $\gamma\in ]0,1[$ is a discount factor and $P\in \Delta_{S}^{S\times A}$ is the dynamic of the MDP. Thus for each $(s,a)\in S\times A$, $P(.|s,a)$ is a distribution over $S$ and $P(s'|s,a)$ is the probability to reach $s'$ by choosing the action $a$ in the state $s$. At each time step $t$, the agent uses the information encoded in the state $s_t\in S$ in order to choose an action $a_t \in A$ according to a (deterministic\footnote{We restrict ourselves here to deterministic policies, but the loss of generality is minimal as there exists at least one deterministic optimal policy.}) \emph{policy} $\pi\in A^S$. The agent then transits to a new state $s_{t+1}\in S$ according to the markovian transition probabilities $P(s_{t+1}|s_t,a_t)$. The reward function $R$ is a local measure of the quality of the control. The global quality of the control induced by a policy $\pi$, with respect to a reward $R$ is assessed by the value function $V^\pi_R \in \mathbb{R}^{S}$ which associates each state with the discounted cumulative reward for following policy $\pi$ from this state: $V^\pi_R(s) = \E[\sum_{t\geq 0}\gamma^tR(s_t)|s_0 = s]$. Therefore, an optimal policy $\pi^*_R$ is a policy whose value function (the optimal value function $V^*_R$) is greater than that of any other policy, for all states : $\forall \pi, \forall s \in S, V^*_R(s) \geq V^\pi_R(s)$. The stationary distribution induced by a policy $\pi$ is written $\rho_\pi,\textrm{ s.t. }\rho_\pi^TP^\pi = \rho_\pi$.

The Bellman evaluation operator $T^\pi_R: \mathbb{R}^{S} \rightarrow  \mathbb{R}^{S}$ is defined by $T^{\pi_R}V = R + \gamma P_\pi V$ where $P_\pi$ is $(p(s'|s,\pi(s)))_{s,s' \in S}$. The Bellman optimality operator follows naturally : $T^*_RV = \max_\pi T^\pi_RV$. Both operators are contractions. The fixed point of the Bellman evaluation operator $T^\pi_R$ is the value function of $\pi$ with respect to reward R: $V^\pi_R$; the fixed point of the Bellman optimality operator $T^*_R$ is the optimal value function $V_R^*$ with respect to reward $R$. Optimal value function and optimal policy are also linked via the quality function $Q^\pi_R\in\mathbb{R}^{S\times A}$, a state-action value function that adds a degree of freedom on the choice of the first action, formally defined by $Q^\pi_R(s,a) = T^a_RV^\pi_R(s)$, with $a$ the policy that always returns action $a$ ($T^aV = R + \gamma P_a V$ with $P_a = (p(s'|s,a))_{s,s' \in S}$). The optimal policy follows a greedy mechanism with respect to its optimal quality function 
\begin{equation}
  \label{eq:greedy}
  \pi^*_R(s)\in\argmax_aQ^*_R(s,a).
\end{equation}

When the state space is too large to allow matricial representation or when the transition probability or even the reward function are unknown except through observations gained by interacting with the system, RL or approximate dynamic programming may be used to retrieve the optimal control policy \cite{sutton1998reinforcement}. We call this the direct problem.

In this work, we wish to tackle the inverse problem. We observe an expert's deterministic\footnotemark[\value{footnote}] policy $\pi^E$, assuming that there exist some unknown reward $R^E$ for which the expert is optimal. The non optimality of the expert is an interesting setting that has been discussed for example in \cite{meloXXXanalysis} but that we are not addressing here. We do not try to find this unknown reward $R^E$ but rather a non trivial reward $R$ for which the expert is also near-optimal. The trivial reward $0$ is a solution to this ill-posed problem (no reward means that every behavior is optimal). Because of its ill-posed nature, this expression of \emph{Inverse Reinforcement Learning} (IRL) still has to find a satisfactory solution although lots of progress have been made, see section \ref{sec:related}.
\section{The cascading algorithm}
\label{sec:algo}
%% \begin{itemize}
%% \item Mentionner (sans en parler, ce sera fait dans \hyperref[sec-7-1]{Imitation}) l'imitation. Une approche supervisée évidente est la classification, avec pour entrée un set $D_C$ de données issues de l'expert. Rappeler (expliqué pour la première fois dans \hyperref[sec-3]{Formalisme} qu'on se restreint (sans perte de généralité, le préciser) au cas des politique déterministes.
%% \item Tous les classifieurs utilisent une fonction de score (je n'ai pas trouvé un seul exemple d'un classifieur qui n'en utilise pas), il s'agit juste d'un manière d'écrire les choses.
%% \item Expliquer que l'on identifie le mécanisme de classification au mécanisme glouton qui régit la politique optimale (donc celle de l'expert).
%% \item Sans chercher à justifier mathématiquement la chose (ce sera fait dans la partie \hyperref[sec-5]{Analyse}), expliquer qu'intuitivement l'on va identifier fonction de score du classifieur et fonction de qualité de l'expert (qui est optimale pour une certaine récompense, rappelons-le).
%% \begin{itemize}
%% \item Repartir de l'équation d'évaluation de Bellman pour isoler le terme $R(s,a)$ et introduire $r:S\times A\in \mathbb{R}$, la récompense issue de cette équation lorsque l'on remplace $Q^E$ par la fonction de score : $r(s,a) =q(s,a) - \sum_{\mathrm{s'}}P(s'|s,a)q(s',\pi(s')) $
%% \item On estime ce terme à partir des transitions $D_R$ car les probabilités de transitions sont inconnues : $\hat r(s,a) = q(s,a)-\gamma \sum_{s'} {1\over N_{s'}}\sum_{(s,a,s',a')\in D_R}q(s',a')$
%% \item La fonction de récompense est donnée par une régression sur $D_R \cup {\hat r}$.
%% \end{itemize}
%% \item Donner l'algorithme l'environnement \texttt{algorithm2e}
%% \item Rapidement, l'étape de régression permet d'ajouter la structure du MDP au résultat de classif, en utilisant l'équation de Bellman.
%% \item Bien préciser qu'il ne s'agit ici que d'une explication qualitative, ``avec les mains'', la preuve du fait que ça résoud bien le problème est donnée dans \hyperref[sec-5]{Analyse}, une démonstration sur des problèmes si complexes que l'état de l'art ne peut les résoudre (sauf SCIRL) est donnée dans \hyperref[Experience]{Experience}. Les avantages comparés sont expliqués dans \hyperref[Autres-Approches]{Autres Approches}. Enfin, des explications plus intuitives sur les mécanismes internes de l'algorithme, ainsi que des résultats expérimentaux supplémentaires sont fournis en \hyperref[sec-9]{Annexes}.
%% \end{itemize}
Our first step in the quest for a reward function solving the IRL problem is a classification step using a \emph{score function-based multi-class classifier} (MC$^2$). This classifier learn a score function $q:S\times A \rightarrow \mathbb{R}$ that rates the association of a certain label $a\in A$ with a certain input $s\in S$. The classification rule $\pi_C: S \rightarrow A$ is simply to select (one of) the label(s) that achieve the highest score for the given input :
\begin{equation}
  \label{eq:greedy2}
\pi_C(s) \in \argmax_a q(s,a).
\end{equation}
Most notably, \emph{Multi-class Support Vector Machines} can be seen as MC$^2$ algorithms. We give more examples and detail our choice of a MC$^2$ algorithms for the experiments in subsection \ref{sec:instantiation}. 

Given a dataset $D_C = \{(s_i,a_i=\pi^E(s_i))_i\}$ of actions $a_i$ (deterministically) chosen by the expert on states $s_i$, we train such a classifier. Apart from discussion and comparison purpose, we do not make use of the classification policy $\pi_C$. What is of interest to us is the score function $q$ itself. One can easily notice the similarity between equations \ref{eq:greedy2} and \ref{eq:greedy} that describes the relation between the optimal policy in a MDP and its optimal quality function. As the expert is optimal with respect to some unknown reward function $R_E$ and we seek a reward function $R$ for which it is also optimal, it is only natural to view the score function $q$ of the classifier as some kind of optimal quality function for the classifier policy $\pi_C$. By inversing the Bellman evaluation equation, one can express the reward as a function of the quality function : $R(s,a) = Q^*_R(s,a) - \sum_{s'}P(s'|s,a)Q^*_R(s'\pi^*(s'))$. This leave us with $r$, the reward function relative to our score/quality function $q$ : $r(s,a) =q(s,a) - \sum_{\mathrm{s'}}P(s'|s,a)q(s',\pi_C(s'))$.

We wish to solve the general, or approximate, IRL problem where the transition probabilities are unknown. For this we use another dataset $D_R = \{(s_j,a_j,s'_j)_j\}$ where $s'_j$ is the state an agent taking action $a_j$ in state $s_j$ transitioned to. Action $a_j$ need not be chosen by any particular policy. The dataset $D_R$ brings us information about the dynamics of the system. Our reward function $r$ must be approximated to $\hat r$ with the help of the information in $D_R$. This is done via datapoints $\{\hat r_j = q(s_j,a_j) - \gamma q(s'_j,\pi_C(s'_j))\}_j$ that we then feed to a regressor FIXME:citation 36/15 papier de munos to obtain $R^C$, a generalisation of $\hat r$ over the whole state-action space. The complete algorithm is given Alg. \ref{algo:cascading}.
\begin{algorithm}%[H]
    %\small
  \caption{Cascading IRL algorithm}
  \label{algo:cascading}
  \emph{\textbf{Given}} a training set $D_C=\{(s_i,a_i=\pi^E(s_i))\}_{1\leq i \leq D}$ and another training set $D_R=\{(s_{j},a_{j},s'_{j})\}_{1\leq j \leq D'}$\;\\
  \emph{\textbf{Train}} a score function-based classifier on $D_C$, obtaining decision rule $\pi_C$ and score function $q:S\times A \rightarrow \mathbb R$\;\\
  \emph{\textbf{Learn}} a reward function $\hat R^C$ from the dataset $\{(s_{j},a_{j},\hat{r}_j)\}_{1\leq j \leq D'}$, $\forall (s_j,a_j,s'_j) \in D_R,\hat{r}_j=q(s_{j},a_{j})-\gamma q(s'_{j},\pi_C(s'_{j}))$\;\\
  \emph{\textbf{Output}} the reward function $\hat R^{C}$ \;
\end{algorithm}

Cascading two supervised approaches like we do is a way to inject the MDP structure in the resolution of the problem. Indeed, mere classification only takes into account information from the expert (i.e., which actions goes with what state), but there is more to a MDP than a single policy. Using the Bellman equation in the expression of $\hat r$, we make use of the information lying in the transitions $(s,a,s')$, namely information about the transition probability $P$. The final regression step is just a way to try to generalize this information about $P$ to the whole state-action space, in order to have a well-behaved reward function.

As the analysis (Sec \ref{sec:analysis}) reveals, both classification error and regression error should be controlled. By carefully choosing or tuning both supervised algorithm, it should possible to mitigate any ill effects of the specific settings the cascading algorithm is used in. If one operates in a well-known domain where a lo of data is available, then ease-of-use will probably dictate the choice of two off-the-shelf implementations with default settings. If data is lacking, one could for example have to trade off between bias and variance. The possibility to use and leverage the wide range of techniques developped for these two well known supervised problems is a strong advantage of the proposed cascading approach.
%Reformuler ce passage
The explanation in this section only is a quatitative attempt to make the reader understand the idea behind the algorithm. The analysis of the next section tackle things in a formal, yet less intuitive, way. In Sec \ref{sec:experiments}, the use of heuristics (not tackled in the analysis) is empirically shown to be relevant. Data constraints are so high that to the best of our knowledge only one other published algorithm is able to solve the IRL problem in such conditions. Other approaches are compared with one another in Sec. \ref{sec:related}. Finally, appendices provide the reader with other experiments that exemplify the inner working of the algorithm.

\section{Analysis}
\label{sec:analysis}


%% \begin{itemize}
%% \item On rappelle le problème posé au début.
%% \item Le théorème 1 montre que modulo un bonne classif et un bon régresseur, on résoud exactement le problème que l'on s'était posé
%% \item On en fait la démo rapide (éventuellement des bouts peuvent être déportés en \st{Pologne} \hyperref[sec-9]{Annexes}).
%% \item On insiste sur le fait que la récompense trouvée n'est pas triviale, on a transcendé la nature mal posée du problème.
%% \item On dit un mot rapide sur la complexité en échantillon (ça dépend du classifieur et du régresseur, mais \hyperref[sec-6]{Expériences} montre qu'on est bon là dessus)
%% \item ainsi que sur la complexité temporelle (ça dépend toujours, mais avec une M-SVM et un moinde carré ça prend O(quelque chose) (indice: pas beaucoup)) FIXME:compléter la complexité
%% \end{itemize}

In this Section, we prove that the deterministic expert policy $\pi_E$ is near optimal for the reward $\hat{R}^C$ outputted by the regression step. We formulate this result in a more formal way in Theorem \ref{theorem: theorem analysis}

\begin{theorem}
\label{theorem: theorem analysis}
Let $\pi_E$ be the deterministic expert policy, $\rho_E$ its stationary distribution and $\hat{R}^C$ the reward outputted by the regression step. We have:
\begin{equation}
0\leq\E_{s\sim\rho_E}[V^*_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C}]\leq \frac{1}{1-\gamma}(\Delta q \epsilon_C+(1+C_*)\epsilon_R)).
\end{equation}
$\Delta q$, and $C_*$ are constants and are defined in the proof. $\epsilon_C$ represents the classification error and $\epsilon_R$ represents the regression error: both terms are well-defined in the proof.
\end{theorem}
\begin{proof}
First let introduce some notations. $q\in\mathbb{R}^{S\times A}$ is the score function outputted by the classification step, $\forall s \in S,\pi_C\in\argmax{a\in A}q(s,a)$ is a deterministic classifier policy and $\epsilon_C$ is the classification error:
\begin{equation}
\epsilon_C=\E_{s\sim\rho_E}[\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}}].
\end{equation}
Moreover we recall the definition of the reward $R^C\in\mathbb{R}^{S\times A}$:
\begin{equation}
\forall (s,a)\in S\times A, R^C(s,a)=q(s,a) +\gamma\sum_{s'\in S}P(s'|s,a)q(\pi_C(s'),a),
\end{equation}
and the definition of $\hat{R}^C\in\mathbb{R}^{S\times A}$ which is the reward function outputted by the regression step.
The difference between $R^C$ and $\hat{R}^C$ is noted $\epsilon^R=R^C-\hat{R}^C$, $\epsilon^R\in\mathbb{R}^{S\times A}$ must not be mistaken for the regression error $\epsilon_R\in\mathbb{R}$ defined below.
We also introduce the reward function $R^E\in\mathbb{R}^{S\times A}$ which will be useful in our proof:
\begin{equation}
\forall (s,a)\in S\times A, R^E(s,a)=q(s,a) +\gamma\sum_{s'\in S}P(s'|s,a)q(\pi_E(s'),a).
\end{equation}
Let $X\in\mathbb{R}^{S\times A}$, $\pi\in A^S$, and $a\in A$, we introduce $X_\pi\in\mathbb{R}^S$ and $X_a\in\mathbb{R}^S$ such that: $\forall s\in S, X_\pi(s)=X(s,\pi(s))$ and $\forall s\in S, X_a(s)=X(s,a)$.
Thus we have the following vectorial equations:
\begin{align}
&R^C_a=q_a-\gamma P_aq_{\pi_C},
\\
&R^E_a=q_a+\gamma P_aq_{\pi_E},
\\
&\epsilon^R_a=R^C_a-\hat{R}^C_a.
\end{align}
Now we are going to bound the term: $\E_{s\sim\rho_E}[V^*_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C}]\geq0$ (the lower bound is obvious).
We note $\hat{\pi}_C$ a deterministic optimal policy of the reward $\hat{R}^C$. We have:
\begin{equation}
V^*_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C}=(V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\hat{\pi}_C}_{R^C})+(V^{\hat{\pi}_C}_{R^C}-V^{\pi_E}_{R^C})+(V^{\pi_E}_{R^C}-V^{\pi_E}_{\hat{R}^C}).
\end{equation}
We are going to bound each of this three terms. But, first, let $\pi$ be a given deterministic policy. We have:
\begin{equation}
V^{\pi}_{R^C}-V^{\pi}_{\hat{R}^C}=V^{\pi}_{\epsilon^R}=(I-\gamma P_\pi)^{-1}\epsilon^R_{\pi}.
\end{equation}
If $\pi=\pi_E$, $\rho_E^T(V^{\pi}_{R^C}-V^{\pi}_{\hat{R}^C})=\rho_E^T(I-\gamma P_{\pi_E})^{-1}\epsilon^R_{\pi_E}=\frac{1}{1-\gamma}\rho_E^T\epsilon^R_{\pi_E}\leq\frac{1}{1-\gamma}\|\epsilon^R_{\pi_E}\|_{1,\rho_E}$.
If $\pi\neq\pi_E$, we introduce the following concentration coefficient:
\begin{equation}
C_{\pi}=(1-\gamma)\sum_{t\geq0}\gamma^tc_{\pi}(t), \text{ with } c_{\pi}(t)=\max_{s\in S}\frac{(\rho_E^TP^t_\pi)(s)}{\rho_E(s)}.
\end{equation}
We have then: $\rho_E^T(V^{\pi}_{R^C}-V^{\pi}_{\hat{R}^C})\leq\frac{C_\pi}{1-\gamma}\rho_E^T\epsilon^R_{\pi}\leq\frac{C_\pi}{1-\gamma}\|\epsilon^R_{\pi}\|_{1,\rho_E}$. So, if we note, $C_*=C_{\hat{\pi}_C}$ and $\epsilon_R=\max_{\pi\in A^S}\|\epsilon_\pi^R\|_{1,\rho_E}$, we are able to give an upper bound to the first and third terms:
\begin{equation}
\rho_E^T((V^{\pi_E}_{R^C}-V^{\pi_E}_{\hat{R}^C})+(V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\hat{\pi}_C}_{R^C}))\leq\frac{1+C_*}{1-\gamma}\epsilon_R.
\end{equation}
Now, there is a still an upper bound to find for the second term. It is possible to decompose the second term:
\begin{equation}
V^{\hat{\pi}_C}_{R^C}-V^{\pi_E}_{R^C}=(V^{\hat{\pi}_C}_{R^C}-V^{\pi_C}_{R^C})+(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E})+(V^{\pi_E}_{R^E}-V^{\pi_E}_{R^C}).
\end{equation}
By construction, $\pi_C$ is optimal for $R^C$, so $V^{\hat{\pi}_C}_{R^C}-V^{\pi_C}_{R^C}\leq0$ which implies:
\begin{equation}
V^{\hat{\pi}_C}_{R^C}-V^{\pi_E}_{R^C}\leq(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E})+(V^{\pi_E}_{R^E}-V^{\pi_E}_{R^C}).
\end{equation}
By construction, we have $V^{\pi_C}_{R^C}=q_{\pi_C}$ and $V^{\pi_E}_{R^E}=q_{\pi_E}$, thus:
\begin{equation}
\rho_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E})=\rho_E^T(q_{\pi_C}-q_{\pi_E})=\sum_{s\in S}\rho_E(s)(q(s,\pi_C(s))-q(s,\pi_E(s)))[\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}}].
\end{equation}
We note $\Delta q=\max_{s\in S}(\max_{a\in A}q(s,a)-\min_{a\in A}q(s,a))=\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))$ and we have:
\begin{equation}
\rho_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E})\leq\Delta q\sum_{s\in S}\rho_E(s)[\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}}]= \Delta q \epsilon_C.
\end{equation}
Finally, we also have:
\begin{align}
\rho_E^T(V^{\pi_E}_{R^E}-V^{\pi_E}_{R^C})&=\rho_E^T(I-\gamma P_{\pi_E})^{-1}(R^E_{\pi_E}-R^C_{\pi_E}),
\\
&=\rho_E^T(I-\gamma P_{\pi_E})^{-1}\gamma P_{\pi_E}(q_{\pi_C}-q_{\pi_E}),
\\
&=\frac{\gamma}{1-\gamma}\rho_E^T(q_{\pi_C}-q_{\pi_E})\leq \frac{\gamma}{1-\gamma}\Delta q \epsilon_C.
\end{align}
So the upper bound for the second term term is: $\rho_E^T(V^{\hat{\pi}_C}_{R^C}-V^{\pi_E}_{R^C})\leq \frac{\Delta q}{1-\gamma}\epsilon_C$.
If we combine all of the results, we obtain the final bound:
\begin{equation}
0\leq\E_{s\sim\rho_E}[V^*_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C}]\leq \frac{1}{1-\gamma}(\Delta q \epsilon_C+(1+C_*)\epsilon_R)).
\end{equation}
\end{proof}

Now, let us comment this results with some remarks. the bound is obviously true for $\hat{R}_C$, but in this case the classifier or the regressor have bad performance which is not likely. The algorithm and the analysis are invariant by  dilatation of $q$, hence of $\Delta q$. We could normalize $\Delta q$ by imposing $\Delta q=1$ (the range of variation of $q$ induces the one of $R^C$, $\hat{R}^C$ and $V^{\pi_C}_{\hat{R^C}}$). The constant $C_*$ could be estimated a posteriori ( after $\hat{R}^C$ being calculated). We could upper bound $C_*$ by a more usual and general concentration coefficient but $C_*$ gives a tighter final result (approximately, if $\hat{\pi_C}\approx\pi_E$ then $C_*=1$).

In order to control $\epsilon_R$, we need a precise regression over the expert stationary distribution and for all the actions. In theory, we need to be able to sample the whole dynamic (even the non expert actions). In practice, a heuristic is used (see Section~\ref{sec:experiments}) (however this heuristic is not justified by the analysis).
Finally, if the classifier and the regressor are perfect ($\epsilon_C=\epsilon_R=0$), $\pi_E$ is the only deterministic optimal policy of $\hat{R}^C=R^C$ (the uniqueness suppose that $\forall s\in S, Card(\argmax_{a\in A}q(s,a))=1$ ). This uniqueness also hint at the fact that we found a non-trivial reward (we recall that the null reward admit \emph{every} policy as optimal).

The sample complexity induced by this bound depends on the sample complexity of the classification and regression subroutines. The litterature is wide enough for methods accomodating most of use cases to be found. Being able to leverage such common algorithms as multi-class classifiers and regressors is a big advantage of our cascading approach.

\section{Related Work}
\label{sec:related}
IRL was first introduced in \cite{russell1998learning} and then formalized in \cite{ng2000algorithms}. A seminal paper, \cite{abbeel2004apprenticeship} blurs the frontier between imitation and IRL, as this algorithm outputs (mixing) policies, and not explicitely a reward (though it is possible to extract one). Other approaches with the same philosophy of finding a policy
(through some reward function) such that its feature expectation (or
more generally some measure of the underlying trajectories'
distribution) matches the one of the expert policy are summarized in \cite{neu2009training}. This principle implies repeatedly solving the MDP in order to get information about the feature expectation of the policy induced by the iteratively built reward. This has a high cost, sample-wise as well as computationally. The Projection IRL (PIRL) algorithm of \cite{abbeel2004apprenticeship} is used as a benchmark in our experiments Sec.\ref{sec:experiments}.

Classification and IRL have met in the past in \cite{ratliff2006maximum}, but the labels were complete optimal policies rather than actions and the inputs were MDPs (which had to be solved).

Using the non trivial notion of metric in an MDP, \cite{melo2010learning} build a kernel which is used
in a classification algorithm, showing improvements compared to a
non-structured kernel. However, this approach is not an IRL
algorithm, and more important assessing the metric of an MDP is a
quite involved problem.


Few algorithms do not require solving the direct RL problem. \cite{Dvij:2010} only applies to linearly-solvable MDPs.
\cite{boularias:2011} uses a subgradient ascent of a utility function based on the notion of relative entropy. Importance sampling is suggested as a way to avoid solving the MDP. This requires sampling trajectories according to a non-expert policy and
the direct problem remains at the core of the approach (even if
solving it is avoided).

The SCIRL algorithm \cite{klein2012scirl} is able to sole the IRL problem in the same dire conditions as the cascading approach. However, it outputs a reward on the state space only, which can prove problematic if the expert relies on information about the cost of an action in a certain state. We avoid that by using a reward over the state-action space.

%% \begin{itemize}
%% \item On reprend peu ou prou la même que SCIRL et que celle du précédent papier. On profite de la mention d'une approche pour préciser en quoi la nôtre diffère.
%% \end{itemize}
%% \subsection{Imitation}
%% \label{sec-7-1}

%% \begin{itemize}
%% \item Ce n'est pas le problème que l'on se pose, mais résoudre l'IRL peut permettre une forme fine d'imitation.
%% \item La frontière est un peu floue, exemple de PIRL
%% \item On peut considérer que tous les algos revus par Neu sont des algos d'imitation dans le sens où ils cherchent une proximité des $\mu$.
%% \item Coincidentallement, ils ont besoin de résoudre le problème direct, pas nous.
%% \end{itemize}
%% \subsection{Méthodes entropiques}
%% \label{sec-7-2}

%% \begin{itemize}
%% \item N'ont pas besoin de résoudre le problème direct, mais ont besoin d'autres données que celles de l'expert.
%% \end{itemize}
%% \subsection{Autres méthodes}
%% \label{sec-7-3}

%% \begin{itemize}
%% \item parler des MDPs solvables linéairement, des papiers mentionnés par les reviewers de NIPS, et dire du bien de SCIRL
%% \end{itemize}
\section{Experiments}
\label{sec:experiments}
\subsection{Criterion}
The theoretical bound provided in Sec. \ref{sec:analysis} is about the optimality of the expert with respect to the reward function output by our algorithm (this is also the way the problem was framed in \ref{sec:background}). For the experiments, we use a somewhat more restrictive criterion : a measure of the optimality of an agent trained on the reward output by our algorithm, with respect to the "true" reward function, the reward function of the expert. We compare the mean value function with respect to $R^E$for a RL agent perfectly trained to optimize $R^C$. We do that firstly because, for once, we can. The bound we provide in the analysis is something that can be measured even if $R^E$ does not actually exist. We make use here of the fact that $R^E$ not only exists, but is known to us as we use, as an expert, a RL agent perfectly trained on a handcrafted $R^E$. The second reason why we choose this different metric in this section is because we believe the theoretical bound to be convinving, and what we wanted to illustrate here is not that the theoretical bound holds, but that under really dire sample scarcity (a case not covered by the analysis) our algorithm is well behaved given the use of heuristics.
\subsection{Setting}
The setting of the experiment is a driving simulator inspired from a benchmark seen in \cite{syed2008apprenticeship,syed2008game}.The agent controls a car that can switch between the three lanes of the road, go off-road on either side and modulate between three speed levels. At all times, there will be one car in one of the three lanes. Even at the lowest speed, the player's car moves faster than the others. When the other car disappear at the bottom of the screen, another one appear at the top in a randomly chosen lane. It takes two transitions to completely change lanes, as the player can move left or right for half a lane's length at a time. At the highest speed setting, if the other car appear in the lane the player is in, it is not possible to avoid the collision. The main difference between the original benchmark and ours is that we made the MDP more ergodic by allowing the player to change speed whenever he wishes so, not just during the first transition. If anything, by adding two actions, we enlarged the state-action space and thus made the problem harder. FIXME:METTRE UNE CAPTURE DECRAN. The expert is trained to go as fast as possible (high reward) while avoiding collisions (harshly penalised) and avoiding going off-road (moderately penalised). Any other situation receives a null reward.

FIXME:Comparer à SCIRL et à PIRL
We need some baseline to compare our new approach to. The natural random baseline consists in drawing a random reward vector (with a uniform law) and training an agent on it. Another natural benchmark is to compute the mean value function of the classifier policy. Also there is no concept of reward here, it shows that even when talking only about imitation, IRL is a relevant method.
\subsection{Instanciation}
The cascading algorithm can be instanciated with any classification or regression algorithm. The choice of such subroutines may be dictated by the kind and amount of available data, by ease of use or by computational complexity, for example.

We referred in Sec.\ref{sec:algo} to \emph{score-function based multi-class classifiers} and explained how the classification rule is similar to the greedy mechanism that exists between an optimal action-value function and an optimal policy in a MDP. Most classification algorithm can be seen as such a classifier. In a simple $k$-nearest neighboor approach, for example, the score function $q(s,a)$ is the number of elements of class $a$ among the $k$-nearest neighboors of $s$. The generic M-SVM model makes the score function explicit \cite{guermeur2011generic} (equation 1). In our experiments, we choose to use a structured margin classification approach \cite{taskar2005learning}. FIXME:Recopier le QP

Ease of implementation lead us to choose a simple least-square regressor for our experiments. Using the same features $\phi$ as our classifier, it can be written (using straightforward matricial notations) as :  $\theta = (\Phi^T\Phi + \lambda Id)^{-1}\Phi^T\hat R$. The reward function is : $\hat R^C(s,a) = \theta^T \phi(s,a)$.

It is possible to get imaginative in the last step. For example, using a gaussian process regressor \cite{rasmussen2006gaussian} that outputs both expectation and variance can enable the use of fuzzy reward reinforcement learning. FIXME:Trouver une ref pour les fuzzy rewards

\subsection{Results}
\begin{figure}
  \includegraphics[width=\linewidth]{"Fig3"}
  \caption{Data is shown with mean, standard deviation, minimum and maximum value over 50 runs. The lower baseline is an agent trained on a reward generated by using the same features as the cascading approach and the classifier, but with a random vector of parameters.}
  \label{fig:Highway}
\end{figure}

Results are shown Fig.\ref{fig:Highway}. It is very interesting to compare our algorithm to the behavior of a classifier alone. With \emph{the exact same data}, albeit the use of a very simple heuristics, the cascading approach demonstrate far better performance, from the start. This is a clear illustration of the fact that using the Bellman equation to construct the data fed to the regressor truly make use of the information that the transitions $(s,a,s')$ bear (we recall that the classifier only uses $(s,a)$ couples). Furthermore, the classifier whose results are diplayed here is the output of the first step of the algorithm. Although what we show on the plot is not exactly the classification error, it is closely related (if $\pi^C=\pi^E$ then the classification would have the same performance on our plot as the expert, which is not the case). This shows that our algorithm is empirically more forgiving of classification errors than our theoretical bound let us expect.

PIRL is given the transition probabilities (that is, a $|S|\times |S|\times |A|$ matrix) and yet it does only slightly better than our approach with 100 samples. This also show that we are indeed very sample efficient, without having to repeatedly solve the MDP.

%% \begin{itemize}
%% \item La borne fournie en \hyperref[sec-5]{Analyse} mesure le degré d'optimalité de la politique de l'expert vis à vis de la nouvelle récompense. Les expériences mesurent le degré d'optimalité de l'agent (ici entendu sous forme d'agent entraîné parfaitement sur la récompense sortie par notre algorithme) vis à vis de la vraie récompense (celle de l'expert).
%% \begin{itemize}
%% \item On se permet de faire ça car dans notre dispositif on la connait. Ce n'est pas le cas dans la vraie vie.
%% \item En pratique c'est un critère plus restrictif : formellement, si $\pi^E$ est optimale pour $R_C$, alors l'agent et l'expert ont les mêmes performances (wrt $R_C$), puisque l'agent est optimal pour $R_C$ par définition de l'agent. En revanche on regarde si l'agent est optimal pour $R_E$, ce qui est profondément ``injuste'', puisqu'on ne sait rien de $R_E$. On ne sait même pas si elle existe réellement.
%% \item Il s'agit d'une métrique qu'il n'est pas possible de mesurer sur un problème réel. C'est pourquoi la démonstration est intéressente car elle parle de quelque chose que l'on peut plus facilement évaluer ; et surtout elle parle de ce que l'on s'est proposé de résoudre à la base. Ces expériences ne sont qu'une illustration.
%% \end{itemize}
%% \item Les résultats obtenus sur ce problème sont notamment comparés à la classif, il s'agit d'une illustration du fait que l'algorithme est capable d'extraire les infos sur $P$ contenues dans les transitions, grâce à l'équation de Bellman qui est présente (modulo deux trois manips) dans l'expression de $\hat r$. Nous avons injecté la structure du MDP dans la classification (ça me rappelle quelque chose\ldots{} ;)
%% \item Le seul plot est celui du Highway (similaire à SCIRL). On enlève le Gridworld (reporté en annexe car trop d'infos pour si peu de place)
%% \item On instancie avec Taskar et un moindre carré. Ceci n'a pas la moindre importance.
%% \item Mentionner le fait que les autres algos ne peuvent résoudre le problème tel qu'on le pose. On s'apesentira là dessus dans \hyperref[sec-7]{Autres approches}.
%% \item Expliquer que quitte à tester expérimentallement, on a choisi de s'éloigner du cadre de l'analyse pour se placer dans cadre \emph{plus difficile à gérer}, ou seules les données de l'expert sont disponibles.
%% \begin{itemize}
%% \item Cela implique l'usage d'une heuristique pour la régression :
%% \begin{itemize}
%% \item sans heuristique, c'est le choix par défauit du régresseur (i.e. initialisation à 0, à R$_{\mathrm{min}}$ ou R$_{\mathrm{max}}$) qui décidera de l'optimisme ou pessimisme de l'algo
%% \item Pour éviter ça, on place nous même notre heuristique.
%% \end{itemize}
%% \item que l'on se contente d'expliquer à un niveau intuitif. L'analyse formelle n'a pas été effectuée, ne le sera probablement pas, on se contente de montrer qu'en pratique, ça fonctionne. Et ça fonctionne mieux que les autres algos, sauf SCIRL.
%% \item Cela implique aussi d'utiliser $\pi_C$ plutôt que $a'$.
%% \end{itemize}
%% \item Lien avec l'analyse : on a de bons résultats, même si le classifieur n'est pas bon. Notre borne est donc très très large.
%% \item La complexité en échantillons : avec 50 samples, on fait mieux que PIRL qui a accès à une matrice de taille $|S|\times|S|\times |A|$
%% \item La complexité en temps : on tourne 100 fois plus rapidement que PIRL
%% \end{itemize}
\section{Conclusion}
\label{sec:conclusion}
We have introduced a new way to perform IRL by cascading two well-known supervised approaches, namely classification and regression. The expert is shown to be near-optimal for the reward function the proposed algorithm outputs, given small classification and regression errors. A few practical examples of classifiers and regressors have been given, and one combination has been empirically shown to be very resilient to dire lack of data on the input (only data from the expert was used to retrieve the reward function), provided the use of simple heuristics. On the chosen benchmark (a car driving simulator), our algorithm is shown to outperform other state of the art approaches, and to be on-par with the very recent SCIRL algorithm. We plan on deepening the analysis of the theoretical properties of our approach and on applying it to real world robotics problems.
%% \begin{itemize}
%% \item Ouverture : Existe-t-il des problèmes tels que SCIRL et Cascading diffèrent en performance ?
%% \item Que faire quand on ne peut résoudre le RL facilement ?
%% \item Que faire en cas d'expert sous optimal ?
%% \item ouverture sur l'active learning en reprenant les notions de pessimisme ou d'optimisme en présence d'incertitude
%% \end{itemize}

\section{Annexes}
\label{sec-9}
\subsection{GridWorld}
\label{sec-9-1}

\begin{itemize}
\item On montre avec les deux formes de récompenses qui donnent la même fonction de valeur. On fait référence aux travaux de Ng qui résolvent exactement ce problème. Les récompenses ont la même tête, ça devrait rassurer le lecteur hésitant.
\item si on a le temps, on peut lancer une expérience où le nombre (en supposant une répartittion uniforme) des samples augmente et où la récompense tend vers ``la vraie''.
\end{itemize}
\subsection{Inverted pendulum}
\label{sec-9-2}

\begin{itemize}
\item On montre qu'on généralise sans problème aux espaces continus, on montre qu'avec des données très, très sparses vis à vis de l'espace d'état on arrive pourtant à récupérer une récompense ayant la même fonction de valeur que celle de l'expert.
\item ça montre aussi ce qui se passe quand l'espace d'hypothèse est ``mal choisi'' (i.e. la ``vraie'' récompense n'y était pas représentable).
\end{itemize}
\subsection{Highway}
\label{sec-9-3}

\begin{itemize}
\item Si on a le temps : faire conduire quelqu'un puis lancer l'imitation, voir si on reproduit les consignes données à l'opérateur humain, qui aura eu la bonté de faire quelques erreurs dans l'exécution. Ca permet de voir le comportement en cas d'expert sous optimal et de taire les questions à ce sujet.
\end{itemize}

\bibliographystyle{plain}
\bibliography{Biblio}

\end{document}
