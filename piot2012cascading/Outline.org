* Abstract
* Introduction
  De la même manière que pour le papier de SCIRL, l'intro est une vaste annonce du papier, avec des références pointues (sous-section, voire sous,sous-section) vers les parties du papier qui expliquent tel ou tel point en détail.
* Formalisme
  - Parler du domaine de la prise de décision séquentielle
  - Expliquer le problème direct, en profiter pour introduire l'équation d'évaluation de Bellman et l'équation d'optimalité de Bellman
    - $RL(MDP=\{S,A,\gamma,(P),R\}) = \pi^*:S\times A \rightarrow \mathbb{R}$
    - $\forall \pi, Q^\pi(s,a) = R(s,a) + \sum_{s'}P(s'|s,a)Q^\pi(s',\pi(s'))$
    - $\pi^* = \arg\max_aQ^*(s,a)$
  - Introduire le problème inverse dans une forme mathématique relativement propre
    - $IRL( MDP\backslash R = \{S,A,\gamma,(P)\} \cup \{\pi^E\} = R : S\times A \rightarrow \mathbb{R}$ telle que $\pi^E$ est optimale pour $R$
    - Mentionner la nature mal posée du problème sans s'appesentir dessus. Préciser toutefois qu'aucune approche ne le résoud de manière mathématiquement satisfaisante à cause de ça.
    - insister sur l'optimalité de l'expert comme hypothèse. Un expert sous optimal, c'est un autre champ de littérature, éventuellement donner deux trois refs.
* Description de l'algorithme
  - Mentionner (sans en parler, ce sera fait dans [[Imitation]]) l'imitation. Une approche supervisée évidente est la classification, avec pour entrée un set $D_C$ de données issues de l'expert. Rappeler (expliqué pour la première fois dans [[Formalisme]] qu'on se restreint (sans perte de généralité, le préciser) au cas des politique déterministes.
  - Tous les classifieurs utilisent une fonction de score (je n'ai pas trouvé un seul exemple d'un classifieur qui n'en utilise pas), il s'agit juste d'un manière d'écrire les choses.
  - Expliquer que l'on identifie le mécanisme de classification au mécanisme glouton qui régit la politique optimale (donc celle de l'expert).
  - Sans chercher à justifier mathématiquement la chose (ce sera fait dans la partie [[Analyse]]), expliquer qu'intuitivement l'on va identifier fonction de score du classifieur et fonction de qualité de l'expert (qui est optimale pour une certaine récompense, rappelons-le).
    - Repartir de l'équation d'évaluation de Bellman pour isoler le terme $R(s,a)$ et introduire $r:S\times A\in \mathbb{R}$, la récompense issue de cette équation lorsque l'on remplace $Q^E$ par la fonction de score : $r(s,a) =q(s,a) - \sum_{s'}P(s'|s,a)q(s',\pi(s')) $
    - On estime ce terme à partir des transitions $D_R$ car les probabilités de transitions sont inconnues : $\hat r(s,a) = q(s,a)-\gamma \sum_{s'} {1\over N_{s'}}\sum_{(s,a,s',a')\in D_R}q(s',a')$
    - La fonction de récompense est donnée par une régression sur $D_R \cup {\hat r}$.
  - Donner l'algorithme l'environnement =algorithm2e=
  - Rapidement, l'étape de régression permet d'ajouter la structure du MDP au résultat de classif, en utilisant l'équation de Bellman.
  - Bien préciser qu'il ne s'agit ici que d'une explication qualitative, "avec les mains", la preuve du fait que ça résoud bien le problème est donnée dans [[Analyse]], une démonstration sur des problèmes si complexes que l'état de l'art ne peut les résoudre (sauf SCIRL) est donnée dans [[Experience]]. Les avantages comparés sont expliqués dans [[Autres Approches]]. Enfin, des explications plus intuitives sur les mécanismes internes de l'algorithme, ainsi que des résultats expérimentaux supplémentaires sont fournis en [[Annexes]].
* Analyse

  - On rappelle le problème posé au début.
  - Le théorème 1 montre que modulo un bonne classif et un bon régresseur, on résoud exactement le problème que l'on s'était posé
  - On en fait la démo rapide (éventuellement des bouts peuvent être déportés en +Pologne+ [[Annexes]]).
  - On insiste sur le fait que la récompense trouvée n'est pas triviale, on a transcendé la nature mal posée du problème.
  - On dit un mot rapide sur la complexité en échantillon (ça dépend du classifieur et du régresseur, mais [[Expériences]] montre qu'on est bon là dessus)
  - ainsi que sur la complexité temporelle (ça dépend toujours, mais avec une M-SVM et un moinde carré ça prend O(quelque chose) (indice: pas beaucoup)) FIXME:compléter la complexité
* Expériences
  - La borne fournie en [[Analyse]] mesure le degré d'optimalité de la politique de l'expert vis à vis de la nouvelle récompense. Les expériences mesurent le degré d'optimalité de l'agent (ici entendu sous forme d'agent entraîné parfaitement sur la récompense sortie par notre algorithme) vis à vis de la vraie récompense (celle de l'expert).
    - On se permet de faire ça car dans notre dispositif on la connait. Ce n'est pas le cas dans la vraie vie.
    - En pratique c'est un critère plus restrictif : formellement, si $\pi^E$ est optimale pour $R_C$, alors l'agent et l'expert ont les mêmes performances (wrt $R_C$), puisque l'agent est optimal pour $R_C$ par définition de l'agent. En revanche on regarde si l'agent est optimal pour $R_E$, ce qui est profondément "injuste", puisqu'on ne sait rien de $R_E$. On ne sait même pas si elle existe réellement.
    - Il s'agit d'une métrique qu'il n'est pas possible de mesurer sur un problème réel. C'est pourquoi la démonstration est intéressente car elle parle de quelque chose que l'on peut plus facilement évaluer ; et surtout elle parle de ce que l'on s'est proposé de résoudre à la base. Ces expériences ne sont qu'une illustration.
  - Les résultats obtenus sur ce problème sont notamment comparés à la classif, il s'agit d'une illustration du fait que l'algorithme est capable d'extraire les infos sur $P$ contenues dans les transitions, grâce à l'équation de Bellman qui est présente (modulo deux trois manips) dans l'expression de $\hat r$. Nous avons injecté la structure du MDP dans la classification (ça me rappelle quelque chose... ;)
  - Le seul plot est celui du Highway (similaire à SCIRL). On enlève le Gridworld (reporté en annexe car trop d'infos pour si peu de place)
  - On instancie avec Taskar et un moindre carré. Ceci n'a pas la moindre importance.
  - Mentionner le fait que les autres algos ne peuvent résoudre le problème tel qu'on le pose. On s'apesentira là dessus dans [[Autres approches]].
  - Expliquer que quitte à tester expérimentallement, on a choisi de s'éloigner du cadre de l'analyse pour se placer dans cadre /plus difficile à gérer/, ou seules les données de l'expert sont disponibles.
    - Cela implique l'usage d'une heuristique pour la régression :
      - sans heuristique, c'est le choix par défauit du régresseur (i.e. initialisation à 0, à R_min ou R_max) qui décidera de l'optimisme ou pessimisme de l'algo
      - Pour éviter ça, on place nous même notre heuristique. 
    - que l'on se contente d'expliquer à un niveau intuitif. L'analyse formelle n'a pas été effectuée, ne le sera probablement pas, on se contente de montrer qu'en pratique, ça fonctionne. Et ça fonctionne mieux que les autres algos, sauf SCIRL.
    - Cela implique aussi d'utiliser $\pi_C$ plutôt que $a'$.
  - Lien avec l'analyse : on a de bons résultats, même si le classifieur n'est pas bon. Notre borne est donc très très large.
  - La complexité en échantillons : avec 50 samples, on fait mieux que PIRL qui a accès à une matrice de taille $|S|\times|S|\times |A|$
  - La complexité en temps : on tourne 100 fois plus rapidement que PIRL
* Autres approches
  - On reprend peu ou prou la même que SCIRL et que celle du précédent papier. On profite de la mention d'une approche pour préciser en quoi la nôtre diffère.
** Imitation
   - Ce n'est pas le problème que l'on se pose, mais résoudre l'IRL peut permettre une forme fine d'imitation.
   - La frontière est un peu floue, exemple de PIRL
   - On peut considérer que tous les algos revus par Neu sont des algos d'imitation dans le sens où ils cherchent une proximité des $\mu$. 
   - Coincidentallement, ils ont besoin de résoudre le problème direct, pas nous.
** Méthodes entropiques
   - N'ont pas besoin de résoudre le problème direct, mais ont besoin d'autres données que celles de l'expert.
** Autres méthodes
   - parler des MDPs solvables linéairement, des papiers mentionnés par les reviewers de NIPS, et dire du bien de SCIRL
* Conclusion
  - Ouverture : Existe-t-il des problèmes tels que SCIRL et Cascading diffèrent en performance ?
  - Que faire quand on ne peut résoudre le RL facilement ?
  - Que faire en cas d'expert sous optimal ?
  - ouverture sur l'active learning en reprenant les notions de pessimisme ou d'optimisme en présence d'incertitude
   
* Annexes
** GridWorld
  - On montre avec les deux formes de récompenses qui donnent la même fonction de valeur. On fait référence aux travaux de Ng qui résolvent exactement ce problème. Les récompenses ont la même tête, ça devrait rassurer le lecteur hésitant.
  - si on a le temps, on peut lancer une expérience où le nombre (en supposant une répartittion uniforme) des samples augmente et où la récompense tend vers "la vraie".
** Inverted pendulum
  - On montre qu'on généralise sans problème aux espaces continus, on montre qu'avec des données très, très sparses vis à vis de l'espace d'état on arrive pourtant à récupérer une récompense ayant la même fonction de valeur que celle de l'expert.
  - ça montre aussi ce qui se passe quand l'espace d'hypothèse est "mal choisi" (i.e. la "vraie" récompense n'y était pas représentable).
** Highway
  - Si on a le temps : faire conduire quelqu'un puis lancer l'imitation, voir si on reproduit les consignes données à l'opérateur humain, qui aura eu la bonté de faire quelques erreurs dans l'exécution. Ca permet de voir le comportement en cas d'expert sous optimal et de taire les questions à ce sujet.
