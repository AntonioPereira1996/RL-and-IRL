We thank the reviewers for their time and remarks.

The first reviewer's view of the Reward \hat r_i on which we train the regression algorithm (3rd § of the review) probably stems from us not being clear enough when explaining what the training set D_R is (section 3 of the paper). It is composed of what is known in the Reinforcement Learning (RL) community as transitions. These tuples {s_i,a_i,s'_i} are sampled according to the dynamics of the system, a representation of which is the transition probability function P(s'|s,a).
We consider this function not to be known, but having access to such transitions allows us to build an unbiased estimator of R^C. Indeed, the constructed samples \hat r_i are not what the reviewer describes, as the argument of the second term q(.,.) in the equation is s' and not s. There is no such simple explanation of what this constructed reward is, although one can notice its expression comes from the Bellman Equation (line 102). The reward function we find is a way to "explain" the behavior of the expert (encoded by the score function q seen as a state-action value function), taking the dynamics of the system into account.

The driving simulator is a complex discrete problem in the sense that, to the best of our knowledge, it has never been tackled by a published algorithm fed only with transitions from the expert. The cardinal of the state space is around 1000, yet with as little as 50 samples we are able to recover a reward function on par with what PIRL finds when given access to the transition probability and full expert policy (which, as we recall, are unknown to our aglorithm).

??
We provided additional material containing easier-to-read plots, as rightfully requested by reviewer 1.
??

The performance criterion is the mean of the value function *with respect to the true (theoretically unknown) reward function* for an agent trained on the reward function found by our algorithm. This mean is computed uniformly over the whole state space. A napkin explanation of this criterion is to consider the setting to be a video game. The performance criterion is the total, final score of our player, averaged when the player starts in every possible state.

We indeed consider the expert policy to be deterministic.

Although an empirical study with different classifiers is in progress, we are confident this is not an important factor. As stated in the analysis, as long as \epsilon_C is small, our algorithm should not suffer from a change of classifier subroutine.

The space constraints in the NIPS format did not allow us to expand on the soundness of the IRL problem. What is of interest to us is a reward function for which the expert is optimal. Of course we would be glad the find the "true" reward function over which the expert was trained on the MDP, but this is an ill-posed problem as every reward function is in an infinite set of strictly equivalent rewards (positive scaling for example is one of many ways to change a reward without changing the assiciated behaviors). The bottom line is that "finding a non trivial reward function for which the expert is optimal" is the problem we are trying to solve. We suggest skimming over Russel's Learning agents for uncertain environments (1998) and Ng and Russel's Algorithms for inverse reinforcement learning (2000) for a discussion about these matters and the usefulness of finding a reward function.

Note that we do not, as in Abbeel and Ng's 2004 paper, try to match two policies (or a metric on feature expectation difference) but indeed to solve the IRL problem in its "reward finding" formulation.
Finding an optimal behavior from a reward is the topic of the RL field, far too large to be addressed here. We only optimized the reward in the experiments because our performance criterion requested so. The policy of the agent is not what we are after. 
Our performance criterion also requested the "true" reward function of the expert to be known whereas the problem formulation expects it to be unknown, if it exists at all.

Knowing all this, we hope it becomes clear that there is no circular reasoning in this work. We assume there exists a non trivial reward function for which the expert's behavior is optimal and prove that our algorithm exhibits such a function. A whole body of litterature exists that proves that this is not a trivial problem, especially when the only information we have access to is limited to samples from the expert.

Our experiments are there to show the advantages in term of sample efficiency of our work against state of the art methods in state of the art benchmarks. We provide a performance criterion based on the "true" reward function which happens to be known by an oracle on those toy problems, in order to illustrate the most straightforward way of using the result of the IRL problem : training an agent that is on par with the expert in terms of control quality. There exists other ways, such as /a posteriori/ semantic emplanation of a behavior (in economics or biology) or task transfer, but this is too large to be covered here.

??
As it happens, we actually have results on the cart pole, where the state space is continuous. We included them in the additionnal material.
??

Although the provided empirical evaluation is on toy problems, we insist that our algorithm is to the best of our knowledge the only published algorithm able to solve the IRL problem on such benchmarks with only access to transitions from the expert. PIRL was given access to the transition probability and full expert policy in order to solve the problem. This is indeed a very practical result. We do not simply add theorems to the litterature.

We do not need the expert to take suboptimal actions, ever. The first step of the algorithm can be any standard classifier. This contradicts what reviewer 2 states : "to be able to rank actions, actions should be observed, hence have been performed by the expert". A dataset {s_i, a_i=\pi_E(s_i)} is enough for a classifier to rank actions.

When solving the cart pole problem, we used feature over a continuous state space (although the action space stayed discrete).
??
These results were not included in the main paper but are now provided as additionnal material.
??

NOTE A MATTHIEU, BILAL ET OLIVIER
§2 Pour répondre au premier reviewer, j'ai essayé d'être aussi diplomatique que possible, je sais pas si j'ai vraiment réussi. Le fait reste qu'il a mal lu le papier (il parle que quelque chose qui correspondrait à q(s,a)-gamma*q(s,pi(s)) alors qu'on utilise ...-gamma*q(S_PRIME,...) .

§3 Je peux fournir un additionnal matérial avec plein de courbes. Chaque courbe sur un seul graphe (pour éviter la fig 1a) avec min max, ainsi que la tête de la récompense pour le GridWorld et aussi sur le pendule. Comme ça pour la fig 1a on droppe l'écart-type et on garde juste le min-max de PIRL pour montrer que parfois il plante complètement.

Pour répondre au reviewer 2, c'est assez difficile d'introduire le fait qu'il a rien compris sans froisser personne. Pour le coup du classifier qui aurait besoin de samples sous optimaux, j'ai pas trop pris de gants je me suis dit qu'il était important que le méta reviewer saisisse que ce reviewer là a quand même fait des erreurs grossières dans sa relecture.

Je me demande si le reviwer 3 est pas aussi un des reviwer de SCIRL ?
FIN DE NOTE
