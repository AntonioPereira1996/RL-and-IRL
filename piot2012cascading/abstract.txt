This paper considers the inverse reinforcement learning (IRL) problem. The IRL framework assumes that an expert, demonstrating a task, is acting optimally in a Markov Decision Process (MDP) with respect to an unknown reward function to be discovered. This reward function is seen as the most succinct description of the task, allowing for task transfer from the expert to an agent with potentially different abilities. The proposed contribution consists in cascading a classification and a regression steps to produce a non-trivial reward function for which we show the expert policy to be near optimal. In addition to being generic, this approach is model-free, it does not require solving any direct reinforcement learning problem (unlike most of IRL algorithms) and it solely relies on transitions from the expert (no need to sample trajectories according to other policies). All of this is illustrated through nontrivial experiments.
