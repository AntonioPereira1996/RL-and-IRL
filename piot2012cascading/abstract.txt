This paper considers the inverse reinforcement learning (IRL) problem. The IRL framework assumes that an expert, demonstrating a task, is acting optimally in a Markov Decision Process (MDP) with respect to an unknown reward function to be discovered. This reward function is seen as the most succinct description of the task, allowing for task transfer from the expert to an agent with potentially different abilities. By cascading two well-known supervised learning (SL) methods we create a veritable IRL approach in the sense that the output of our algorithm is a non-trivial reward function for which the expert policy is near optimal. In addition to being model-free, our approach does not require the repeated computation of a (near)-optimal policy for different rewards, unlike most of existing IRL algorithms. We describe our approach and provide its mathematical analysis. Finally we empirically show its soundness through nontrivial experiments.
